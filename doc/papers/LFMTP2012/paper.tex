\documentclass{llncs}

\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{macros}
\usepackage{mathpartir}
\usepackage{url}

\allowdisplaybreaks[1]

\begin{document}

\title{Certifying, incremental type checking}

\author{Matthias Puech\inst{1,2} \and Yann R\'egis-Gianas\inst{1}}
\institute{Univ Paris Diderot, Sorbonne Paris Cit\'e, PPS, UMR 7126
  CNRS, $\pi r^2$, INRIA Paris-Rocquencourt, F-75205 Paris, France
  \and Department of Computer Science, University of Bologna, 40126
  Bologna, Italy}

\maketitle

\begin{abstract}
 %% 4 sentences:
 %% State the problem
  Type checkers are traditionally implemented as batch processes,
  taking a program and returning a binary information: well-typed or
  not.
 %% Say why it’s an interesting problem
  This laconicism presents a double problem: first, how can I trust
  that the checking algorithm (a complex program) is sound \wrt\ the
  declarative type system? Secondly, how can I avoid costly rechecks
  of the whole program when I only make a small, localized change?
 %% Say what your solution achieves
  A lightweight solution to both problems is letting the checker
  return a typing derivation: trust is then ensured by checking that
  this derivation is well-formed, and incrementality by the
  possibility of reusing already-computed pieces of derivations.
 %% Say what follows from your solution
  We present a system turning a type system and its checking algorithm
  into a safe and incremental one, consisting of a generic data
  structure to represent reusable derivations based on \LF\ and a
  general-purpose programming language to generate them.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}
%% It’s an interesting problem (1 page)

%% Describe the problem (Use an example)

The positive impact of rich type systems on programming language
design is not to be demonstrated anymore, both in terms of the safety
of program execution --- they ``do not go wrong'', and programming
practices --- modularity, early bug catching. Verifying well-typing of
real-world programs is thus a critical matter, but it is a complex
operation, for the compiler designer and for the human operator:

\paragraph{For the compiler designer}

Type systems are usually presented in a declarative manner as a set of
inference rules, together with proofs that all programs accepting a
derivation in this system respect a certain dependability claim, the
famous ``progress and preservation'' pair. Yet, actual algorithms of
verification often differ very much from their declarative
description. This is of course true when programs are provided with
little type annotations (``type inference''), in which case the
missing information has to be algorithmically synthesized, but is also
true in many cases for what is commonly referred as ``type checking'',
or explicit ``church-style'' calculi. Take for example dependent type
systems, relying on a certain notion of definitional equality on
types, or type systems with subtype polymorphism: both rely on a rule
of the form
$$
\infer{\vdash M : A \\ A\leq B}{\vdash M : B}
$$
(for a given notion of $\leq$) which is not syntax-directed. The
implementer then has to convert the declarative system to an
algorithm, and convince himself that both are equivalent. This task is
highly non-trivial and non-modular: for instance, how to infer the
type of a System \sysname{T} recursor $\recb M N x y P$, in presence
of subtyping? The problem is that we have to guess a type $A$ more
general than that of $N$ and $P$, while giving that type to variable
$\var y$. A well-known algorithm if the subtyping relation forms a
complete lattice is to iterate the typing of $P$ with the join of the
previously computed and the new type until a fixpoint is reached.

The most common option is to formally prove the equivalence, \ie\ that
the (typing) relation is actually a function. Although these proofs
can be difficult to obtain, it is the most satisfactory because it
allows to take only the result of the algorithm into
consideration. But let us choose another path for a minute:
derivations in the declarative systems, if they cannot always be
easily inferred, can be \emph{verified} if provided by an oracle. If
the oracle is wrong, we can tell cheaply by verifying the witness
typing derivation it provides. Let us then imagine a type-checker to
be the pair of \emph{(i)} an untrusted procedure $\finfer M$ to infer
a typing derivation out of program $M$; it doesn't have to be
complete, or even sound, and \emph{(ii)} a trusted and fast kernel
$\fcheck{\md}$ to verify these derivations \wrt\ a given set of inference
rules $\Sigma$. This is a \emph{certifying} scheme (see
\cite{leroy2006compcert} for a discussion on approaches to reliable
compilers): if $\fcheck{\finfer M} = \cst{true}$ then there really
\emph{is} a derivation $\md$ for $M$, but the converse is not
necessarily true: function $\finfer{}\!\!$ could fail, or have bugs
(return an incorrect derivation). Explicitly generating the derivation
in the declarative system has the following advantages over the first
approach:
\begin{itemize}
\item it is lightweight: the untrusted algorithm is usually shorter
  than the corresponding equivalence proof
\item compilers often need to propagate type information further in
  the chain of transformation; the typing derivation produced by
  $\finfer{}$ being explicit, it could constitute the data structure
  being exploited.
\item it inherits benefits from certifying approaches like
  proof-carrying code \cite{necula1997proof}: certificates can be
  transmitted with the program, saving regeneration time and avoiding
  to disclose the generation program.
\end{itemize}
\ldots\ but there is more:

\paragraph{For the programmer}

As type systems become richer, it becomes increasingly difficult for
the programmer to write a well-typed program in one try, compile it
and run it: writing a program longer than a few dozens of lines
becomes a tight and non-linear interaction between the human and the
type-checker involving constant experiments, fixes, etc. If this
interaction is possible for short programs and fast checkers, it
becomes increasingly hindered by the latency of complete rechecks:
even if the modifications made to the program between two interactions
are small, the whole program is usually rechecked. This is especially
true for languages embedding formal verification aspects, like proof
assistants: there, tight interaction with a type or proof checker is
unavoidable due to the difficulty of writing correct proofs without
guidance from the system, and the time taken by many tools to infer
easy parts of the proofs (proof search). The constant modification of
the source makes it necessary to observe \emph{incrementally} the
effect of a small change on the whole edifice: we need to take
advantage of the results of previous checks in order to build the
result for a modified version of the program.

Several generic methods have been devised for turning a batch program
into an incremental one
\cite{pugh1989incremental,acar2003selective}. Unfortunately, they all
suffer from their genericity on the particular problem of type
checking:
\begin{itemize}
\item first, they ignore the higher-order nature of program syntax,
  and the kind of structural quotients we make on syntax trees with
  binders and environments ($\alpha$-equivalence, weakening\ldots): a
  hypothetical derivation of\ \ $\vdash x+2 : \cst{nat}$ is still valid
  when put under a binder\ \ $\vdash\lam y (x + 2) -
  y:\cst{nat}\to\cst{nat}$ and should not be regenerated;
\item for very large programs, we might want to feed the type checker
  only with a small \emph{delta} on the previous version and not the
  whole program, thus saving the time to recognize already checked
  parts;
\item finally, there is no way to a posteriori validate the result of
  the incremental process: the user has to trust not only the
  type-checking algorithm but also its transformation into an
  incremental one, which increases the size of the trusted base.
\end{itemize}

Now, if we consider as above a type-checker as a derivation-generating
procedure, and storing these derivations between calls to the
procedure, we could recognize already-built subderivations for
subterms common to an old, checked version and a new, to-be-checked
version, and graft together these pieces of derivation to form a new,
large one for the new version of the program, saving the regeneration
of all equal subterms' derivations in compatible contexts. This way we
constantly maintain the verifiable witness of well-typing for the
program, and take advantage of the quotients made on higher-order
syntax.

We can thus add to the list of advantages begun earlier that
explicitly generating typing derivations makes possible a form of safe
incrementality by sharing common subderivations.

In this paper, we present a framework to incrementally generate typing
certificates represented in the \LF\ logical framework, and implemented
as an \sysname{ML} library. We specifically propose:
\begin{itemize}
\item a simple language for representing programs and proof
  deltas, derived from contextual \LF\ \cite{nanevski2008contextual};
\item a computational language to write untrusted
  derivation-generating type checkers, based on a notion of function
  inverse %TODO le nom?
  to refer to already-computed values;
\item a generic algorithm to evaluate these type-checkers on program
  deltas and verify on-the-fly the generated certificates
  incrementally against a \emph{repository} of already constructed and
  checked derivations.
\end{itemize}

\section{Use case}
%% It’s an unsolved problem (1 page)

\subsection{A backend for incremental type checking}
\label{sec:use-incremental}

\begin{figure}
  \centering
%  \small
  \begin{mathpar}
    \infer{ }{\jts{\cst o} {\even}}

    \infer{\jts M \odd} {\jts {\s M} \even}

    \infer{\jts M \even} {\jts {\s M} \odd}

    \infer{\jts M \nat} {\jts {\s M} \nat}

    \infer{\jts M {A\to B} \and \jts N A} {\jts {\app M N} B}

    \infer{\infer*{}{\mbox{$
          \begin{array}{c}
            [\jts {\var x} A] \\[-0.3em]
            \vdots\\
            \jts M B
          \end{array}$
        }}} {\jts {\lam x M} {A\to B}}

    \infer{\jts M A \and \jsub A B}{\jts M B}

    \infer{\jts M \nat
      \and
      \jts N A
      \and
      \infer*{}{\mbox{$
        \begin{array}{c}
          [\jts {\var x} \nat] \quad
          [\jts {\var y} A] \\[-0.3em]
          \vdots \\
          \jts P A
        \end{array}
        $}}
    }{\jts {\recb M N x y P} A}

    \infer{ }{\jsub A A}

    \infer{ }{\jsub \even\nat}

    \infer{ }{\jsub \odd\nat}

    \infer{\jsub {A'} A \and \jsub B {B'} }{\jsub {A\to B}{A'\to B'}}
  \end{mathpar}
  \caption{System \sysname{T_{<:}}, declaratively}
  \label{fig:tsub}
\end{figure}

A student is learning about System \sysname {T_{<:}} --- a simply
typed $\lambda$-calculus with natural numbers $\cst{nat}$ coming in
two sets $\cst{even}$ and $\cst{odd}$, introduction $\z$ and $\s{M}$,
elimination ($\recb M N x y P$), and subtyping on those types; see
Fig. \ref{fig:tsub}. He (or she) wants to write a program computing
$(2+2)\times 3$. He knows that sometimes those $\cst{rec}$ arguments
are quite hard to get correct, but \sysname{T_{<:}} is a typed
language; he wants to take advantage of this to build his program
incrementally using the teacher's provided incremental verifier. As a
first attempt, he wants to verify $P_1$:
$$
\recb{\s\z}{\s\z} x y {\s {\var x}}\ \text{.}
$$
For this, he sends the command $\finfer{P_1}$ to the system. The
$\finfer{}$ operator takes a well-formed program and tries to construct
a valid typing derivation for its argument in the current, empty
context. The system succeeds and returns a typing derivation $\md_1$
of \/ $\vdash P_1 : \cst{nat}$, valid in an empty context.

The student then decides to factorize a definition for $\var{add}$ out
of his code:
\begin{align*}
  &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
    {\var y} u v {\s{\var u}}}
  \app{\var{add}}\app{(\s\z)}{(\s\z)} \ \text{.}
\end{align*}
But he (or his text editor) realizes that some parts of this program
have not changed: both $\s\z$, and the function body $\s{\var u}$, and
he could reuse the pieces of derivation just built. Let us call these
respectively $\md_2$, $\md_3$ and $\md_4$. $\md_4$ is hypothetical: it
depends on the hypothesis $H$ that\ \ $\vdash \var u:\cst{nat}$ .
Provided there is an operator $\fget\md$ from the derivation to the
program it types, what really needs to be sent to the system is:
\begin{align*}
  &\finfer{}(
    \letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
      {\var y} u v {\fget{\gsubst{\md_4}{\msubst{H}{\finfer{\var u}}}}}}
    \app{\var{add}}\app{\fget{\md_2}}{\fget{\md_2}}
  )
\ \text{,}
\end{align*}
that is, only the changed subterms of the program. Derivation $\md_2$
used to live in the empty context, we are then free to use it in the
new context where $\var{add}:\cst{nat}\to\cst{nat}$, because System
\sysname T enjoys the \emph{weakening} property; $\md_4$ depending on
a derivation\ \ $\vdash\var x:\cst{nat}$, we instantiate this hole
with the derivation $\finfer{\var u}$ inferred recursively for the new
recursive argument $\var u$. $\md_2$ derives $\vdash \s\z :
\cst{odd}$, but $\var{add}$ awaits two $\cst{nat}$ arguments. When the
system generates this derivation, it inserts a coercion $\cst{odd}
\leq \cst{nat}$ at these two points. Taking advantage of $\md_2$ and
$\md_4$ the system succeeds with a new derivation made out of these,
without having to regenerate them because it knows that
$\finfer{\fget\md} = \md$.

Let us call $\md_5$ the subderivation generated for $\tlam x
{\cst{nat}} \tlam y {\cst{nat}} \recb {\var x} {\var y} u v
{\fget{\gsubst{\md_4}{\msubst H {\finfer{\var u}}}}}$, $\md_6$ for the
arguments $\app{\fget{\md_2}}{\fget{\md_2}}$ and $\md_7$ for the call
$\app{\var{add}} {\fget{\md_6}}$. Even if it occurs originally in the
scope of $\var{add}$, $\md_6$ doesn't depend on the hypothesis $\var
H$ that $\vdash \var{add}:\cst{nat}\to\cst{nat}$ because of the
\emph{strengthening} property. Contrarily, $\md_7$ does: each time we
want to reuse it, we will have to provide a proof of this fact.

The student then provides a definition for multiplication, and writes
the operation $(2 + 2)\times3$:
\begin{align*}
  \finfer{}(
  &\letb{add}{\fget{\md_5}}
  \\
  &\quad\letb{mul} { \tlam x {\cst{nat}} \tlam y
    {\cst{nat}} \recb \z {\var y} z t {
      \app{\var{add}}\app{\var x} {\var z}} }
  \\
  &\quad\quad\app{\var{mul}}\app
  {(\fget{\gsubst{\md_7}{\msubst H {\finfer{\var{add}}}}})}
  {(\s{\fget{\md_2}})}
  )
  \ \text{.}
\end{align*}
Here again, he took care of reusing all derivations for equal
subterms. In particular, $\md_7$ expects a proof that the variable
$\var{add}$ is of type $\cst{nat}\to\cst{nat}$; he provides
$\finfer{\var{add}}$ meaning: at this point we will have generated (or
reused in this case) the derivation of $\var{add}$, so we just have to
use it here. $\md_2$ was deriving $\vdash\s\z:\cst{nat}$ (with a
coercion since it was used as an argument to $\var{add}$); the system
thus infer the more general type $\cst{nat}$ for $\s{\fget{\md_2}}$.

Finally, he decides (in an odd move) to inline the definition of
$\var{mul}$ into its call site. Call $\md_8$ the derivation of the
definition of $\var{mul}$ (it depends on an $\var H$ of
$\vdash\var{add}:\cst{nat}\to\cst{nat}$) and $\md_9$ the derivation of
$(2+2)\times 3$ depending on derivations $\var{H}$ and $\var{H'}$ for $+$ and
$\times$. He orders:
\begin{align*}
\finfer{}(
\letb{add}{\fget{\md_5}}
\fget{({\gsubst{\md_9} {\msubst {H} {(\fget{\var{add}})}; \msubst {H'}
      {\gsubst{\md_8}{\msubst H {(\fget{\var{add}})}}}}})}
)
  \ \text{.}
\end{align*}
The definition for $\var{add}$ is unchanged, but the instantiation of
open derivations enables to directly substitute one into the other and
get the appropriate program: call $\md_{10}$ the whole derivation just
returned, $\fget{\md_{10}}$ returns:
\begin{align*}
  &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
    {\var y} x y {\s{\var x}}}
  \\&\quad
  {(\tlam x {\cst{nat}} \tlam y
    {\cst{nat}} \recb \z {\var y} z t {
      \app{\var{add}}\app{\var x} {\var z}})}
  \\&\quad\quad
  \app{(\app {\var{add}} \app {(\s{\s\z})}
    {(\s{\s\z})})}{(\s{\s{\s\z}})}
  \ \text{.}
\end{align*}

\paragraph{Discussion}

We choose here to represent deltas as usual terms with variables $\var
x, \var y$ and special, \emph{meta}-variables referring to
already-computed results $\md_i$. Commands sent to the system consists
of terms, possibly containing special typed operators having a
computational content: $\finfer{\cdot}$ takes a term $M$ and produces
the (dependent) pair of a type $A$ and a derivation of the judgment
$\vdash M:A$ in the current context; $\fget{\cdot}$ does the exact
converse: it takes a derivation $\vdash M:A$ and projects it back as
the term $M$. The successful result of a command execution is a
derivation where each subderivation has been given a unique
metavariable name for later reuse (a \emph{repository}). As
subderivations can have free variables, we close them by a local
environment that must be instantiated by a \emph{substitution}.

Although this small example may seem anecdotal, the interest of such a
mechanism becomes clear on very large programs: then the cost of
rechecking well-typing of a small modification amounts to rechecking
the path in the term from the root to the changed part, and the
changed part itself.

%TODO figure?

Seeing type-checking as issuing a completely explicit typing
derivation enables thus to think of the relationship between the input
and the output. Feeding subterms of the output $\md_i$ back into
subterms of the input $P_i$ is a well-known technique for handling
changes in the input: combined with a way to automatically recognize
already-computed input, it is \emph{memoization}, a general-purpose
technique for incremental computation that has been extensively
studied and combined with other techniques
\cite{pugh1989incremental,acar2003selective}. However, it is best
understood in the case of purely first-order data, and to our best
knowledge remains unexplored for higher-order structures containing
binders, like programs and proofs.

A memoized type-checker stores in a table bindings from its input (the
program) to its output (the derivation), and returns automatically the
stored output if the current input matches a binding in the table. It
thus performs two operations at once: output reuse and recognition of
already-seen input. Recognition --- that is from a modified program,
generate a delta from previous versions --- is in the higher-order
case a complex task that we hope to address in the future; we tackle
here the verification of output reuse: given a delta, construct a new
derivation and verify it.

\subsection{A language to write incremental type-checkers}

A few days earlier, the teacher is preparing the tool for the students
to train composing System \sysname{T_{<:}} programs. He (or she)
provides the abstract syntax, the typing rules and the untrusted
typing algorithm in order to build the incremental type checker. How
should he write the algorithm? We know that we want to return a
derivation, and take advantage of a higher-order representation of the
syntax, so it shouldn't be necessary to thread any explicit
environment. The definition should then start with:

\begin{mathleft}
  \finfer{}\ :\ \prd M {\cst{tm}} \sig A {\cst{tp}} (\vdash M : A) =
\end{mathleft}

\noindent
Function $\finfer{}$ takes a term $M$ and produces the pair of an
inferred type $A$ and a derivation that $M$ has type $A$. Its code
will be untyped, but the teacher provides this type so that the
checker can verify that each call is fed and returns values of the
right type: it is its \emph{contract} so to say. He first decomposes
term $M$ and treats the easy cases of $\cst{o}$ and $\cst{s}$:

\begin{mathleft}
  \lamd M \match{M} \\
  \quad\caseb{\z} \pair {\cst{even}}{\infer{ }{\vdash \cst o : \cst{even}}} \\
  \quad\caseb{\s M}
  \match{\finfer M} \\
  \quad\quad\caseb{\pair{\cst{even}}\md}
  \pair {\cst{odd}}{\infer\md{\vdash\app{\cst{s}} M : \cst{odd}}} \\
  \quad\quad\caseb{\pair{\cst{odd}}\md}
  \pair {\cst{even}}{\infer\md{\vdash\app{\cst{s}} M : \cst{even}}} \\
  \quad\quad\caseb{\pair{\cst{nat}}\md}
  \pair {\cst{nat}}{\infer\md{\vdash\app{\cst{s}} M : \cst{nat}}} \\
\end{mathleft}

\noindent
We write $\pair{\cdot}{\cdot}$ the constructor of $\Sigma$-types;
patterns don't have to be exhaustive since we are in an untrusted
setting; for conciseness, we use case failure for signaling typing
error. There are three cases for $\s{M}$ in the system, corresponding
to the three rules in the declarative system. For the application
case, we rely on the well-known property that we may need to insert a
coercion from the actual type of the argument to its expected type:

\begin{mathleft}
  \quad\caseb{\app{M}N} \\
  \quad\quad\letd {\pair {A_1\to B} {\md_1}} {\finfer{M}} \\
  \quad\quad\letd {\pair {A_2} {\md_2}} {\finfer{N}} \\
  \quad\quad\letd {\md_{\leq}} {\fleq {A_1} {A_2}} \\
  \quad\quad
  \match{\md_{\leq}} \\
  \quad\quad\quad\caseb{\infer{ }{\vdash A\leq A}}
  \pair {B} {\infer{\md_1 \and \md_2}{\vdash \app M N : B}} \\
  \quad\quad\quad\caseb{\_}
  \pair {B} {\infer{\md_1 \and \infer*{\md_2 \and \md_{\leq}}{\vdash N
      : A_1}}{\vdash \app M N : B}}
\end{mathleft}

\noindent
The $\syntax{let}$ construct is just syntactic sugar for a one-branch
$\syntax{case}$; we use an auxiliary function $$\fleq{}{}\ :\ \prd A
{\cst{tp}} \prd B {\cst{tp}} \vdash A\leq B = \ldots$$ trying to
construct a subtyping derivation for its argument, or failing with an
error. Notice the slight optimization to the size of the derivation:
if the types $A_1$ and $A_2$ are the same syntactically, we don't
generate a subtyping rule. This is done by matching on the returned
derivation $\md_\leq$.

Then comes the abstraction case $\tlam x A M$. The difficulty is that
the recursive call $\finfer{M}$ must be done in an enlarged
environment where $\vdash x : A$, otherwise the check that the
returned open derivation $\md$ is correct would fail ($x$ is
unknown). We thus introduce a new construct $\matchin M \Gamma \ldots$
and its $\syntax{let}$ counterpart ($\match M \ldots$ being syntactic
sugar for $\matchin M \envnil \ldots$). Even then, in the recursive
call, the variable case $\var x$ would fail since we would have no way
of telling its type anymore. Thus we substitute all occurrences of
$\var x$ with the \emph{output} of function $\finfer{}$ on it (since
it is known at this point), \ie\ a pair of $A$ and $\md_1$. For this
substitution to be well-typed, we now need to coerce this output to a
valid input (a term). We introduce an operator which is in some sense
the \emph{inverse} of $\finfer{}$: operator $\fget{}$ maps a pair of a
type and a derivation to the term it types.

\begin{mathleft}
  \quad\caseb{\tlam x A M} \\
  \quad\quad
  \letdin{\pair B \md}{\env {$\md_{\var x}$} {(\vdash {\var x} : A)}}
  {\finfer {\gsubst M {\msubst x {\fget{\pair A {\md_{\var x}}}}}}}
\end{mathleft}

At this point, we get back $B$ and $\md$ from the recursive call. We
know that $B$ can't depend on $\md_x$ (because a System
\sysname{T_{<:}} type is always closed), but $\md$ can: it is not
\emph{per se} a hypothetical derivation but an \emph{open} derivation
where a special judgement named $\md_x$ can occur. To return the
complete derivation of $\vdash\tlam A x M : A \to B$, we must then
close, or ``hypothesize'' $\md$ by $\md_x$, and then apply rule
\rulename{Lam}:

\begin{mathleft}
  \quad\quad
  \pair {A\to B}
  {\infer{
\mbox{$      \begin{array}{c}
        {[\md_{\var x}]} \\
        {\md}
      \end{array}
    $}    }{
    \vdash\lam x M : A\to B
  }}
\end{mathleft}

Concerning the treatment of environments, the case of $\recb M N x y
P$ is similar: we first compute recursively the derivations $\md_M$
and $\md_N$ which should be well-typed in the current environment, and
then $\md_P$ in the enlarged environment, with variables $\var x$ and
$\var y$ in $P$ substituted with the (future) result of $\finfer{}$.

Remains one problem with subtyping: terms $N : A_N$ and $P : A_P$
should be coercible to the same type $A$, but $P$ should be typed
under $\vdash y:A$, which is not known before typing $P$. A trick to
break the loop is to iterate the typing of $P$ with more and more
general types, until we reach a fixpoint (we reach a fixpoint if there
is a unique more general type); in our particular case of subtyping
relation, this means iterating only twice: we will then attain
\cst{nat}.

\begin{mathleft}
  \quad\caseb{\recb M N x y P} \\
  \quad\quad
  \letd {\pair {A_M} {\md_M}} {\finfer{M}} \\
  \quad\quad\letd {\md_{A_M}} {{A_M} \leq \cst{nat}} \\
  \quad\quad
  \letd {\pair {A_N} {\md_N}} {\finfer{N}} \\
  \quad\quad
  \syntax{let} {\pair {A_P} {\md_P}} \syntax{in}
  {(\envcons {\env {$\md_x$} {(\vdash
        \var x : \cst{nat})}} {$\md_y$} {(\vdash\var y : A_N)})} = \\
  \quad\quad\quad
  {\finfer{\gsubst P {\msubstcons {\msubst x
          {\fget{\pair{\cst{nat}}{\md_x}}}} y
        {\fget{\pair{A_N}{\md_y}}}}}} \syntax{in}\\
  \quad\quad\letd {\pair A {\pair{\md_{A_N}}{\md_{A_P}}}} {{A_N} \sqcap {A_P}} \\
  \quad\quad
  \syntax{let} {\pair {\_} {\md_P}} \syntax{in}
  {(\envcons {\env {$\md_x$} {(\vdash
        \var x : \cst{nat})}} {$\md_y$} {(\vdash\var y : A)})} = \\
  \quad\quad\quad
  {\finfer{\gsubst P {\msubstcons {\msubst x
          {\fget{\pair{\cst{nat}}{\md_x}}}} y
        {\fget{\pair{A}{\md_y}}}}}} \syntax{in}\\
  \hspace{-2em}\pair {A} {
    \infer{
      \infer*{\md_M \and \md_{A_M}}
      {\vdash M : A} \and
      \infer*{\md_N \and \md_{A_N}}
      {\vdash N : A}\and
      \infer*{\infer*{}{\mbox{$
        \begin{array}{c}
          [\md_x] [\md_y] \\
          \md_P
      \end{array}
        $}} \and \md_{A_P}}{\vdash P : A}
    }{
      \vdash \recb M N x y P : A
    }
  }
\end{mathleft}

\noindent
Again, we use a new auxiliary function
$$\sqcap\ :\ \prd{A}{\cst{tp}} \prd{B}{\cst{tp}} \sig C {\cst{tp}}
(\vdash A\leq C )\times(\vdash B\leq C) = \ldots$$ computing the sup
of two types $A$ and $B$, and returning proofs of of subtyping. Note
that we didn't perform here the optimization seen in the application
case.

The type-checker is now done. There is no case for variables, as we
won't ever meet this case: we already substituted all variables with
their result derivation. Evaluation only need to make sure that in
this case, the derivation will be returned, \ie\ that it reduces the
following way:
$$
\finfer{\fget{\pair A \md}} = \pair A \md
$$
Now $\fget{}$ can be derived automatically from the type of
$\finfer{}$: it is its inverse. It has definition:

% TODO confirmer qu'on met bien la def. des f^0
\begin{mathleft}
  \fget{}\ :\ \prd{\{M\}}{\cst{tm}} \prd {A}{\cst{tp}} \prd{\md}{(\vdash
    M:A)} \cst{tm} =
  \\\qquad\quad \lamd M \lamd A \lamd \md M
\end{mathleft}

\noindent
where argument $M$ is implicit.

\paragraph{Discussion}

We exploit here an idea similar to run-time \emph{contracts}
\cite{wadler2009well}: there is no static guarantee that computations
won't return ill-typed values, but arguments and results are checked
at run-time at the boundaries of defined functions. This allows to
omit formal justifications, like the fact that we need to iterate
twice the typing of $P$ in $\recb M N x y P$, or that $B$ can't depend
on $\md_x$ in the $\lambda$ case. Of course this incurs on the
performance; we will see a compensation for it by the slicing
mechanism described below in \ref{sec:repr}.

The idea of untrustedly generating certificates verified \emph{a
  posteriori} is very wide-spread in \sysname{LCF}-style proof
assistants: during manual proof search, the interaction is driven by
\emph{tactics}: %TODO cite
they are untrusted programs generating pieces of proofs, assembled by
other, higher-order tactics; verification of the generated proofs is
done when the evaluation of all tactics is complete. The differences
with our mechanism are two-fold: first, tactics usually take one (or
several) \emph{goal}(s) as input, \ie\ holes in a proof, whereas our
functions can take any term; secondly, proof-check is usually done at
the end of the evaluation, which can lead to less tractable bugs; by
making evaluation and checking mutually recursive, we choose to detect
errors earlier in these witnesses: exactly at the point where they are
generated.

Introducing the ``inverse'' of the derivation inference function has
two purposes: as we saw in \ref{sec:use-incremental}, it allows to
refer to already-constructed derivations when writing deltas. It also
permits to abstract from the concrete environment when writing the
type-checker itself, that one must thread throughout the process in a
traditional, first-order type-checker. It is a generalization of the
so-called \emph{context-free} typing presented in
\cite[chap. 4]{boespflug2011conception} where a special variable
construct $\{x:A\}$ denoting free variables is added to the syntax of
terms, and the rules
$$
\infer{
  \vdash \gsubst M {\msubst x {\{\var x:A\}}} : B
}{
  \vdash \tlam x A M : A\to B
}
\qquad\text{and}\qquad
\infer{ }{
  \vdash \{\var x : A\} : A
}
$$
% TODO: vérif ce paragraphe après avoir parlé de fcts inv
replace the usual ones. The difficulty with this technique,
particularly in a dependent setting, is to ensure that term $\{\var
x:A\}$ reduces to $\var x$ so that they are considered equal, \ie\ we
can strip off the annotations, while not forgetting them too soon:
otherwise we would arrive to the unhandled case $\finfer{\var
  x}$. This requires evaluating the type-checker with a carefully
designed strategy (see \ref{sec:comput}).

% TODO cite lee2007mechanizing

\section{Here is my idea}
%% Here is my idea (2 pages)

% lambda-lifting

\subsection{The representation language}
\label{sec:repr}

The emission of certificates by untrusted applications first raises
the question of the \emph{data structure} to use for these
certificates. The requirements are:
\begin{itemize}
\item conciseness of the certificates, since they are stored or even
  transferred remotely;
\item a well-understood, short and independent verification algorithm.
  This is known as the \emph{de Bruijn principle};
\item universality with respect to the particular problem: having a
  certificate language for each domain of application would just shift
  the problem of trust; having one universally accepted certificate
  language that can be tailored to many problems puts a much stronger
  trust on the unique verifier.
\end{itemize}

The Edinburgh Logical Framework \cite{harper1993framework}, or \LF\
for short, is a well-accepted solution to this question. It is a typed
lambda-calculus and can be viewed as a language for
\emph{representing} terms, derivations and statements of $\Pi_0^1$
meta-theorems (but not their proofs) in a given inference system
supporting hypothetical judgments \cite{pfenning2001logical}. For
these, it provides built-in support for encoding languages with
binders and hypothesis (a feature called $\lambda$-tree syntax or HOAS
\cite{pfenning1988higher}). The user gives a \emph{signature} defining
abstract syntax and inference rules of his \emph{object language}, and
gets an algorithm to verify terms and derivations expressed in the
\LF\ syntax.

For instance, the rule for System \sysname{T}'s recursor can be
encoded by the following constant:

\begin{mathleft}
  \cst{is\_rec}\ :\
  \prd{M}{\cst{tm}}\prd{N}{\cst{tm}}\prd{P}{\cst{tm}\to\cst{tm}\to\cst{tm}}
  \\\quad
  \prd A {\cst{tp}}
  \app {\cst{is}}\app M {\cst{nat}} \to \app{\cst{is}}\app N A \to \\\quad
  \left(
  \prd x {\cst{tm}} \prd y {\cst{tm}}
  \app{\cst{is}}\app x \cst{nat} \to \app{\cst{is}}\app y A \to
  \app{\cst{is}}\app {(\app P \app x y)} A
  \right)\to\\\quad
  \app{\cst{is}}\app{(\app{\cst{rec}}\app M \app N (\lam x\lam y \app
    P\app x y))} A
\end{mathleft}

This framework has been used to issue certificates for a
number of practical applications like proof-carrying code
\cite{necula1997proof}, authentication framework \cite{appel1999proof}
\ldots %TODO ATP

To write deltas by sharing subterms, we need to be able to address any
subterm and instantiate its free variables. For this, we define the
\emph{slicing} of a well-typed \LF\ term: we extend \LF\ with
contextual metavariables $\meta X$, $\meta Y$\ldots\ as in
\cite{nanevski2008contextual}. Metavariables stand for well-typed
terms defined in a repository, and are instantiated wrt. their free
variables. A repository also contains a distinguished metavariable
corresponding to the ``tip'' of the term stored in it. For instance, the
repository
$$
\small
\left(\begin{array}{llll}
    \meta X &\mapsto
    \envnil&\vdash
    \app{\cst{lam}} \lam x \smeta Y {\msubst u {\smeta T {\msubst w x}}} &:
    \cst{tm}
    \\
    \meta Y &\mapsto
    \env u {\cst{tm}} &\vdash
    \app{\cst{lam}} \lam y \smeta Z {\msubstcons {\msubst u
      {\var u}} v {\var y} } &:
    \cst{tm}
    \\
    \meta Z &\mapsto
    \envcons {\env u {\cst{tm}}} v {\cst{tm}} &\vdash
    \app{\app{\cst{app}}{\var v}}{\var u} &:
    \cst{tm}
    \\
    \meta T &\mapsto
    \env x {\cst{tm}} &\vdash
    \app{\cst{app}}\app{\var x}{\cst{o}} &:
    \cst{tm}

\end{array}\right), \smeta X {}
$$
is one of the possible way of slicing of the term
$$
\fbox{$
  \app {\cst{lam}} \lam x
  \fbox{$
    \app {\cst{lam}} \lam y
    \fbox{$
      \app {\cst{app}} \app {\var y}
      (\fbox{$
        \app{\cst{app}}\app{\var x}{\cst o}
      $})
    $}
  $}
$}
$$
where all boxed subterms are referable slices.

\subsection{The computational calculus}
\label{sec:comput}

% inverse

\section{The calculus}
%% My idea works (details, data) (5 pages)

\subsection{Value calculus}

The value calculus defines the \emph{values} we want to
manipulate. These contain constants, and --- contrarily to \eg\
\sysname{ML} values --- binders and variables enabling $\lambda$-tree
syntax. Thus, they embed a superficial notion of computation:
hereditary substitution. This calculus is an extension of \emph{spine
  canonical \LF} \cite{pfenning2007term} enriched with metavariables
which are references to open terms as in
\cite{nanevski2008contextual}. We present its typing algorithm
relatively to a signature. It features an explicit caching mechanism
using metavariables.

\paragraph{Syntax}

In this variant, only $\eta$-long and $\beta$-normal values have an
existence: canonicity for $\beta$-reduction is enforced in the syntax
and canonicity for $\eta$-expansion will be enforced by typing,
following \cite{hl07mechanizing}. Besides, application of a
\emph{head} (\ie\ a constant or variable) is $n$-ary (\ie\ to a
\emph{spine} of objects) as opposed to the more standard binary
application: this eases the definition substitution and
$\eta$-expansion, and corresponds to a focused sequent calculus known
as \sysname{LJT} \cite{herbelin1995λ}.

\begin{align*}
  K &\gequal
  \prd x A K \gor
  \type &
  \text{Kind}\\
  A &\gequal
  \prd x A A \gor
  P &
  \text{Type family} \\
  P &\gequal
  \app {\cst a} S &
  \text{Atomic type} \\
  M &\gequal
  \lam x M \gor
  F &
  \text{Canonical object} \\
  F &\gequal \app H S
  \gor
  \smeta X \sigma &
  \text{Atomic object} \\
  H &\gequal
  \var x \gor
  \cst c &
  \text{Head}\\
  S &\gequal
  \spinenil \gor
  \spinecons M S &
  \text{Spine}\\
  \sigma &\gequal
  \msubstnil \gor
  \msubstcons \sigma x M &
  \text{Parallel substitution} \\
  \Gamma &\gequal
  \envnil \gor
  \envcons \Gamma x A &
  \text{Environment}
\end{align*}

As is customary, we adopt the Barendregt convention for naming bound
variables, write $A\to A'$ for $\prd x A A'$ when $\var x\notin \FV{A'}$,
$\meta X$ for $\smeta X \msubstnil$, and adopt indifferently list, map
or set notations for $\sigma$ and $\Gamma$. The identity substitution
$\msubstid{\Gamma}$ is a notation for $\{\msubst x {\var x}\ |\ \var x
\in\dom\Gamma\}$.

Metavariables $\smeta X \sigma$ stand for open objects with free
variables in $\dom{\sigma}$; To each use of them is attached a
substitution $\sigma$ defining these free variables. They are defined
in a repository (see below). We write $\FMV{M}$ for the set of
metavariables in $M$ and speak of a \emph{metaclosed} object $M$ if
$\FMV{M} = \emptyset$.

We distinguish syntactically between \emph{checkable} $M$ and
\emph{inferrable} $F$ objects (resp. \emph{canonical} and
\emph{atomic}), an idea dating back to \cite{pierce2000local}:
canonical objects are checked against a given type, whereas we can
synthesize the type of atomic objects; the coercion from the first
grammatical category to the second is where we verify equality between
the synthesized and the checked types.

\paragraph{Substitution}

We overload \emph{substitution} as a partial operation on objects,
given a parallel substitution $\sigma$. Since the syntax of object is
not stable by (textual) substitution, it is defined as
\emph{hereditary}\footnote{ Note that this notion of substitution,
  present in \cite{pfenning2007term}, differs from the more standard
  definition in \cite{hl07mechanizing} as it presupposes $\eta$-long
  normal forms: once a \emph{cut} starts, there must be exactly as
  many formal and actual arguments. A non-empty spine cut against
  another non-empty spine $\gcut {\app H S} {S'}$ blocks the
  substitution instead of reducing to $\app H {(\gconcat S S')}$ (an
  implicit $\eta$-expansion); conversely, exhausting actual but not
  formal arguments as in $\gcut{\lam x M}{\gnil}$ blocks the
  substitution too. For example, the reduction $\gcut {\lam x \app
    {\var x}{\var x}} {\lam x \app {\var x}{\var x}}$ gets stuck
  because the second $\var x$ in the left part is substituted by a
  function yet is not itself a function. }: it is mutually recursive
with a \emph{cut} function $\gcut M S$ which can in turn trigger a
chain of substitutions, leading to a canonical objects or an error.
\begin{align*}
  \gsubst {(\lam x M)} \sigma &= \lam x {\gsubst M \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\cst c} S)} \sigma &= \app {\cst c} {(\gsubst S
    \sigma)}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \app {\var x} {\gsubst S \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \gcut M {\gsubst S \sigma} &
  \text{if $\sigma(\var x) = M$}
  \\
  \gsubst {\smeta X {\sigma}} {\sigma'} &=
  \smeta X {\gcomp\sigma{\sigma'}}
  \\
  \gsubst\spinenil\sigma &=
  \spinenil \\
  \gsubst{\spinecons M S}\sigma &=
  \spinecons{\gsubst M\sigma}{\gsubst S\sigma}
  \\
  \gcut {\lam x M} {\spinecons N S} &=
  \gcut {\gsubst M {\msubst x N}} S
  \\
  % TODO cas meta/S ? rem: on en a pas besoin ds le proto.
  % \gcut {\smeta X\sigma} S &=
  % \smeta X{\gcomp\sigma\sigma'}
  % \\
  \gcut {\app H S} \spinenil &=
  \app H S
  \\
  \gcut {\smeta X\sigma} \spinenil &= \smeta X\sigma
  \\
  \gcomp \sigma \msubstnil &=
  \sigma
  \\
  \gcomp \sigma {(\msubstcons{\sigma'} x M)} &=
  \gcomp {\gsubst\sigma {\msubst x M}} \sigma'
  \\
  \gsubst {(\msubstcons{\sigma'} x M)} \sigma &=
  \msubstcons{\gsubst{\sigma'}\sigma} x {\gsubst M \sigma}
  \\
  \gsubst {\msubstnil}{\sigma} &= \msubstnil
\end{align*}

We extend this operation trivially on families $\gsubst A \sigma$ and
kinds $\gsubst K \sigma$. We also allow to abusively write
non-canonical application $\app M S$ as a shorthand for $\gcut M S$
if it is defined.

% TODO pourquoi cette def?
% TODO terminaison?
% TODO stabilité de la eta?

\paragraph{Typing algorithm}

Typing an object while storing intermediate derivations requires
threading and maintaining a data structure of named partial
derivations (\emph{slices}) throughout the typing process. We present
a bidirectional typing algorithm updating an input repository.

A \emph{signature} declares object and type constants to be used in
objects. To each object constant we assign an annotation $L$ setting
the slicing behaviour of the constant during type checking: either
sliceable $\annsliceable$ (each application of this constant will be
cached and given a name), non-sliceable $\annnonsliceable$ (we don't
create a name for applications of this constant), or defined
$\anndefined{T}$ (see \ref{sec:computational-calculus}).
\begin{align*}
  L &\gequal
  \annsliceable \gor
  \annnonsliceable \gor
  \anndefined{T}
  \\
  \Sigma &\gequal
  \gnil \gor
  \gcons \Sigma {\cst c :^L A} \gor
  \gcons \Sigma {\cst a : K}
  &\text{Signature}
\end{align*}
We write $\Sigma(\cst c) = A; L$ for $(\cst c :^L A) \in \Sigma$
suggesting the implementation of signatures as maps.

Our algorithm accepts exactly the well-typed objects of the usual \LF,
except that we thread a repository $\mr$ and store in it all
well-typed applicative subterms $F$ along with their classifier $A$
and the environment in which they are well-typed.

% TODO parler d'effacer les R et retomber sur LF
% TODO expl. c'est tout comme LF sauf qqes règles...
% TODO parler de la redondance en LF normal

A \emph{repository} $\mr$ is a finite map from metavariable to
judgments of atomic object well-typing, along with a distinguished
metavariable, corresponding to the \emph{tip} of the term contained in
it:
$$ \mr\;:\;(\meta X \mapsto (\Gamma\vdash\tp F P)), \smeta X
\sigma $$
The algorithm will maintain the invariant that a repository
$\mr$ is \emph{acyclic}, that is for all $\meta X$ in $\mr$, the
fixpoint of $\FMV{\meta X}$ does not contain $\meta X$. If $\mr =
(\Delta, X[\sigma])$, we write $\hat {\mr}$ for $X[\sigma]$, abusively
$\mr$ for $\Delta$ and $\mr (\smeta X \sigma)$ for $\gsubst M\sigma$
if $\Delta(\meta X) = (\Gamma\vdash M : A)$.

We refer to cached subterms by metavariables. We force instantiation
of all variables in their environment by attaching a substitution to
them ($\smeta X \sigma$).

When checking large objects, the environment can grow fast. As an
optimization to reduce the size of the stored environments and
substitutions in slices $M$, we strengthen environments to their
minimum size (the free variables of $M$): We define the operation of
\emph{strengthening} from an environment $\Gamma$ and an object $M$
well-typed in $\Gamma$ to the minimal environment in which $M$ can
actually be typed, \ie\ containing only its free variables:
$$
\stren{\Gamma,M} = \{x : A\ |\ x\in \FV{M} \wedge \Gamma(x)=A\}
$$
% TODO lemme qui dit que c'est correct wrt variables libres des A et
% du type

The algorithm is presented in Fig. \ref{fig:obj-typing} as a
syntax-directed inference system. All judgments are of the form $\gjo
U V W$ where $U$ and $V$ are inputs, $V$ is the induction hypothesis
and $W$ the output. They are parameterized by a constant and implicit
signature $\Sigma$, and all premises are ordered in the algorithmic
order of computation.

\begin{figure*}

  \fbox{$\jm\Delta\Gamma M A {\Delta'} {M'}$}
  \qquad
  Canonical object

  \begin{mathpar}
    \infer[MLam]{
      \jm\Delta{\envcons\Gamma x A} M {A'} {\Delta'} {M'}
    }{
      \jm\Delta\Gamma {\lam x M} {\prd x A {A'}} {\Delta'} {\lam x M'}
    }

    \infer[MAtom]{
      \jf\Delta\Gamma F {\Delta'} {F'} {P}
       \and
      \jea{\Delta'}\Gamma P {\app{\cst a} S}
    }{
      \jm\Delta\Gamma F {\app{\cst a} S} {\Delta'} {F'}
    }
    % TODO commenter type ground
  \end{mathpar}

  \fbox{$\jf\Delta\Gamma F {\Delta'} {F'} P$}
  \qquad
  {Atomic object}

  \begin{mathpar}

    % Non-sliceable
    \infer[FAppNoSlice]{
      \jh\Delta\Gamma H A \annnonsliceable
      \and
      \jl\Delta\Gamma A S {\Delta'} {S'} P
    }{
      \jf\Delta\Gamma {\app H S} {\Delta'} {\app H {S'}} P
    }

    % Meta
    \infer[FMeta]{
      \Delta(\meta X) = (\Gamma'\vdash F:P)
      \and
      \js\Delta\Gamma \sigma {\Gamma'} {\Delta'} {\sigma'}
    }{
      \jf\Delta\Gamma {\smeta X \sigma} {\Delta'} {\smeta X {\sigma'}} {\gsubst{P}{\sigma'}}
    }

    % Sliceable
    \infer[FAppSlice]{
      \jh\Delta\Gamma H A \annsliceable
      \and
      \jl\Delta\Gamma A S {\Delta'} {S'} {P}
      \and
      \stren{\Gamma, \app H {S'}}=\Gamma'
    }{
      \jf\Delta\Gamma {\app H S} {\mapadd{\Delta'}{\meta X}{(\Gamma'\vdash
          {\app H {S'}}:P)}} {\smeta X{\msubstid{\Gamma'}}} P
    }
    \quad\text{($\meta X$ fresh in $\Delta'$)}

    % Defined
    \infer[FAppEval]{
      \jh\Delta\Gamma H A {\anndefined T}
      \and
      % check args pas nécessaire (fait ds la red)
      % \jl\Delta\Gamma A S {{\Delta'}} {S'} P
      % \and
      \jcc\Delta\Gamma A T S {\Delta''} F
      % check result pas nécessaire (fait ds la red)
      % \and
      % \jm{\Delta''}\Gamma F P {\Delta'''} {F'}
    }{
      \jf\Delta\Gamma {\app H S} {\Delta'''} {F'} P
    }

  \end{mathpar}

  \fbox{$\jh\Delta\Gamma H A L$}
  \qquad
  {Head}

  \begin{mathpar}
    % constant
    \infer[HConst]{
      \Sigma(\cst c) = A; L
    }{
      \jh\Delta\Gamma {\cst c} A L
    }

    % variable
    \infer[HVar]{
      \Gamma(\var x) = A
    }{
      \jh\Delta\Gamma {\var x} A \annnonsliceable
    }
  \end{mathpar}

  \fbox{$\jl\Delta\Gamma A S {\Delta'}{S'} P$}
  \qquad
  {Spine}

  \begin{mathpar}
    \infer[SCons]{
      \jm\Delta\Gamma M A {\Delta'} {M'}
      \and
      \jl{\Delta'}\Gamma {\gsubst{A'}{\msubst x {M'}}} S {\Delta''} {S'} P
    }{
      \jl\Delta\Gamma {\prd x {A} {A'}} {\spinecons M S} {\Delta''} {\spinecons
      {M'}{S'}} P
    }

    \infer[SNil]{ }{
      \jl\Delta\Gamma {\app{\cst a} S} \spinenil \Delta \spinenil {{\app{\cst a} S}}
    }
  \end{mathpar}

  \fbox{$\js\Delta\Gamma\sigma{\Gamma'}{\Delta'}{\sigma'}$}
  \qquad
  {Substitution}

  \begin{mathpar}

    % Cons
    \infer[$\sigma$Cons]{
      \js\Delta\Gamma\sigma{\Gamma'}{\Delta'}{\sigma'}
      \and
      \jm{\Delta'}\Gamma M {\gsubst A {\sigma'}} {\Delta''} {M'}
    }{
      \js\Delta\Gamma{(\msubstcons \sigma x M)} {(\envcons {\Gamma'} x {A'})}
      {\Delta''} {(\msubstcons {\sigma'} x {M'})}
    }

    % Nil
    \infer[$\sigma$Nil]{ }{
      \js\Delta\Gamma\msubstnil\envnil\Delta\envnil
    }

  \end{mathpar}

  \caption{Typing algorithm for objects}
  \label{fig:obj-typing}
\end{figure*}

The main judgement is atomic object typing $\jf\Delta\Gamma F
{\Delta'} {F'} P$, which means that in repository $\mr$ and environment $\Gamma$, the
applicative term $F$ is well-typed of type $A$, and the slicing of $F$
produces a new repository $\mmr$. The object $F'$ is the
\emph{residual} of this operation: only certain kind of applications
are sliced, and it is possible that the ``tip'' of application $F$ is
not sliced (because it is declared as non-sliceable). In that case,
$M$ is this tip; if $F$ is directly sliceable, then $M$ is the
metavariable referring to it.

These typing rules use definitional equality judgement between two
families $\jea\Delta\Gamma A {A'}$. This equivalence is usually defined
to be just $\alpha$-equivalence since objects are in
$\beta\eta$-canonical form. Here, objects can contain metavariables
defined in $\mr$, so we shall thread it. Moreover, we will see in the
next section how defined constants compute and thus influence
definitional equality. Definitional equality is defined later in Fig.
\ref{fig:def-eq}.

Signatures themselves need to be checked. We present the checking
algorithm in Fig. \ref{fig:sign-typing}.

\begin{figure*}
  \fbox{$\jsig\Delta\Sigma{\Delta'}{\Sigma'}$}
  \qquad
  {Signature}

  \begin{mathpar}

    \infer{
      \jsig\Delta\Sigma{\Delta'}{\Sigma'}
      \and
      \jfam{\Delta'}\gnil {\!_{\Sigma'} A}{\Delta''} {A'}
    }{
      \jsig\Delta{\gcons\Sigma{{\cst c :^L A}}}{\Delta''}{\gcons{\Sigma'}{{\cst c :^L A'}}}
    }

    \infer{
      \jsig{\Delta'}\Sigma{\Delta''}{\Sigma'}
      \and
      \jkind\Delta\gnil {\!_{\Sigma'} K}{\Delta'} {K'}
    }{
      \jsig\Delta{\gcons\Sigma{{\cst a : K}}}{\Delta''}{\gcons{\Sigma'}{{\cst a : K'}}}
    }

    \infer{ }{
      \jsig\Delta\gnil\Delta\gnil
    }
  \end{mathpar}

  \fbox{$\jfam\Delta\Gamma A {\Delta'} {A'}$}
  \qquad
  {Family}

  \begin{mathpar}

    \infer{
      \jfam\Delta\Gamma {A_1} {\Delta'} {A_1'}
      \and
      \jfam{\Delta'}{\gcons\Gamma{\tp {\var x} {A_1'}}} {A_2} {\Delta''} {A_2'}
    }{
      \jfam\Delta\Gamma {\prd x {A_1} {A_2}} {\Delta''} {\prd x {A_1'} {A_2'}}
    }

    \infer{
      \Sigma(a) = K
      \and
      \jfl\Delta\Gamma K S {\Delta'} {S'}
    }{
      \jfam\Delta\Gamma {\app{\cst a} S} {\Delta'} {\app{\cst a} {S'}}
    }
  \end{mathpar}


  \fbox{$\jfl\Delta\Gamma K S {\Delta'} {S'}$}
  \qquad
  {Family spine}

  \begin{mathpar}
    \infer{
      \jm\Delta\Gamma M A {\Delta'} {M'}
      \and
      \jfl{\Delta'}\Gamma {\gsubst{K}{\msubst x {M'}}} S {\Delta''} {S'}
    }{
      \jfl\Delta\Gamma {\prd x {A} {K}} {\spinecons M S} {\Delta''} {\spinecons
      {M'}{S'}}
    }

    \infer{ }{
      \jfl\Delta\Gamma\type\spinenil\Delta\spinenil
    }
  \end{mathpar}

  \fbox{$\jkind\Delta\Gamma K {\Delta'} {K'}$}
  \qquad
  {Kind}

  \begin{mathpar}
    \infer{
      \jfam\Delta\Gamma A{\Delta'}{A'}
      \and
      \jkind{\Delta'}{\envcons\Gamma x A} K {\Delta''} {K'}
    }{
      \jkind\Delta\Gamma{\prd x A K}{\Delta''}{\prd x {A'}{K'}}
    }

    \infer{ }{\jkind\Delta\Gamma\type\Delta\type}
  \end{mathpar}

  \caption{Typing algorithm for signatures}
  \label{fig:sign-typing}
\end{figure*}

\paragraph{Commit}
% TODO commit

\paragraph{Example}

Let us work out an example.
% TODO example

\paragraph{Checkout}

We define the partial \emph{checkout} operation $\checkout{\mr}$
(resp. $\checkoutr{M}$, $\checkoutr{S}$) from a repository
(resp. object, spine) to a metaclosed object as unfolding the
definitions of all metavariables recursively:
\begin{align*}
  \checkout{\mr} &= \checkoutr{\hat{\mr}} \\
  \checkoutr{\smeta X \sigma} &= \checkoutr{\mr(\smeta X\sigma)}\\
  \checkoutr{\lam x M} &= \lam x \checkoutr{M} \\
  \checkoutr{\app H S} &= \app H {(\checkoutr{S})} \\
  \checkoutr{\spinecons M S} &= \spinecons {\checkoutr M} {\checkoutr S} \\
  \checkoutr{\spinenil} &= \spinenil
\end{align*}

A repository is said to be \emph{well-formed} if $\checkout {\mr}$
is defined.

% \begin{theorem}
%   If \/ $\jm\mr\Gamma M A \mmr {M'}$ then \/ $\jem\mmr\Gamma M {M'} A$
% \end{theorem}

% \begin{theorem}
%   If \/ $\jm\mr\Gamma M A \mmr {M'}$ then \/ $\jlfm\Gamma {(\checkoutrr
%     M)} (\checkoutrr A)$
% \end{theorem}

\subsection{The computation calculus}
\label{sec:computational-calculus}

We now turn to the definition of our computation calculus. As we saw,
code is attached to special, \emph{defined} constants in the signature
that manipulates \LF\ objects. We have the ability to construct
objects out of other, and deconstruct them by case analysis:
\begin{align*}
  T &\gequal
  \lam x T \gor
  U & \text{Term} \\
  U &\gequal
  F \gor
  \matchin U \Gamma C & \text{Atomic term}\\
  C &\gequal \casebnobar P U \gor
  C\ \caseb P U & \text{Branches}\\
  P &\gequal
  \app H {\var x} \ldots\ {\var x} &\text{Pattern}
\end{align*}

This language is un(i)typed, in that all \LF\ objects are viewed as
having the same implicit type $\cst{obj}$: there is no static
guarantee that terms produced by a constant $\cst c : A_1\to A_2$ will
have type $A_2$. It is also non-terminating: we could declare a
recursive constant $\cst c : \cst a\to \cst a = \lam x \app {\cst c}
{\var x}$. We expose here only the minimal building blocks of this
language, but really any language with algebraic datatypes would do
since we will be manipulating directly the first-order representation
of \LF\ objects. In fact, whatever the language is, you can think of
defined constants as mere black boxes taking and producing terms, and
being able to call other black boxes.

Nevertheless, in order to maintain the well-typing of the generated
term, we want to maintain the invariant that each time a piece of term
goes in or out of a black box, it is checked against its specification
(\ie\ its type). We thus maintain \emph{dynamic typing} of
computations: arguments and results are type-checked at run-time.

\paragraph{Substitution}

\begin{align*}
  \gsubst {(\lam x T)} \sigma &= \lam x {\gsubst T\sigma} \qquad
  \text{if $\var x\notin\dom\sigma$}\\
  \gsubst {(\matchin U \Gamma C)} \sigma &=
  \matchin {\gsubst
    U\sigma} {\gsubst\Gamma\sigma} {\gsubst C\sigma} \\
  \gsubst {(\casebnobar{\app H {\spinecons {\var x_1} {\spinecons \ldots {\var
          x_n}}}} U)} \sigma &= {\casebnobar{\app H {\spinecons {\var x_1} {\spinecons \ldots {\var
          x_n}}}} \gsubst U \sigma} \quad \text{if $\var x_i\notin\dom\sigma$} \\
\end{align*}

\begin{figure*}
  \fbox{$\jcc\Delta\Gamma T S A {\Delta'} F$}
  \qquad
  {Term evaluation}

  \begin{mathpar}
    \infer[TLam]{
      \jm\Delta\Gamma M {A_1} {\Delta'} {M'}
      \and
      \jcc{\Delta'}\Gamma{\gsubst {A_2} {\msubst x {M'}}}{\gsubst T {\msubst x {M'}}} S {\Delta''} F
    }{
      \jcc\Delta\Gamma{\prd x {A_1} {A_2}}{\lam x T}{\spinecons M S}
      {\Delta''} F
    }

    \infer[TAtom]{
      \jcr\Delta\Gamma U {\Delta'} F
      \and
      \jm{\Delta'}\Gamma F {\app {\cst a} S} {\Delta''} {F'}
    }{
      \jcc\Delta\Gamma {\app {\cst a} S} U \spinenil {\Delta''} {F'}
    }
  \end{mathpar}

  \fbox{$\jcr\Delta\Gamma U {\Delta'} F$}
  \qquad
  {Atomic term evaluation}

  \begin{mathpar}
    \infer[UVar]{ }{
      \jcr\Delta\Gamma{\app {\var x} S} \Delta {\app {\var x} S}
    }

    \infer[UConst]{
      \Sigma(c) = A; (\annsliceable\text{ or }\annnonsliceable)
    }{
      \jcr\Delta\Gamma{\app {\cst c} S} \Delta {\app {\cst c} S}
    }

    \infer[UEval]{
      \Sigma(c) = A; \anndefined T
      \and
      \jcc\Delta\Gamma T S A {\Delta'} F
    }{
      \jcr\Delta\Gamma{\app {\cst c} S} \Delta {\app {\cst c} S}
    }

    \infer[UCase]{
      \jcr\Delta{\gconcat\Gamma{\Gamma'}} U {\Delta'} {\app {H_i} {\spinecons
          {M_1}{\spinecons\ldots{M_{m_i}}}}}
      \and
      \jcr{\Delta'}\Gamma {\gsubst{U_i}{\msubst {x$_1$} {M_1} \ldots\
          \msubst {x$_{m_i}$} {M_{m_i}}}} {\Delta''} F
    }{
      \jcr\Delta\Gamma{\matchin{U}{\Gamma'}{
          (\casebnobar{\app {H_1}{\spinecons {\var x_{11}} {\spinecons
                \ldots\ {\var x_{1m_1}}}}}{U_1})\
          |\ \ldots\
          \caseb{(\app {H_n}{\spinecons {\var x_{n1}} {\spinecons
              \ldots\ {\var x_{nm_n}}}}}{U_n})
        }}{\Delta''} F
    }

  \end{mathpar}

  \caption{Evaluation of terms}
  \label{fig:whr}
\end{figure*}

\begin{figure*}
  \centering
  TODO
  \caption{Definitional equality}
  \label{fig:def-eq}
\end{figure*}

\section{Related work}
% 1-2 pages

\section*{Conclusion and future work}
% 0.5 pages

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{splncs}
\bibliography{../../english}


% \begin{thebibliography}{}
% \softraggedright
% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
% P. Q. Smith, and X. Y. Jones. ...reference text...
% \end{thebibliography}

\end{document}

%  LocalWords:  subtype
