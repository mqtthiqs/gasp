\documentclass{llncs}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{macros}
\usepackage{mathpartir}
\usepackage{url}

\allowdisplaybreaks[1]
\renewcommand{\floatpagefraction}{0.8}

\begin{document}

\title{Certifying, incremental type checking}

\author{Matthias Puech\inst{1,2} \and Yann R\'egis-Gianas\inst{1}}
\institute{Univ Paris Diderot, Sorbonne Paris Cit\'e, PPS, UMR 7126
  CNRS, $\pi r^2$, INRIA Paris-Rocquencourt, F-75205 Paris, France
  \and Department of Computer Science, University of Bologna, 40126
  Bologna, Italy}

\maketitle

\begin{abstract}
 %% 4 sentences:
 %% State the problem

  A lightweight way to design a trusted type-checker is to let it
  return a \emph{certificate} of well-typing, and check it a
  posteriori (for instance \sysname{Agda} and \sysname{GHC} adopt this
  architecture internally).  Most of the time, these type-checkers are
  confronted sequentially with very similar pieces of input: the same
  program, each time slightly modified. Yet they rebuild the entire
  certificates, even for unchanged parts of the code. Instead of
  discarding them between runs, we can take advantage of these
  certificates to improve the reactivity of type-checking, by feeding
  parts of the certificates for unchanged parts of the original
  program back into the new input, thus building an incremental
  type-checking.

  We present a language-independent framework making possible to turn
  a type system and its certifying checking algorithm into a safe,
  incremental one. It consists in a model of incrementality by
  sharing, a generic data structure to represent reusable derivations
  based on contextual \LF, and a general-purpose language to write
  certifying programs.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}
%% It’s an interesting problem (1 page)

%% Describe the problem (Use an example)

Verifying the well-typing of real-world programs is a critical matter:
on it relies their safety, and it encourages good programming
practices: modularity, early bug catching\ldots. But it is a complex
operation, for the compiler designer and for the human operator:

\paragraph{For the compiler designer}

Type systems are usually presented in a declarative manner as a set of
inference rules, together with proofs that all programs accepting a
derivation in this system respect a certain dependability claim, the
famous ``progress and preservation'' pair. Yet, actual algorithms of
verification often differ very much from their declarative
description. This is of course true when programs are provided with
little type annotations (``type inference''), in which case the
missing information has to be algorithmically synthesized, but is also
true in many cases for mere ``type checking'', or explicit
``church-style'' calculi. Take for example dependent type systems,
relying definitional equality on types, or type systems with subtype
polymorphism: both rely on a rule of the form
$$
\infer{\vdash M : A \\ A\leq B}{\vdash M : B}
$$
(for a given notion of $\leq$) which is not syntax-directed. The
implementer has to turn the declarative system into an algorithm, and
convince himself that both are equivalent. This task is highly
non-trivial and non-modular: for instance, how to infer the type of a
System \sysname{T} recursor $\recb M N x y P$, in presence of
subtyping? The problem is that we have to guess a type $A$ more
general than that of $N$ and $P$, while giving that type to variable
$\var y$. A well-known algorithm if the subtyping relation forms a
complete lattice is to iterate the typing of $P$ with the join of the
previously computed and the new type until a fixpoint is reached.

The most common option is to formally prove the equivalence, \ie\ that
the (typing) relation is actually a function. Although these proofs
can be difficult to obtain, it is the most satisfactory because it
allows to take only the result of the algorithm into
consideration. But let us choose another path for a minute:
derivations in the declarative systems, if they cannot always be
easily inferred, can be \emph{verified} if provided by an oracle. If
the oracle is wrong, we can tell cheaply by verifying the witness
typing derivation it provides. Let us then imagine a type-checker to
be the pair of \emph{(i)} an untrusted procedure $\finfer M$ to infer
a typing derivation out of program $M$; it does not have to be
complete, or even sound, and \emph{(ii)} a trusted and fast kernel
$\fcheck{\md}$ to verify these derivations \wrt\ a given set of
inference rules $\Sigma$. This is a \emph{certifying} scheme (see
\cite{leroy2006compcert} for a discussion on approaches to reliable
compilers): if $\fcheck{\finfer M} = \cst{true}$ then there really
\emph{is} a derivation $\md$ for $M$, but the converse is not
necessarily true: function $\finfer{}\!\!$ could fail, or return an
incorrect derivation. Explicitly generating the derivation in the
declarative system has the following advantages over the first
approach:
\begin{itemize}
\item it is lightweight: the untrusted algorithm is usually shorter
  than the corresponding equivalence proof
\item compilers often need to propagate type information further in
  the chain of transformation; the typing derivation produced by
  $\finfer{}$ being explicit, it could constitute the data structure
  being exploited.
\item it inherits benefits from certifying approaches like
  proof-carrying code \cite{necula1997proof}: certificates can be
  transmitted with the program, saving regeneration time and avoiding
  to disclose the generation program.
\end{itemize}
\ldots\ but there is more:

\paragraph{For the programmer}

As type systems become richer, it becomes increasingly difficult for
the programmer to write a well-typed program in one try, compile it
and run it: writing a program longer than a few dozens of lines
becomes a tight and non-linear interaction between the human and the
type-checker involving constant experiments, fixes, etc. If this
interaction is possible for short programs and fast checkers, it
becomes increasingly hindered by the latency of complete rechecks:
even if the modifications made to the program between two interactions
are small, the whole program is usually rechecked. This is especially
true for languages embedding formal verification aspects, like proof
assistants: there, tight interaction with a type or proof checker is
unavoidable due to the difficulty of writing correct proofs without
guidance from the system, and the time taken by many tools to infer
easy parts of the proofs (proof search). The constant modification of
the source makes it necessary to observe \emph{incrementally} the
effect of a small change on the whole edifice: we need to take
advantage of the results of previous checks in order to build the
result for a modified version of the program.

Several generic methods have been devised for turning a batch program
into an incremental one \cite{acar2006,carlsson2002}. Unfortunately,
they suffer from their genericity on the particular problem of type
checking:
\begin{itemize}
\item first, they ignore the higher-order nature of program syntax,
  and the kind of structural quotients we make on syntax trees with
  binders and environments ($\alpha$-equivalence, weakening\ldots): a
  hypothetical derivation of\ \ $\vdash x+2 : \cst{nat}$ is still valid
  when put under a binder\ \ $\vdash\lam y (x + 2) -
  y:\cst{nat}\to\cst{nat}$ and should not be regenerated;
\item for very large programs, we might want to feed the type checker
  only with a small \emph{delta} on the previous version and not the
  whole program, thus saving the time to recognize already checked
  parts;
\item finally, there is no way to a posteriori validate the result of
  the incremental process: the user has to trust not only the
  type-checking algorithm but also its transformation into an
  incremental one, which increases the size of the trusted base.
\end{itemize}

Now, if we consider as above a type-checker as a derivation-generating
procedure, and storing these derivations between calls to the
procedure, we could recognize already-built subderivations for
subterms common to an old, checked version and a new, to-be-checked
version, and graft together these pieces of derivation to form a new,
large one for the new version of the program, saving the regeneration
of all equal subterms' derivations in compatible contexts. This way we
constantly maintain the verifiable witness of well-typing for the
program, and take advantage of the quotients made on higher-order
syntax.

We can thus add to the list of advantages begun earlier that
explicitly generating typing derivations makes possible a form of safe
incrementality by sharing common subderivations.

In this paper, we present a framework to incrementally generate typing
certificates represented in the \LF\ logical framework, and implemented
as an \sysname{ML} library. We specifically propose:
\begin{itemize}
\item a simple language for representing programs and proof
  deltas, derived from contextual \LF\ \cite{nanevski2008contextual};
\item a computational language to write untrusted
  derivation-generating type checkers, based on a notion of function
  inverse
  to refer to already-computed values;
\item a generic algorithm to evaluate these type-checkers on program
  deltas and verify on-the-fly the generated incremental certificates
  against a \emph{repository} of already constructed and checked
  derivations.
\end{itemize}

\section{Using the framework}
%% It’s an unsolved problem (1 page)

\subsection{As a programmer}
\label{sec:use-incremental}

A student is learning about System \sysname {T_{<:}} --- a simply
typed $\lambda$-calculus with natural numbers $\cst{nat}$ coming in
two sets $\cst{even}$ and $\cst{odd}$, introduction $\z$ and $\s{M}$,
elimination ($\recb M N x y P$), and subtyping on these types (for
formal details, see appendix). He (or she) wants to write a program
computing $(2+2)\times 3$ in that system. He knows that sometimes the
arguments of $\cst{rec}$ are quite hard to get correct, but
\sysname{T_{<:}} is a typed language; he wants to take advantage of
this to build his program incrementally using the teacher's provided
incremental verifier. As a first attempt, he wants to verify $P_1 =
\recb{\s\z}{\s\z} x y {\s {\var x}}$: he sends the command
$\finfer{P_1}$ to the system. The $\finfer{}$ operator takes a
well-formed program and tries to construct a valid typing derivation
for its argument in the current, empty context. The system succeeds
and returns a typing derivation $\md_1$ of \/ $\vdash P_1 :
\cst{nat}$, valid in an empty context.

The student then decides to factorize a definition for $\var{add}$ out
of his code:
\begin{align*}
  &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
    {\var y} u v {\s{\var u}}}
  \app{\var{add}}\app{(\s\z)}{(\s\z)} \ \text{.}
\end{align*}
But he (or his text editor) realizes that some parts of this program
have not changed: both $\s\z$, and the function body $\s{\var u}$. He
could reuse the pieces of derivation just built. Let us call these
respectively $\md_2$, $\md_3$ and $\md_4$. $\md_4$ is hypothetical: it
depends on the hypothesis $H$ that\ \ $\vdash \var u:\cst{nat}$ .
Provided there is an operator $\fget\md$ from the derivation to the
program it types, what really needs to be sent to the system is:
\begin{align*}
  &\finfer{}(
    \letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
      {\var y} u v {\fget{\gsubst{\md_4}{\msubst{H}{\finfer{\var u}}}}}}
    \app{\var{add}}\app{\fget{\md_2}}{\fget{\md_2}}
  )
\ \text{,}
\end{align*}
that is, only the changed subterms of the program: this is a
\emph{delta}. Derivation $\md_2$ used to live in the empty context, we
are then free to use it in the new context where
$\vdash\var{add}:\cst{nat}\to\cst{nat}$, because System \sysname T
enjoys the \emph{weakening} property; $\md_4$ depending on a
derivation\ \ $\vdash\var x:\cst{nat}$, we instantiate this hole with
the derivation $\finfer{\var u}$ inferred recursively for the new
recursive argument $\var u$. $\md_2$ derives $\vdash \s\z :
\cst{odd}$, but $\var{add}$ awaits two $\cst{nat}$ arguments. When the
system generates this derivation, it inserts a coercion $\cst{odd}
\leq \cst{nat}$ at these two points. Taking advantage of $\md_2$ and
$\md_4$ the system succeeds with a new derivation made out of these,
without having to regenerate them.

Let us call $\md_5$ the subderivation generated for the definition of
$\var{add}$, $\md_6$ for the arguments
$\app{\fget{\md_2}}{\fget{\md_2}}$ and $\md_7$ for the application
$\app{\var{add}} {\fget{\md_6}}$. Even if it occurs originally in the
scope of $\var{add}$, $\md_6$ does not depend on the hypothesis $\var
H$ that $\vdash \var{add}:\cst{nat}\to\cst{nat}$ because of the
\emph{strengthening} property. Contrarily, $\md_7$ does: each time we
want to reuse it, we will have to provide a proof of this fact.

The student then provides a definition for multiplication, and writes
the operation $(2 + 2)\times3$:
\begin{align*}
  \finfer{}(
  &\letb{add}{\fget{\md_5}}
  \letb{mul} { \tlam x {\cst{nat}} \tlam y
    {\cst{nat}} \recb \z {\var y} z t {
      \app{\var{add}}\app{\var x} {\var z}} }
  \\
  &\quad\quad\app{\var{mul}}\app
  {(\fget{\gsubst{\md_7}{\msubst H {\finfer{\var{add}}}}})}
  {(\s{\fget{\md_2}})}
  )
  \ \text{.}
\end{align*}
Here again, he took care of reusing all derivations for equal
subterms. In particular, $\md_7$ expects a proof that the variable
$\var{add}$ is of type $\cst{nat}\to\cst{nat}$; he provides
$\finfer{\var{add}}$ meaning: at this point we will have generated (or
reused in this case) the derivation of $\var{add}$, so we just have to
use it here. $\md_2$ was deriving $\vdash\s\z:\cst{nat}$ (with a
coercion since it was used as an argument to $\var{add}$); the system
thus infers the more general type $\cst{nat}$ for $\s{\fget{\md_2}}$.

% Finally, he decides (in an odd move) to inline the definition of
% $\var{mul}$ into its call site. Call $\md_8$ the derivation of the
% definition of $\var{mul}$ (it depends on an $\var H$ of
% $\vdash\var{add}:\cst{nat}\to\cst{nat}$) and $\md_9$ the derivation of
% $(2+2)\times 3$ depending on derivations $\var{H}$ and $\var{H'}$ for $+$ and
% $\times$. He orders:
% \begin{align*}
% \finfer{}(
% \letb{add}{\fget{\md_5}}
% \fget{({\gsubst{\md_9} {\msubst {H} {(\fget{\var{add}})}; \msubst {H'}
%       {\gsubst{\md_8}{\msubst H {(\fget{\var{add}})}}}}})}
% )
%   \ \text{.}
% \end{align*}
% The definition for $\var{add}$ is unchanged, but the instantiation of
% open derivations enables to directly substitute one into the other and
% get the appropriate program: call $\md_{10}$ the whole derivation just
% returned, $\fget{\md_{10}}$ returns:
% \begin{align*}
%   &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
%     {\var y} x y {\s{\var x}}}
%   \\&\quad
%   {(\tlam x {\cst{nat}} \tlam y
%     {\cst{nat}} \recb \z {\var y} z t {
%       \app{\var{add}}\app{\var x} {\var z}})}
%   \app{(\app {\var{add}} \app {(\s{\s\z})}
%     {(\s{\s\z})})}{(\s{\s{\s\z}})}
%   \ \text{.}
% \end{align*}

\paragraph{Discussion}

We choose here to represent deltas as usual terms with variables $\var
x, \var y$ and special, \emph{meta}-variables referring to
already-computed results $\md_i$. Commands sent to the system consists
of terms, possibly containing special typed operators having a
computational content: $\finfer{\cdot}$ takes a term $M$ and produces
the (dependent) pair of a type $A$ and a derivation of the judgment
$\vdash M:A$ in the current context; $\fget{\cdot}$ does the exact
converse: it takes a derivation $\vdash M:A$ and projects it back as
the term $M$. The successful result of a command execution is a
derivation where each subderivation has been given a unique
metavariable name for later reuse (a \emph{repository}). As
subderivations can have free variables, we close them by a local
environment that must be instantiated by a \emph{substitution}.

Although this small example may seem anecdotal, the interest of such a
mechanism becomes clear on very large programs: then the cost of
rechecking well-typing of a small modification amounts to rechecking
the path in the term from the root to the changed part, and the
changed part itself.

Seeing type-checking as issuing a completely explicit typing
derivation enables thus to think of the relationship between the input
and the output. Feeding subterms of the output $\md_i$ back into
subterms of the input $P_i$ is a well-known technique for handling
changes in the input: combined with a way to automatically recognize
already-computed input, it is \emph{memoization}, a general-purpose
technique for incremental computation that has been extensively
studied and combined with other techniques \cite{acar2006}. However,
it is best understood in the case of purely first-order data, and to
our best knowledge remains unexplored for higher-order structures
containing binders, like programs and proofs.

A memoized type-checker stores in a table bindings from its input (the
program) to its output (the derivation), and returns automatically the
stored output if the current input matches a binding in the table. It
thus performs two operations at once: output reuse and recognition of
already-seen input. Recognition --- that is from a modified program,
generate a delta from previous versions --- is in the higher-order
case a complex task that we hope to address in the future; we tackle
here the verification of output reuse: given a delta, construct a new
derivation and verify it.

\subsection{As a type system designer}
\label{sec:lang-incr}

Earlier, the teacher is preparing the tool for the students. He (or
she) provides the abstract syntax, the typing rules and the untrusted
typing algorithm in order to build the incremental type checker. How
should he write the algorithm? We want it to return a derivation, and
take advantage of a higher-order representation of the syntax, so it
should not be necessary to thread any explicit environment. The
definition should then start with:

\begin{mathleft}
  \finfer{}\ :\ \prd M {\cst{tm}} \sig A {\cst{tp}} (\vdash M : A) =
\end{mathleft}

\noindent
Function $\finfer{}$ takes a term $M$ and produces the pair of an
inferred type $A$ and a derivation that $M$ has type $A$. Its code
will be untyped, but the teacher provides this type so that the
checker can verify that each call is fed and returns values of the
right type: it is its \emph{contract} so to say. He first decomposes
term $M$ and treats the easy cases of $\cst{o}$ and $\cst{s}$. The
code continues with:

\begin{mathleft}
  \lamd M \match{M} \\
  \quad\caseb{\z} \pair {\cst{even}}{\infer{ }{\vdash \cst o : \cst{even}}} \\
  \quad\caseb{\s M}
  \match{\finfer M} \\
  \quad\quad\caseb{\pair{\cst{even}}\md}
  \pair {\cst{odd}}{\infer\md{\vdash\app{\cst{s}} M : \cst{odd}}} \\
  \quad\quad\caseb{\pair{\cst{odd}}\md}
  \pair {\cst{even}}{\infer\md{\vdash\app{\cst{s}} M : \cst{even}}} \\
  \quad\quad\caseb{\pair{\cst{nat}}\md}
  \pair {\cst{nat}}{\infer\md{\vdash\app{\cst{s}} M : \cst{nat}}} \\
\end{mathleft}

\noindent
We write $\pair{\cdot}{\cdot}$ for the constructor of $\Sigma$-types.
Patterns do not have to be exhaustive since we are in an untrusted
setting. Fr conciseness, we use case failure for signaling typing
error. There are three cases for $\s{M}$ in the system, corresponding
to the three rules in the declarative system. For the application
case, we rely on the well-known property that we may need to insert a
coercion from the actual type of the argument to its expected type:

\begin{mathleft}
  \quad\caseb{\app{M}N} \\
  \quad\quad\letd {\pair {A_1\to B} {\md_1}} {\finfer{M}}
  \letd {\pair {A_2} {\md_2}} {\finfer{N}}
  \letd {\md_{\leq}} {\fleq {A_1} {A_2}} \\
  \quad\quad
  \match{\md_{\leq}} \\
  \quad\quad\quad\caseb{\infer{ }{\vdash A\leq A}}
  \pair {B} {\infer{\md_1 \and \md_2}{\vdash \app M N : B}} \\
  \quad\quad\quad\caseb{\_}
  \pair {B} {\infer{\md_1 \and \infer*{\md_2 \and \md_{\leq}}{\vdash N
      : A_1}}{\vdash \app M N : B}}
\end{mathleft}

\noindent
The $\syntax{let}$ construct is just syntactic sugar for a one-branch
$\syntax{case}$; we use an auxiliary function $$\fleq{}{}\ :\ \prd A
{\cst{tp}} \prd B {\cst{tp}} \vdash A\leq B = \ldots$$ trying to
construct a subtyping derivation for its argument, or failing with an
error. Notice the slight optimization to the size of the derivation:
if the types $A_1$ and $A_2$ are the same syntactically, we do not
generate a subtyping rule. This is done by matching on the returned
derivation $\md_\leq$ in the case of the \textsc{Refl} rule.

Then comes the abstraction case $\tlam x A M$. The difficulty is that
the recursive call $\finfer{M}$ must be done in an enlarged
environment where $\vdash x : A$. Otherwise the check that the
returned open derivation $\md$ is correct would fail ($x$ is
unknown). We thus introduce a new construct $\matchin M \Gamma \ldots$
($\match M \ldots$ being syntactic sugar for $\matchin M \envnil
\ldots$). Even then, in the recursive call, we would have no way of
synthesizing a variable $\var x$'s type anymore. Thus we substitute
all occurrences of $\var x$ with the \emph{output} of function
$\finfer{}$ on it (since it is known at this point), \ie\ a pair of
$A$ and $\md_1$. For this substitution to be well-typed, we now need
to coerce this output to a valid input (a term). We introduce an
operator which is in some sense the \emph{inverse} of $\finfer{}$:
operator $\fget{}$ maps a pair of a type and a derivation to the term
it types.

\begin{mathleft}
  \quad\caseb{\tlam x A M}
  \letdin{\pair B \md}{\env {$\md_{\var x}$} {(\vdash {\var x} : A)}}
  {\finfer {\gsubst M {\msubst x {\fget{\pair A {\md_{\var x}}}}}}}
\end{mathleft}

At this point, we get back $B$ and $\md$ from the recursive call. We
know that $B$ cannot depend on $\md_x$ (because a System
\sysname{T_{<:}} type is always closed), but $\md$ can: it is not
\emph{per se} a hypothetical derivation but an \emph{open} derivation
where a special judgement named $\md_x$ can occur. To return the
complete derivation of $\vdash\tlam x A M : A \to B$, we must then
close, or ``hypothesize'' $\md$ by $\md_x$, and then apply rule
\rulename{Lam}:

\begin{mathleft}
  \quad\quad
  \pair {A\to B}
  {\infer{
\mbox{$      \begin{array}{c}
        {[\md_{\var x}]} \\
        {\md}
      \end{array}
    $}    }{
    \vdash\lam x M : A\to B
  }}
\end{mathleft}

Concerning the treatment of environments, the case of $\recb M N x y
P$ is similar: we first compute recursively the derivations $\md_M$
and $\md_N$ which should be well-typed in the current environment, and
then $\md_P$ in the enlarged environment, with variables $\var x$ and
$\var y$ in $P$ substituted with the (future) result of $\finfer{}$.

One problem remains with subtyping: terms $N : A_N$ and $P : A_P$
should be coercible to the same type $A$, but $P$ should be typed
under $\vdash y:A$, which is not known before typing $P$. A trick to
break the loop is to iterate the typing of $P$ with a more and more
general type, until we reach a fixpoint. We reach a fixpoint if there
is a unique more general type, in our particular case of subtyping
relation, this means iterating only twice: we will then attain
\cst{nat}.

\begin{mathleft}
  \quad\caseb{\recb M N x y P} \\
  \quad\quad
  \letd {\pair {A_M} {\md_M}} {\finfer{M}}
  \letd {\md_{A_M}} {{A_M} \leq \cst{nat}}
  \letd {\pair {A_N} {\md_N}} {\finfer{N}} \\
  \quad\quad
  \syntax{let} {\pair {A_P} {\md_P}} \syntax{in}
  {(\envcons {\env {$\md_x$} {(\vdash
        \var x : \cst{nat})}} {$\md_y$} {(\vdash\var y : A_N)})} = \\
  \quad\quad\quad
  {\finfer{\gsubst P {\msubstcons {\msubst x
          {\fget{\pair{\cst{nat}}{\md_x}}}} y
        {\fget{\pair{A_N}{\md_y}}}}}} \syntax{in}\\
  \quad\quad\letd {\pair A {\pair{\md_{A_N}}{\md_{A_P}}}} {{A_N} \sqcap {A_P}} \\
  \quad\quad
  \syntax{let} {\pair {\_} {\md_P}} \syntax{in}
  {(\envcons {\env {$\md_x$} {(\vdash
        \var x : \cst{nat})}} {$\md_y$} {(\vdash\var y : A)})} = \\
  \quad\quad\quad
  {\finfer{\gsubst P {\msubstcons {\msubst x
          {\fget{\pair{\cst{nat}}{\md_x}}}} y
        {\fget{\pair{A}{\md_y}}}}}} \syntax{in}\\
  \quad\quad\pair {A} {
    \infer{
      \infer*{\md_M \and \md_{A_M}}
      {\vdash M : A} \and
      \infer*{\md_N \and \md_{A_N}}
      {\vdash N : A}\and
      \infer*{\infer*{}{\mbox{$
        \begin{array}{c}
          [\md_x] [\md_y] \\
          \md_P
      \end{array}
        $}} \and \md_{A_P}}{\vdash P : A}
    }{
      \vdash \recb M N x y P : A
    }
  }
\end{mathleft}

\noindent
Again, we use a new auxiliary function
$$\sqcap\ :\ \prd{A}{\cst{tp}} \prd{B}{\cst{tp}} \sig C {\cst{tp}}
(\vdash A\leq C )\times(\vdash B\leq C) = \ldots$$ computing the sup
of two types $A$ and $B$, and returning proofs of subtyping. Note that
we could have performed the optimization seen in the application case
as well.

The type-checker is now done. There is no case for variables, as we
will not ever meet this case: we already substituted all variables
with their result derivation. Evaluation only needs to make sure that
in this case, the derivation will be returned, \ie\ that equation
$\finfer{\fget{\pair A \md}} = \pair A \md$ holds.

Note that $\fget{}$ can be derived automatically from the type of
$\finfer{}$: we will call it its inverse, since it maps the return
type to the arguments. It has definition:

\begin{mathleft}
  \fget{}\ :\ \prd{M}{\cst{tm}} \prd {A}{\cst{tp}} \prd{\md}{(\vdash
    M:A)} \cst{tm} =
  \lamd M \lamd A \lamd \md M
\end{mathleft}

\noindent
where argument $M$ is left implicit (it can be inferred).

\paragraph{Discussion}

We exploit an idea similar to run-time \emph{contracts}
\cite{wadler2009well}: there is no static guarantee that computations
will not return ill-typed values, but arguments and results are
checked at run-time at the boundaries of defined functions. This
allows to omit formal justifications, like the fact that we need to
iterate twice the typing of $P$ in $\recb M N x y P$, or that $B$
cannot depend on $\md_x$ in the $\lambda$ case, while detecting errors
early.

The idea of untrustedly generating certificates verified \emph{a
  posteriori} is very wide-spread in \sysname{LCF}-style proof
assistants: during manual proof search, the interaction is driven by
\emph{tactics}: they are untrusted programs generating pieces of
proofs, assembled by other, higher-order tactics; verification of the
generated proofs is done when the evaluation of all tactics is
complete. The differences with our mechanism are two-fold: first,
tactics usually take one (or several) \emph{goal}(s) as input, \ie\
holes in a proof, whereas our functions can take any term; secondly,
proof-check is usually done at the end of the evaluation, which can
lead to less tractable bugs; by making evaluation and checking
mutually recursive, we choose to detect errors earlier in these
witnesses: exactly at the point where they are generated.

Introducing the ``inverse'' of the derivation inference function has
two purposes: as we saw in \ref{sec:use-incremental}, it allows to
refer to already-constructed derivations when writing deltas. It also
permits to abstract from the concrete environment when writing the
type-checker itself, that one must thread throughout the process in a
traditional, first-order type-checker. It is a generalization of the
so-called \emph{context-free} typing presented in
\cite[chap. 4]{boespflug2011conception} where a special term construct
$\{x:A\}$ denoting free variables is added to the syntax of terms, and
rules
$$
\infer{
  \vdash \gsubst M {\msubst x {\{\var x:A\}}} : B
}{
  \vdash \tlam x A M : A\to B
}
\qquad\text{and}\qquad
\infer{ }{
  \vdash \{\var x : A\} : A
}
$$
replace the usual ones. The difficulty with this technique,
particularly in a dependent setting, is to ensure that term $\{\var
x:A\}$ reduces to $\var x$ so that they are considered equal, \ie\ we
can strip off the annotations, while not forgetting them too soon:
otherwise we would hit the unhandled case $\finfer{\var x}$. This
requires evaluating the program deltas with a carefully designed
strategy (see \ref{sec:comput}).

\section{Design of the framework}
%% Here is my idea (2 pages)

\subsection{The representation language}
\label{sec:repr}

The emission of certificates by untrusted applications first raises
the question of the \emph{data structure} to use for these
certificates. % The requirements are:
% \begin{itemize}
% \item conciseness of the certificates, since they are stored or even
%   transferred remotely;
% \item a well-understood, short and independent verification algorithm.
%   This is known as the \emph{de Bruijn principle};
% \item universality with respect to the particular problem: having a
%   certificate language for each domain of application would just shift
%   the problem of trust; having one universally accepted certificate
%   language that can be tailored to many problems puts a much stronger
%   trust on the unique verifier.
% \end{itemize}
The Edinburgh Logical Framework \cite{harper1993framework}, or \LF\
for short, is one of the well-accepted solution to this question. It
is a dependently typed lambda-calculus for \emph{representing}
higher-order terms and derivations
\cite{pfenning1988higher}. The user gives a
\emph{signature} defining abstract syntax and inference rules of his
\emph{object language}, and gets an algorithm to verify terms and
derivations expressed in the \LF\ syntax.
% For instance, the rule for System \sysname{T}'s recursor can be
% encoded by the following constant:
% \begin{mathleft}
%   \cst{is\_rec}\ :\
%   \prd{M}{\cst{tm}}\prd{N}{\cst{tm}}\prd{P}{\cst{tm}\to\cst{tm}\to\cst{tm}}
%   \\\quad
%   \prd A {\cst{tp}}
%   \app {\cst{is}}\app M {\cst{nat}} \to \app{\cst{is}}\app N A \to \\\quad
%   \left(
%   \prd x {\cst{tm}} \prd y {\cst{tm}}
%   \app{\cst{is}}\app x \cst{nat} \to \app{\cst{is}}\app y A \to
%   \app{\cst{is}}\app {(\app P \app x y)} A
%   \right)\to\\\quad
%   \app{\cst{is}}\app{(\app{\cst{rec}}\app M \app N (\lam x\lam y \app
%     P\app x y))} A
% \end{mathleft}

To write deltas by sharing subterms, we need to be able to refer to
any subterm. For this, we first extend \LF\ with contextual
metavariables (written $\meta X$, $\meta Y$) standing for open terms,
following \cite{nanevski2008contextual}, secondly, we make sure that
every subterm is named by a metavariable (an operation called
\emph{slicing}). We thus introduce Sliced \LF\ (\SLF, see
Fig. \ref{fig:syntax-LF}): it is an extension of \emph{spine canonical
  \LF} \cite{pfenning2007term} enriched with metavariables which are
references to open terms as in \cite{nanevski2008contextual}.

\begin{figure}[t]
  \begin{align*}
    K &\gequal \prd x A K \gor \type &
    \text{Kind}\\
    A &\gequal \prd x A A \gor P &
    \text{Type family} \\
    P &\gequal \app {\cst a} S &
    \text{Atomic type} \\
    M &\gequal \lam x M \gor F &
    \text{Canonical object} \\
    F &\gequal \app H S \gor \smeta X \sigma &
    \text{Atomic object} \\
    H &\gequal \var x \gor \cst c \gor \fct{f} &
    \text{Head}\\
    S &\gequal \spinenil \gor \spinecons M S &
    \text{Spine}\\
    \sigma &\gequal \msubstnil \gor \msubstcons \sigma x M &
    \text{Parallel substitution} \\
    \Gamma &\gequal \envnil \gor \envcons \Gamma x A &
    \text{Environment}
  \end{align*}
  \caption{Syntax of \SLF}
  \label{fig:syntax-LF}
\end{figure}

In this variant, only $\eta$-long and $\beta$-normal objects have an
existence. Canonicity for $\beta$-reduction is enforced in the syntax
and canonicity for $\eta$-expansion will be enforced by typing,
following \cite{hl07mechanizing}. Besides, application of a
\emph{head} (\ie\ a variable, constant, or function symbol) is $n$-ary
(\ie\ to a \emph{spine} of objects) as opposed to the more standard
binary application: this eases the definition of substitution and
$\eta$-long forms, and corresponds to a focused sequent calculus known
as \sysname{LJT}~\cite{herbelin1995lambda}. We differ the discussion
of function symbols $\fct f$ to section \ref{sec:comput}. A
\emph{value} is an object containing no function symbols. The
definition of hereditary substitution $\gsubst M \sigma$ is skipped
and can be found in the appendix.

We distinguish syntactically between \emph{checkable} $M$ and
\emph{inferrable} $F$ objects (resp. \emph{canonical} and
\emph{atomic}), as in \eg\ \cite{pierce2000local}. Canonical objects
are checked against a given type, whereas we can synthesize the type
of atomic objects; the coercion from canonical to atomic objects
triggers the equality check between synthesized and checked types.

As is customary, we adopt the Barendregt convention for naming bound
variables, write $A_1\to A_2$ for $\prd x {A_1} {A_2}$ when $\var
x\notin \FV{A_2}$, $\meta X$ for $\smeta X \msubstnil$, and adopt
indifferently list, map or set notations for $\sigma$ and
$\Gamma$. The identity substitution $\msubstid{\Gamma}$ is a notation
for $\{\msubst x {\var x}\ |\ \var x \in\dom\Gamma\}$. Metavariables
$\smeta X \sigma$ stand for open objects with free variables in
$\dom{\sigma}$. To each use of them is attached a substitution
$\sigma$ defining these free variables. We write $\FMV{M}$ for the set
of metavariables in $M$ and call an object $M$ \emph{metaclosed} if
$\FMV{M} = \emptyset$.

\begin{definition}
  A \emph{repository} $\mr$ is a finite map from metavariable to
  triplets of environments, atomic objects and atomic types (\ie\
  judgments of well-typing), along with a distinguished metavariable,
  corresponding to the \emph{tip} of the term it contains:
$$ \mr\;:\;(\meta X \mapsto (\Gamma\vdash\tp F P))\times\smeta X
\sigma $$
\end{definition}

If $\mr = (\Delta, X[\sigma])$, we write $\hat {\mr}$ for the tip
$X[\sigma]$, abusively $\mr$ for $\Delta$ and $\mr (\smeta X \sigma)$
for $\gsubst M\sigma$ if $\Delta(\meta X) = (\Gamma\vdash M : A)$.
The algorithm in \ref{sec:eval} will maintain two invariants: a
repository $\mr$ is \emph{acyclic}, (for all $\meta X$ in
$\mr$, the fixpoint of $\FMV{\mr(\meta X)}$ does not contain $\meta
X$) and all $F$ and $P$ in it are values.

\begin{definition}
  We define the partial \emph{checkout} operation $\checkout{\mr}$
  (resp. $\checkoutr{M}$, $\checkoutr{S}$) from a repository
  (resp. object, spine) to a metaclosed object as unfolding the
  definitions of all metavariables recursively:
  \begin{align*}
    \checkout{\mr} &= \checkoutr{\hat{\mr}} &
    \checkoutr{\smeta X \sigma} &= \checkoutr{\mr(\smeta X\sigma)}\\
    \checkoutr{\lam x M} &= \lam x \checkoutr{M} &
    \checkoutr{\app H S} &= \app H {(\checkoutr{S})} \\
    \checkoutr{\spinecons M S} &= \spinecons {\checkoutr M} {\checkoutr S} &
    \checkoutr{\spinenil} &= \spinenil
  \end{align*}
\end{definition}

\begin{example}
  The repository
$
(
\meta X \mapsto
\envnil\vdash
\app{\cst{lam}} \lam x \smeta Y {\msubst u {\smeta T {\msubst w x}}} :
\cst{tm},
\meta Y \mapsto
\env u {\cst{tm}} \vdash
\app{\cst{lam}} \lam y \smeta Z {\msubstcons {\msubst u
    {\var u}} v {\var y} } :
\cst{tm}
,
\meta Z \mapsto
\envcons {\env u {\cst{tm}}} v {\cst{tm}} \vdash
\app{\app{\cst{app}}{\var v}}{\var u} :
\cst{tm}
,
\meta T \mapsto
\env x {\cst{tm}} \vdash
\app{\cst{app}}\app{\var x}{\cst{o}} :
\cst{tm}
), \smeta X {}
$
checks out as the metaclosed atomic object
$
  \app {\cst{lam}} \lam x
    \app {\cst{lam}} \lam y
      \app {\cst{app}} \app {\var y}
      (
        \app{\cst{app}}\app{\var x}{\cst o}
      )
$.
\end{example}

\subsection{The computation language}
\label{sec:comput}

Secondly, we introduce the \CL\ language compute over our \SLF\
objects (Fig.~\ref{fig:syntax-CL}). We can construct atomic objects
out of others, and deconstruct them by case analysis. The definition
$T$ of a function always begins with a series of $\lambda$.

\begin{figure}[t]
  \begin{align*}
    T &\gequal \lam x T \gor
    U & \text{Term} \\
    U &\gequal F \gor
    \matchin U \Gamma C & \text{Atomic term}\\
    C &\gequal \casebnobar P U \gor
    C\ \caseb P U & \text{Branches}\\
    P &\gequal \app H {\var x}\ \ldots\ {\var x} &\text{Pattern}
  \end{align*}
  \caption{Syntax of \CL}
\label{fig:syntax-CL}
\end{figure}

This language is untyped, but types are dynamically checked: there is
no static guarantee that object $\app {\fct f} x$ produced by a
function $\fct f : A_1\to A_2 = T$ will have type $A_2$, nor that
pattern matching is exhaustive. It is also non-terminating: we can
define a recursive function $\fct f : \cst a\to \cst a = \lam x \app
{\fct f} {\var x}$. Nevertheless, we want to maintain the invariant
that each time an object goes in or out of a function $\fct f : A_1\to
A_2$, it is checked against the type of this function (resp. $A_1$ or
$A_2$). You can think of interpreted functions as mere black boxes
taking and producing objects, and being able to feed objects to other
black boxes; yet the wires linking them are labeled with types and
objects circulating in them are checked against that type.

Besides constants, the signature defines \emph{interpreted functions}
$\fct f$, $\fct g$. Each function comes with its type and its
code. 

\begin{definition}
  A \emph{signature} declares object and type constants and functions
  to be used in objects.
  \begin{align*}
    \Sigma &\gequal
    \gnil
    \gor
    \gcons \Sigma {\cst a : K}
    \gor
    \gcons \Sigma {\cst c : A}
    \gor
    \gcons \Sigma {\fct f : A = T}
    &\text{Signature}
  \end{align*}
\end{definition}

\begin{definition}
  To each function $\fct f : A = T$ in $\Sigma$ we associate a family
  of \emph{inverses} $\fcti f {\;n} : (A)^n = \pi_n(A)$ for $n\in\mathbb
  N$, where type $(A)^n$ and term $\pi_n(A)$ are defined recursively:
  \begin{align*}
    (\prd x {A_1} {A_2})^{n+1} &= \prd x {A_1} (A_2)^n &
    (\prd x {A_1} {A_2})^0 &= \prd x {A_1} (A_2)_{A_1} \\
    (\prd x {A_1} {A_2})_A &= \prd x {A_1} (A_2)_A &
    (P)_A &= \prd x P A \\
    \pi_{n+1}(\prd x {A_1}{A_2}) &= \lam x \pi_n(A_2) &
    \pi_0(\prd x {A_1} {A_2}) &= \lam {y} \pi^y(A_2) \\
    \pi^y(\prd x {A_1}{A_2}) &= \lam x \pi^y (A_2) &
    \pi^y(P) &= \var y
  \end{align*}
\end{definition}
Functions $(A)^n$ and $\pi_n(A)$ use auxiliary functions $(A)_B$ and
$\pi^y(A)$ to put the $n$-th argument at the end of the type and term.

\begin{example}
  \begin{itemize}
  \item
    Function $\fct{infer} : (\prd M {\cst{tm}} \app{\cst{sig}} M)
    = T$ (called $\finfer{}$ above, with $\cst{sig}$ the \LF\ encoding
    of the previous $\Sigma$-type) has one inverse $\fcti{infer}{0} :
    (\prd M {\cst{tm}} {\app{\cst{sig}} M} \to \cst{tm}) = \lam x \lam
    y \var x$ (called $\fget{}$ above).
  \item
    Function $\fct{equal} : \prd M {\cst{tm}} \prd N {\cst{tm}}
    \app{\cst{eq}}\app M N = T'$ has two inverses: $\fcti{equal}{0}$
    and $\fcti{equal}{1}$ of the same type $\prd M {\cst{tm}} \prd N
    {\cst{tm}} \app{\cst{eq}}\app M N \to \cst{tm}$ with code
    respectively $\lam m \lam n \lam h \var m$ and $\lam m \lam n \lam
    h \var n$.
  \end{itemize}
\end{example}

\subsection{The evaluation algorithm}
\label{sec:eval}

The evaluation algorithm, called \emph{commit}
$\function{commit$_\mr$}{F}$ takes an atomic object $F$ (possibly
containing function symbols to evaluate) and a repository $\mr$, and
produces a new repository $\mr'$ which is a strict extension of $\mr$
with a different tip, and which checks out to the value of $F$. It has
three roles, which are necessarily interleaved:
\begin{itemize}
\item it evaluates $M$ by reducing function symbols,
\item it type-checks all values fed to and produced by functions \wrt\
  $\mr$,
\item it slices those values. This requires threading the map of $\mr$
  throughout evaluation and typing.
\end{itemize}

We now turn to the bidirectional typed reduction and slicing of the
algorithm (Fig. \ref{fig:obj-typing}, \ref{fig:obj-typing2},
\ref{fig:evaluation}), presented as an inference system. All judgments
are of the form $\gjao L U V W$ where $U$, $V$ and $L$ are inputs, are
directed by the syntax of $V$ and $L$, and $W$ is the output. They are
parameterized by a constant and implicit signature $\Sigma$.

Label $L$ is a mode influencing the strategy of evaluation of defined
function applications. They range over $\top$, $\bot$ and $\fct f$,
meaning respectively strong reduction, pure checking with no reduction
and call-by-name, head reduction. The purpose of the $\fct f$
annotation is shown in rule \textsc{FAppEvalInv}: it detects the
special redex form $\app{\fct f}\app {(\app{\fcti f
    0}S_0)}\app\ldots{(\app{\fcti f n} S_n)}$, reducing it to the last
argument of the spines $\function{last}{S_0}$, provided all $S_i$ are
equal. When $n=0$, we recover the equality $\finfer{\fget{X}} = X$.

The main judgment is atomic object typed reduction
$\jf\aeval\Delta\Gamma F {\Delta'} {F'} P$, which reads: in repository
$\Delta$ and environment $\Gamma$, applicative term $F$ is well-typed of
type $P$, and the slicing of its value produces a new repository
$\Delta'$. $F'$ is the \emph{residual} of this operation: only constant
applications $\app{\cst c} S$ are sliced, $F'$ is the ``tip'' of $F$
that was not sliced.

These typing rules use typed definitional equality between families
$\jea\Delta\Gamma {A_1} {A_2}$ and between spines $\jel\Delta\Gamma A
{S_1} {S_2} P$. The formal definition of these judgments is eluded
here. It compares syntactically the values of its arguments. Since it
potentially triggers function evaluation (and thus typing
verification), we carry the repository, the environment and the type
of the compared objects.

\begin{figure}
  \fbox{$\jm L \Delta\Gamma M A {\Delta'} {M'}$}
  \qquad
  Canonical object

  \begin{mathpar}
    \infer[MLam]{
      \jm {\fann L} \Delta{\envcons\Gamma x A} M {A'} {\Delta'} {M'}
    }{
      \jm {L} \Delta\Gamma {\lam x M} {\prd x A {A'}} {\Delta'} {\lam x M'}
    }

    \begin{array}{rl}
      \fann \top &= \top \\
      \fann \bot &= \bot \\
      \fann {\fct f} &= \bot \\
    \end{array}

    \infer[MAtom]{
      \jf L \Delta\Gamma F {\Delta'} {F'} {P'}
       \and
      \jea{\Delta'}\Gamma P {P'}
    }{
      \jm L \Delta\Gamma F P {\Delta'} {F'}
    }
  \end{mathpar}

  \fbox{$\jf L \Delta\Gamma F {\Delta'} {F'} P$}
  \qquad
  {Atomic object}

  \begin{mathpar}

    % Variable
    \infer[FAppVar]{
      \jl {\fann L} \Delta\Gamma A S {\Delta'} {S'} P
    }{
      \jf L \Delta\Gamma {\app {\var x} S} {\Delta'} {\app {\var x} {S'}} P
    }\quad{\small (x:A)\in\Gamma}

    % Variable
    \infer[FAppConst]{
      \jl {\fann L} \Delta\Gamma A S {\Delta'} {S'} P
    }{
      \jf L \Delta\Gamma {\app {\cst c} S} {\Delta'} {\app {\cst c} {S'}} P
    }\quad{\small
      \begin{array}{l}
        (c:A)\in\Sigma\\
        L\neq\top
      \end{array}
    }

    % Constant
    \infer[FAppConstSlice]{
      \jl {\top} \Delta\Gamma A S {\Delta'} {S'} {P}
      \and
      \stren{\Gamma, \app {\cst c} {S'}}=\Gamma'
    }{
      \jf \top \Delta\Gamma {\app {\cst c} S} {\mapadd{\Delta'}{\meta X}{(\Gamma'\vdash
          {\app {\cst c} {S'}}:P)}} {\smeta X{\msubstid{\Gamma'}}} P
    }
    \quad
    \begin{array}{l}
      \small
      (c:A)\in\Sigma\\
      \text{($\meta X$ fresh in $\Delta'$)}
    \end{array}

    % Evaluation
    \infer[FAppEval]{
      \jl{L'}\Delta\Gamma A S {{\Delta'}} {S'} P
      \\\\
      \jcc{\Delta'}\Gamma T {S'} {\Delta''} F
      \and
      \jm L {\Delta''}\Gamma F P {\Delta'''} {F'}
    }{
      \jf L \Delta\Gamma {\app {\fct f} S} {\Delta'''} {F'} P
    } \quad
    \begin{array}{l}\small
      (\fct f : A = T) \in \Sigma \\
      {S' \neq \app {(\app {\fcti f 0} S_0)} \app\ldots {(\app
          {\fcti f n} {S_n})}} \\
      (L = \fct g \wedge L'=\bot) \vee (L=\top\wedge L'=\fct f)
    \end{array}

    \infer[FAppEvalInv]{
      \jl{\fct f} \Delta\Gamma A S {\Delta'}{\app {(\app {\fcti f 0} S_0)} \app\ldots {(\app
          {\fcti f n} {S_n})}} P
      \\\\
      \text{for all $i$,$j$, }\ (\jel\Delta\Gamma A {S_i}{S_j} P)
    }{
      \jf L \Delta\Gamma {\app {\fct f} S}
      {\Delta'}{\function{last}{S_0}} P
    }\quad{\small (L = \top \vee L = \fct g)}

    \infer[FAppNoEval]{
      \jl\bot\Delta\Gamma A S {\Delta'} {S'} P
    } {
      \jf \bot \Delta\Gamma {\app{\fct f} S} {\Delta'} {\app{\fct f}
        {S'}} P
    }\quad{(\fct f : A = T\in\Sigma)}

    % Meta
    \infer[FMeta]{
      \Delta(\meta X) = (\Gamma'\vdash F:P)
      \and
      \js{\fann L}\Delta\Gamma \sigma {\Gamma'} {\Delta'} {\sigma'}
    }{
      \jf L \Delta\Gamma {\smeta X \sigma} {\Delta'} {\smeta X {\sigma'}} {\gsubst{P}{\sigma'}}
    }

  \end{mathpar}

  \caption{Typed evaluation for objects (1)}
  \label{fig:obj-typing}
\end{figure}

\begin{figure}
  \fbox{$\jl L\Delta\Gamma A S {\Delta'}{S'} P$}
  \qquad
  {Spine}

  \begin{mathpar}
    \hspace{-1.5em}
    \infer[SCons]{
      \jm L \Delta\Gamma M A {\Delta'} {M'}
      \and
      \jl L {\Delta'}\Gamma {\gsubst{A'}{\msubst x {M'}}} S {\Delta''} {S'} P
    }{
      \jl L \Delta\Gamma {\prd x {A} {A'}} {\spinecons M S} {\Delta''} {\spinecons
      {M'}{S'}} P
    }
    \quad
    \infer[SNil]{ }{
      \jl L\Delta\Gamma P \spinenil \Delta \spinenil P
    }
  \end{mathpar}
  \fbox{$\js L \Delta\Gamma\sigma{\Gamma'}{\Delta'}{\sigma'}$}
  \qquad
  {Substitution}
  \begin{mathpar}
    % Cons
    \infer[$\sigma$Cons]{
      \js L \Delta\Gamma\sigma{\Gamma'}{\Delta'}{\sigma'}
      \and
      \jm {\fann L} {\Delta'}\Gamma M {\gsubst A {\sigma'}} {\Delta''} {M'}
    }{
      \js L \Delta\Gamma{(\msubstcons \sigma x M)} {(\envcons {\Gamma'} x {A'})}
      {\Delta''} {(\msubstcons {\sigma'} x {M'})}
    }
    \quad
    % Nil
    \infer[$\sigma$Nil]{ }{
      \js L \Delta\Gamma\msubstnil\envnil\Delta\envnil
    }

  \end{mathpar}

  \caption{Typed evaluation for objects (2)}
  \label{fig:obj-typing2}
\end{figure}

To reduce the size of the stored environments and substitutions to
their minimum in slices $\Gamma\vdash F : P$, we use an auxiliary
operation:

\begin{definition}
  \emph{Strengthening} takes an environment $\Gamma$ and an object $F$
  well-typed in $\Gamma$ to an environment $\Gamma'$ containing only
  the free variables of $F$:
$
\stren{\Gamma,F} = \{x : P\ |\ x\in \FV{F} \wedge \Gamma(x)=P\}
$.
\end{definition}

It is correct to strengthen $\Gamma$ only \wrt\ $F$ before storing $F$
as a slice because of the following:

\begin{lemma}
  If $\jm L \Delta\Gamma F A {\Delta'} {F'}$, then
  $\jm L \Delta{\stren{\Gamma, F}} F A {\Delta'}{F'}$
\end{lemma}

% Signatures themselves need to be checked. We present the checking
% algorithm in Fig. \ref{fig:sign-typing}.

% \begin{figure}
%   \fbox{$\jsig\Delta\Sigma{\Delta'}{\Sigma'}$}
%   \qquad
%   {Signature}

%   \begin{mathpar}

%     \infer{
%       \jsig\Delta\Sigma{\Delta'}{\Sigma'}
%       \and
%       \jfam{\Delta'}\gnil {\!_{\Sigma'} A}{\Delta''} {A'}
%     }{
%       \jsig\Delta{\gcons\Sigma{{\cst c :^L A}}}{\Delta''}{\gcons{\Sigma'}{{\cst c :^L A'}}}
%     }

%     \infer{
%       \jsig{\Delta'}\Sigma{\Delta''}{\Sigma'}
%       \and
%       \jkind\Delta\gnil {\!_{\Sigma'} K}{\Delta'} {K'}
%     }{
%       \jsig\Delta{\gcons\Sigma{{\cst a : K}}}{\Delta''}{\gcons{\Sigma'}{{\cst a : K'}}}
%     }

%     \infer{ }{
%       \jsig\Delta\gnil\Delta\gnil
%     }
%   \end{mathpar}

%   \fbox{$\jfam\Delta\Gamma A {\Delta'} {A'}$}
%   \qquad
%   {Family}

%   \begin{mathpar}

%     \infer{
%       \jfam\Delta\Gamma {A_1} {\Delta'} {A_1'}
%       \and
%       \jfam{\Delta'}{\gcons\Gamma{\tp {\var x} {A_1'}}} {A_2} {\Delta''} {A_2'}
%     }{
%       \jfam\Delta\Gamma {\prd x {A_1} {A_2}} {\Delta''} {\prd x {A_1'} {A_2'}}
%     }

%     \infer{
%       \Sigma(a) = K
%       \and
%       \jfl\Delta\Gamma K S {\Delta'} {S'}
%     }{
%       \jfam\Delta\Gamma {\app{\cst a} S} {\Delta'} {\app{\cst a} {S'}}
%     }
%   \end{mathpar}


%   \fbox{$\jfl\Delta\Gamma K S {\Delta'} {S'}$}
%   \qquad
%   {Family spine}

%   \begin{mathpar}
%     \infer{
%       \jm\Delta\Gamma M A {\Delta'} {M'}
%       \and
%       \jfl{\Delta'}\Gamma {\gsubst{K}{\msubst x {M'}}} S {\Delta''} {S'}
%     }{
%       \jfl\Delta\Gamma {\prd x {A} {K}} {\spinecons M S} {\Delta''} {\spinecons
%       {M'}{S'}}
%     }

%     \infer{ }{
%       \jfl\Delta\Gamma\type\spinenil\Delta\spinenil
%     }
%   \end{mathpar}

%   \fbox{$\jkind\Delta\Gamma K {\Delta'} {K'}$}
%   \qquad
%   {Kind}

%   \begin{mathpar}
%     \infer{
%       \jfam\Delta\Gamma A{\Delta'}{A'}
%       \and
%       \jkind{\Delta'}{\envcons\Gamma x A} K {\Delta''} {K'}
%     }{
%       \jkind\Delta\Gamma{\prd x A K}{\Delta''}{\prd x {A'}{K'}}
%     }

%     \infer{ }{\jkind\Delta\Gamma\type\Delta\type}
%   \end{mathpar}

%   \caption{Typing of signatures}
%   \label{fig:sign-typing}
% \end{figure}

\begin{definition}
  Let $\mr = (\Delta, \smeta X \sigma)$.  If $\jf \top\Delta\Gamma F
  {\Delta'} {\smeta {X'} {\sigma'}} P$, then we define the
  \emph{commit} operation $\commit\mr F = (\Delta', \smeta
  X\sigma)$. We say that $\mr$ is \emph{well-constructed} if $\mr =
  \commit\mmr F$ with either $\mmr=(\mapempty, \smeta X\sigma)$ or
  $\mmr$ well-constructed.
\end{definition}

\begin{lemma}
  If $\mr$ well-constructed, then $\checkout\mr$ is defined and is a
  value.
\end{lemma}

We can already state a conservativity result stating that our
algorithm accepts exactly the well-typed values of \LF:

\begin{theorem}
  If $F$ is a value, and $\mr$ is well-constructed then $\commit\mr F
  = \mmr$ iff $\jlfm\Gamma{\checkoutrr F}A$
\end{theorem}

\begin{figure}
  \fbox{$\jcc\Delta\Gamma T S {\Delta'} F$}
  \qquad
  {Term evaluation}

  \begin{mathpar}
    \infer[TLam]{
      \jcc{\Delta}\Gamma{\gsubst T
        {\msubst x {M}}} S {\Delta'} F
    }{
      \jcc\Delta\Gamma{\lam x T}{\spinecons M S}
      {\Delta'} F
    }

    \infer[TAtom]{
      \jcr\Delta\Gamma U {\Delta'} F
    }{
      \jcc\Delta\Gamma U \spinenil {\Delta'} {F}
    }
  \end{mathpar}

  \fbox{$\jcr\Delta\Gamma U {\Delta'} F$}
  \qquad
  {Atomic term evaluation}

  \begin{mathpar}
    % \infer[UVar]{ }{
    %   \jcr\Delta\Gamma{\app {\var x} S} \Delta {\app {\var x} S}
    % }

    % \infer[UConst]{ }{
    %   \jcr\Delta\Gamma{\app {\cst c} S} \Delta {\app {\cst c} S}
    % }
    % \infer[UEval]{
    %   (\fct f : A = T)\in\Sigma
    %   \and
    %   \jcc\Delta\Gamma A T S {\Delta'} F P
    % }{
    %   \jcr\Delta\Gamma{\app {\fct f} S} {\Delta'} F
    % }

    \infer[UAtom]{
      \jf{\fct{f}}\Delta\Gamma F {\Delta'} {F'} P
    }{
      \jcr\Delta\Gamma F {\Delta'}{F'}
    }\quad
    \begin{array}{l}
      \text{$\fct f$ fresh} \\
      F'\neq\smeta X\sigma
    \end{array}

    \infer[UMeta]{
      \jf{\fct{f}}\Delta\Gamma F {\Delta'} {\smeta X \sigma} P
    }{
      \jcr\Delta\Gamma F {\Delta'}{\Delta'(\smeta X \sigma)}
    }\quad
    {\text{($\fct f$ fresh)}}

  \infer[UCase]{
    \jcr\Delta{\gconcat\Gamma{\Gamma'}} U {\Delta'} {\app {H_i} {\spinecons
        {M_1}{\spinecons\ldots{M_{m_i}}}}}
    \and
    \jcr{\Delta'}\Gamma {\gsubst{U_i}{\msubst {x$_1$} {M_1} \ldots\
        \msubst {x$_{m_i}$} {M_{m_i}}}} {\Delta''} F
  }{
    \jcr\Delta\Gamma{\matchin{U}{\Gamma'}{
        (\casebnobar{\app {H_1}{\spinecons {\var x_{11}} {\spinecons
              \ldots\ {\var x_{1m_1}}}}}{U_1})\
        |\ \ldots\
        \caseb{(\app {H_n}{\spinecons {\var x_{n1}} {\spinecons
              \ldots\ {\var x_{nm_n}}}}}{U_n})
      }}{\Delta''} F
    }
  \end{mathpar}
  \caption{Evaluation of \CL\ terms}
  \label{fig:evaluation}
\end{figure}

Rule \textsc{FAppEval} calls the evaluation of a function, written as
a \CL\ term, on a spine. The rules of Fig. \ref{fig:evaluation} follow
the call-by-name strategy, so that an atom is only head-reduced when
destructed by a case analysis. We finish by a soundness result:

\begin{theorem}
  If $\mr$ well-constructed, and $\commit\mr F$ succeeds with $\mmr$,
  then $\checkout{\mmr}$ is a closed value, well-typed \wrt\ \LF.
\end{theorem}

\section{Related work}

\paragraph{Symbolic manipulation of proofs}

Several attempts to design a functional programming language are
proposed to compute over valid proof
derivations~\cite{cave2012,stampoulis2012,poswolsky2008}. Two
essential design aspects of these tools are the choice of the
representation for proof trees and the choice of the meta-language to
manipulate them. Like them, we commit ourselves to Higher-Order
Abstract Syntax (HOAS), and more specifically to a language of the
\LF\ family.

Unlike them, we follow a contract-based approach~\cite{wadler2009well}
to guarantee the validity of the proofs generated by the programs
written using our framework. Thus, our technique is similar to
\LF-based proof-carrying code~\cite{necula1997proof,appel1999proof},
and to the \sysname{LCF} architecture: an untrusted proof generator is
composed with a (small) trusted proof checker. A noticeable difference
is the fact that, in our setting, the checking of proofs is
interleaved with their generation.

This recent work~\cite{cave2012,stampoulis2012,poswolsky2008} tackles
the ambitious challenge of deciding statically if a proof-generating
function will always produce valid proof terms. On one hand, these
approaches are more satisfying than contract-based ones because of the
static guarantees. Besides, they are also more efficient since they
totally remove the need for dynamic verification of proof
well-formedness. On the other hand, they are based on more sophisticated
type systems, which augments the trusted base, and put more
constraints on the way programs are written.

% We write our proof-generating functions in a functional language
% equipped with a pattern matching capable of dealing with binders. Our
% syntax for patterns is close to Delphin's~\cite{poswolsky2008} or
% FreshML's~\cite{shinwell2003} in the sense that it uses a meta-level
% operation reminiscent of the $\nabla$ quantifier~\cite{gacek2011}
% instead of the meta-level binding operation~\cite{jay2009}in order to
% introduce bound names in the evaluation context. Unlike FreshML, our
% operational semantics does not allow bound names to escape their
% scope: a variable escaping a local scope introduced by a pattern
% matching would be rejected by dynamic contract-based verification just
% before the function exit. Yet, in contrast with Pure
% FreshML~\cite{pottier2007} or NomPa~\cite{pouillard2010}, we do not
% enforce this non-escaping property statically but dynamically.

\paragraph{Incremental computation}

Memoization is a widely used technique to reuse previously computed
outputs. The slicing of our values implements the same idea: naming
intermediate results of computation (derivations), and reuse of
computation results by metavariables. Whereas memoization embeds the
recognition of a previously seen input by searching in the associative
table, we delegate this to the user (or a front-end) who is
responsible for giving the delta: these deltas are more involved to
infer in the higher-order case, and may invoke reasons that are
beyond the scope of a basic memoized type-checker to justify the reuse
of a specific typing derivation. For instance, if term~$\tlam x A M$
has been assigned the type $A \rightarrow B$ leading to a typing
derivation $\md$, one can specialize its type to $A'\rightarrow B$ by
applying the subtyping rule directly on $\md$ and providing a proof
that $A' \rightarrow B \leq A \rightarrow B$.

There is more to incrementality than just memoization. Adaptive
functional programming techniques~\cite{acar2006,carlsson2002}
includes memoization but also, thanks to dependency tracking, allows
to recompute the output only for changed parts without even
considering unchanged parts. We do have an explicit representation of
changes (the deltas), but they still must describe the path down to
the changed part.

\section*{Conclusion and future work}
% 0.5 pages

We have presented a lightweight framework to program incremental
certifying type-checker. A prototype implementation is available
online\footnote{\url{http://www.cs.unibo.it/~puech/}} as well as
examples of type-checkers for several variants of
$\lambda$-calculus. This prototype comes as an \sysname{OCaml} library
with its \sysname{CamlP4} syntax extension, offering the commit and
checkout operations on repositories given an \LF\ signature and
\sysname{OCaml} implementations of the defined functions.

This system is meant to serve as a kernel for checking deltas inferred
by a front-end whose development is still to be done. This piece of
software will be responsible for comparing several versions of the same
source code to guess which modifications have been applied to it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{splncs}
\bibliography{../../english}


% \begin{thebibliography}{}
% \softraggedright
% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
% P. Q. Smith, and X. Y. Jones. ...reference text...
% \end{thebibliography}

\pagebreak
\appendix
\section{Appendix}

\begin{figure}
  \centering
%  \small
  \begin{mathpar}
    \infer{ }{\jts{\cst o} {\even}}

    \infer{\jts M \odd} {\jts {\s M} \even}

    \infer{\jts M \even} {\jts {\s M} \odd}

    \infer{\jts M \nat} {\jts {\s M} \nat}

    \infer{\jts M {A\to B} \and \jts N A} {\jts {\app M N} B}

    \infer{\infer*{}{\mbox{$
          \begin{array}{c}
            [\jts {\var x} A] \\[-0.3em]
            \vdots\\
            \jts M B
          \end{array}$
        }}} {\jts {\lam x M} {A\to B}}

    \infer{\jts M A \and \jsub A B}{\jts M B}

    \infer{\jts M \nat
      \and
      \jts N A
      \and
      \infer*{}{\mbox{$
        \begin{array}{c}
          [\jts {\var x} \nat] \quad
          [\jts {\var y} A] \\[-0.3em]
          \vdots \\
          \jts P A
        \end{array}
        $}}
    }{\jts {\recb M N x y P} A}

    \infer{ }{\jsub A A}

    \infer{ }{\jsub \even\nat}

    \infer{ }{\jsub \odd\nat}

    \infer{\jsub {A'} A \and \jsub B {B'} }{\jsub {A\to B}{A'\to B'}}
  \end{mathpar}
  \caption{System \sysname{T_{<:}}, declaratively}
  \label{fig:tsub}
\end{figure}


\paragraph{Substitution}

We overload \emph{substitution} as a partial operation on objects,
given a parallel substitution $\sigma$. Since the syntax of object is
not stable by (textual) substitution, it is defined as
\emph{hereditary}\footnote{ Note that this notion of substitution,
  present in \cite{pfenning2007term}, differs from the more standard
  definition in \cite{hl07mechanizing} as it presupposes $\eta$-long
  normal forms: once a \emph{cut} starts, there must be exactly as
  many formal and actual arguments. A non-empty spine cut against
  another non-empty spine $\gcut {\app H S} {S'}$ blocks the
  substitution instead of reducing to $\app H {(\gconcat S S')}$ (an
  implicit $\eta$-expansion); conversely, exhausting actual but not
  formal arguments as in $\gcut{\lam x M}{\gnil}$ blocks the
  substitution too. For example, the reduction $\gcut {\lam x \app
    {\var x}{\var x}} {\lam x \app {\var x}{\var x}}$ gets stuck
  because the second $\var x$ in the left part is substituted by a
  function yet is not itself a function. }: it is mutually recursive
with a \emph{cut} function $\gcut M S$ which can in turn trigger a
chain of substitutions, leading to a canonical objects or an error.
\begin{align*}
  \gsubst {(\lam x M)} \sigma &= \lam x {\gsubst M \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\cst c} S)} \sigma &= \app {\cst c} {(\gsubst S
    \sigma)}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \app {\var x} {\gsubst S \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \gcut M {\gsubst S \sigma} &
  \text{if $\sigma(\var x) = M$}
  \\
  \gsubst {\smeta X {\sigma}} {\sigma'} &=
  \smeta X {\gcomp\sigma{\sigma'}}
  \\
  \gsubst\spinenil\sigma &=
  \spinenil \\
  \gsubst{\spinecons M S}\sigma &=
  \spinecons{\gsubst M\sigma}{\gsubst S\sigma}
  \\
  \gcut {\lam x M} {\spinecons N S} &=
  \gcut {\gsubst M {\msubst x N}} S
  \\
  % TODO cas meta/S ? rem: on en a pas besoin ds le proto.
  % \gcut {\smeta X\sigma} S &=
  % \smeta X{\gcomp\sigma\sigma'}
  % \\
  \gcut {\app H S} \spinenil &=
  \app H S
  \\
  \gcut {\smeta X\sigma} \spinenil &= \smeta X\sigma
  \\
  \gcomp \sigma \msubstnil &=
  \sigma
  \\
  \gcomp \sigma {(\msubstcons{\sigma'} x M)} &=
  \gcomp {\gsubst\sigma {\msubst x M}} \sigma'
  \\
  \gsubst {(\msubstcons{\sigma'} x M)} \sigma &=
  \msubstcons{\gsubst{\sigma'}\sigma} x {\gsubst M \sigma}
  \\
  \gsubst {\msubstnil}{\sigma} &= \msubstnil
\end{align*}

We extend this operation trivially on families $\gsubst A \sigma$ and
kinds $\gsubst K \sigma$. We also allow to abusively write
non-canonical application $\app M S$ as a shorthand for $\gcut M S$
if it is defined.

% TODO pourquoi cette def?
% TODO terminaison?
% TODO stabilité de la eta?


\end{document}

%  LocalWords:  subtype
