\documentclass[9pt]{sigplanconf}

\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\usepackage{macros}
\usepackage{mathpartir}
\usepackage{url}

\begin{document}

\conferenceinfo{LFMTP '12}{September 9, 2012 -- Copenhagen, Denmark}
\copyrightyear{2012}
\copyrightdata{[to be supplied]}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Incremental tactics}
%\subtitle{Subtitle Text, if any}

\authorinfo{Matthias Puech}
           {Department of Comp. Sci., Univ. of Bologna,\\
             PPS, Team $\pi r^2$ (Univ. Paris Diderot, CNRS, INRIA)}
           {puech@cs.unibo.it}

\authorinfo{Yann R\'egis-Gianas}
           {PPS, Team $\pi r^2$ (Univ. Paris Diderot, CNRS, INRIA)}
           {yrg@pps.jussieu.fr}

\maketitle

%% Here is a problem
\begin{abstract}
 %% 4 sentences:
 %% State the problem
 %% Say why it’s an interesting problem
 %% Say what your solution achieves
 %% Say what follows from your solution
\end{abstract}

\category{D.3.3}{Language Constructs and Features}{Data types and
  structures} \category{F.3.1}{Logics and Meanings of
  Programs}{Specifying and Verifying and Reasoning about
  Programs --- Logics of programs}

\terms
Theory, Languages

\keywords
incrementality, type checking, logical framework, version control

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}
%% It’s an interesting problem (1 page)

%% Describe the problem (Use an example)

The positive impact of rich type systems on programming language
design is not to be demonstrated anymore, both in terms of the safety
of programs --- they ``do not go wrong'', and programming practices
--- modularity, early bug catching. Verifying well-typing of
real-world programs is thus a critical matter, but it is a complex
operation, for the compiler designer and for the human operator:

\paragraph{For the compiler designer}

Type systems are usually presented in a declarative manner as a set of
inference rules, together with proofs that all programs accepting a
derivation in this system respect a certain dependability claim, the
famous ``progress and preservation'' pair. Yet, actual algorithms of
verification often differ very much from their declarative
description. This is of course true when programs are provided with
little type annotations (``type inference''), in which case the
missing information has to be algorithmically synthesized, but is also
true in many cases for what is commonly referred as ``type checking'',
or explicit ``church-style'' calculi. Take for example dependent type
systems, relying on a certain notion of definitional equality on
types, or type systems with subtype polymorphism: both rely on a rule
of the form
$$
\infer{\Gamma\vdash M : A \\ A\leq B}{\Gamma\vdash M : B}
$$
(for a given notion of $\leq$) which is not syntax-directed. The
implementer then has to convert the declarative system to an
algorithm, and convince himself that both are equivalent. This task is
highly non-trivial and non-modular: for instance, how to infer the
type of a System \sysname{T} recursor
$$
\infer{
  \Gamma\vdash M : \cst{nat}
  \and
  \Gamma\vdash N : A
  \and
  \Gamma, \var x:\cst{nat}, \var y:A\vdash P : A
}{
  \Gamma\vdash \recb M N x y P
}
$$
in presence of subtyping? The problem is that we have to guess a type
$A$ more general than that of $N$ and $P$, while giving that type to
variable $\var y$. A well-known algorithm if the subtyping relation
forms a complete lattice is to iterate the typing of $P$ with the join
of the previously computed and the new type until a fixpoint is
reached.

The most common option is to formally prove the equivalence, \ie\ that
the (typing) relation is actually a function. Although these proofs
can be difficult to obtain, it is the most satisfactory because it
allows to take only the result of the algorithm into
consideration. But let us choose another path for a minute:
derivations in the declarative systems, if they cannot always be
easily inferred, can be \emph{verified} if provided by an oracle. If
the oracle is wrong, we can tell cheaply by verifying the witness
typing derivation it provides. Let us then imagine a type-checker to
be the pair of 1/ an untrusted procedure $\finfer M$ to infer a typing
derivation out of program $M$; it doesn't have to be complete, or even
sound, and 2/ a trusted and fast kernel $\fcheck{\md}$ to verify these
derivations in a given set of inference rules $\Sigma$. This is a
\emph{certifying} scheme: %TODO cite Leroy
if $\fcheck{\finfer M} = \cst{true}$ then there really \emph{is} a
derivation $\md$ for $M$, but the converse is not necessarily true:
function $\finfer{}\!\!$ could fail, or have bugs (return an incorrect
derivation). Explicitly generating the derivation in the declarative
system has the following advantages over the first approach:
\begin{itemize}
\item it is lightweight: the untrusted algorithm is usually shorter
  than the corresponding equivalence proof
\item compilers often need to propagate type information further in
  the chain of transformation; the typing derivation produced by
  $\finfer{}$ being explicit, it could constitute the data structure
  being exploited.
\item it inherits benefits from certifying approaches like
  proof-carrying code \cite{necula1997proof}: certificates can be
  transmitted with the program, saving regeneration time and avoiding
  to disclose the generation program.
\end{itemize}
\ldots\ but there is more:

\paragraph{For the programmer}

As type systems become richer, it becomes increasingly difficult for
the programmer to write a correct program in one try, compile it and
run it: writing a program longer than a few dozens of lines becomes a
tight and non-linear interaction between the human and the
type-checker involving constant experiments, fixes, etc. If this
interaction is possible for short programs and fast checkers, it
becomes increasingly hindered by the latency of complete rechecks:
even if the modifications made to the program between two interactions
are small, the whole program is usually rechecked. This is especially
true for languages embedding formal verification aspects, like proof
assistants: there, tight interaction with a type or proof checker is
unavoidable due to the difficulty of writing correct proofs without
guidance from the system, and the time taken by many tools to infer
easy parts of the proofs (proof search). The constant modification of
the source makes it necessary to observe \emph{incrementally} the
effect of a small change on the whole edifice: we need to take
advantage of the results of previous checks in order to build the
result for a modified version of the program.

Several methods have been devised for turning a batch procedure into
an incremental one \cite{pugh1989incremental,
  acar2003selective}. Unfortunately, they all suffer from their
genericity on the particular problem of type checking:
\begin{itemize}
\item first, they ignore the higher-order nature of program syntax,
  and the kind of structural quotients we make on syntax trees with
  binders and environments ($\alpha$-equivalence, weakening\ldots): a
  hypothetical derivation of\ \ $\vdash x+2 : \cst{nat}$ is still valid
  when put under a binder\ \ $\vdash\lam y (x + 2) -
  y:\cst{nat}\to\cst{nat}$ and should not be regenerated;
\item for very large programs, we might want to feed the type checker
  only with a small \emph{delta} on the previous version and not the
  whole program, thus saving the time to recognize already checked
  parts;
\item finally, there is no way to \emph{a posteriori} validate the
  result of the incremental process: the user has to trust not only
  the type-checking algorithm but also its transformation into an
  incremental one, which increases the size of the trusted base.
\end{itemize}

Now, if we consider as above a type-checker as a derivation-generating
procedure, and storing these derivations between calls to the
procedure, we could recognize already-built subderivations for
subterms common to a checked and a new version of the program, and
graft these pieces of derivation to form a new, large one for the
whole modified program, saving the regeneration of all equal subterms'
derivations in compatible contexts. This way we constantly maintain
the verifiable witness of well-typing for the program, and take
advantage of the quotients made on higher-order syntax.

We can thus add to the previous list that explicitly generating typing
derivations makes possible a form of safe incrementality by sharing
common subderivations.

In this paper, we present a framework to incrementally generate typing
certificates represented in the \LF\ logical framework, and implemented
as an \sysname{ML} library. We specifically propose:
\begin{itemize}
\item a simple language for representing programs and proof
  deltas, derived from contextual \LF\ \cite{nanevski2008contextual};
\item a computational language to write untrusted
  derivation-generating type checkers, based on a notion of function
  inverse %TODO le nom?
  to refer to already-computed values;
\item a generic algorithm to evaluate these type-checkers on program
  deltas and verify on-the-fly the generated certificates
  incrementally against a \emph{repository} of already constructed and
  checked derivations.
\end{itemize}

\section{Use case}
%% It’s an unsolved problem (1 page)

\subsection{A backend for incremental type checking}
\label{sec:use-incremental}

A student is learning about System \sysname T --- a simply typed
$\lambda$-calculus with natural numbers introduction $\z$ and $\s{M}$,
and elimination ($\recb M N x y P$).  He (or she) wants to write a
program computing $(2+2)\times 3$. He knows that sometimes those
$\cst{rec}$ arguments are quite hard to get correct, but System
\sysname T is a typed language; he wants to take advantage of this to
build his program incrementally using the teacher's provided
incremental verifier. As a first attempt, he wants to verify $P_1$:
$$
\recb{\s\z}{\s\z} x y {\s {\var x}}\ \text{.}
$$
For this, he sends the command $\finfer{P_1}$ to the system. The
$\finfer{}$ operator takes a well-formed program and tries to construct
a valid typing derivation for its argument in the current, empty
context. The system succeeds and returns a typing derivation $\md_1$
of \/ $\vdash P_1 : \cst{nat}$, valid in an empty context.

The student then decides to factorize a definition for $\var{add}$ out
of his code:
\begin{align*}
  &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
    {\var y} u v {\s{\var u}}}
  \\
  &\quad\app{\var{add}}\app{(\s\z)}{(\s\z)} \ \text{.}
\end{align*}
But he (or his text editor) realizes that some parts of this program
have not changed: both $\s\z$, and the function body $\s{\var x}$, and
he could reuse the pieces of derivation just built. Let us call these
respectively $\md_2$, $\md_3$ and $\md_4$. $\md_4$ is hypothetical: it
depends on the hypothesis $H$ that\ \ $\vdash \var x:\cst{nat}$ .
Provided there is an operator $\fget\md$ from the derivation to the
program it types, what needs really to be sent to the system is:
\begin{align*}
  &\finfer{}(
    \letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
      {\var y} u v {\fget{\gsubst{\md_4}{\msubst{H}{\finfer{\var u}}}}}}
    \\&\qquad
    \app{\var{add}}\app{\fget{\md_2}}{\fget{\md_2}}
  )
\ \text{,}
\end{align*}
that is, only the changed subterms of the program. Derivations $\md_2$
used to live in the empty context, we are then free to use it in the
new context where $\var{add}:\cst{nat}\to\cst{nat}$, because System
\sysname T enjoys the \emph{weakening} property; $\md_4$ depending on
a derivation\ \ $\vdash\var x:\cst{nat}$, we instantiate this hole
with the derivation $\finfer{\var u}$ inferred recursively for then
new recursive argument $\var u$. Taking advantage of $\md_2$ and
$\md_4$ the system succeeds with a new derivation made out of these,
without having to regenerate them because it knows that
$\finfer{\fget\md} = \md$.

Let us call $\md_5$ the subderivation generated for $\tlam x
{\cst{nat}} \tlam y {\cst{nat}} \recb {\var x} {\var y} u v
{\fget{\gsubst{\md_4}{\msubst H {\finfer{\var u}}}}}$, $\md_6$ for the
arguments $\app{\fget{\md_2}}{\fget{\md_2}}$ and $\md_7$ for the call
$\app{\var{add}} {\fget{\md_6}}$. Even if it occurs originally in the
scope of $\var{add}$, $\md_6$ doesn't depend on the hypothesis $\var
H$ that $\vdash \var{add}:\cst{nat}\to\cst{nat}$ because of the
\emph{strengthening} property. Contrarily, $\md_7$ does: each time we
want to reuse it, we will have to provide a proof of this fact.

The student then provides a definition for multiplication, and writes
the operation $(2 + 2)\times3$:
\begin{align*}
  \finfer{}(
  &\letb{add}{\fget{\md_5}}
  \\
  &\quad\letb{mul} { \tlam x {\cst{nat}} \tlam y
    {\cst{nat}} \recb \z {\var y} z t {
      \app{\var{add}}\app{\var x} {\var z}} }
  \\
  &\quad\quad\app{\var{mul}}\app
  {(\fget{\gsubst{\md_7}{\msubst H {\finfer{\var{add}}}}})}
  {(\s{\fget{\md_2}})}
  )
  \ \text{.}
\end{align*}
Here again, he took care of reusing all derivations for equal
subterms. In particular, $\md_7$ expects a proof that the variable
$\var{add}$ is of type $\cst{nat}\to\cst{nat}$; he provides
$\finfer{\var{add}}$ meaning: at this point we will have generated (or
reused in this case) the derivation of $\var{add}$, so we just have to
use it here.

Finally, he decides (in an odd move) to inline the definition of
$\var{mul}$ into its call site. Call $\md_8$ the derivation of the
definition of $\var{mul}$ (it depends on an $\var H$ of
$\vdash\var{add}:\cst{nat}\to\cst{nat}$) and $\md_9$ the derivation of
$(2+2)\times 3$ depending on derivations $\var{H}$ and $\var{H'}$ for $+$ and
$\times$. He orders:
\begin{align*}
\finfer{}(
\letb{add}{\fget{\md_5}}
\fget{({\gsubst{\md_9} {\msubst {H} {(\fget{\var{add}})}; \msubst {H'}
      {\gsubst{\md_8}{\msubst H {(\fget{\var{add}})}}}}})}
)
  \ \text{.}
\end{align*}
The definition for $\var{add}$ is unchanged, but the instantiation of
open derivations enables to directly substitute one in the other and
get the appropriate program: call $\md_{10}$ the whole derivation just
returned, $\fget{\md_{10}}$ returns:
\begin{align*}
  &\letb{add}{\tlam x {\cst{nat}} \tlam y {\cst{nat}} \recb {\var x}
    {\var y} x y {\s{\var x}}}
  \\&\quad
  {\tlam x {\cst{nat}} \tlam y
    {\cst{nat}} \recb \z {\var y} z t {
      \app{\var{add}}\app{\var x} {\var z}}}
  \\&\quad\quad
  \app{(\app {\var{add}} \app {(\s{\s\z})}
    {(\s{\s\z})})}{(\s{\s{\s\z}})}
  \ \text{.}
\end{align*}

\paragraph{Discussion}

We choose here to represent deltas as usual terms with variables $\var
x, \var y$ and special, \emph{meta}-variables referring to
already-computed results $\md_i$. Commands sent to the system consists
of terms, possibly containing special typed operators having a
computational content: $\finfer{\cdot}$ takes a term $M$ and produces
the (dependent) pair of a type $A$ and a derivation of the judgment
$\vdash M:A$ in the current context; $\fget{\cdot}$ does the exact
converse: it takes a derivation $\vdash M:A$ and projects it back as
the term $M$. The successful result of a command execution is a
derivation where each subderivation has been given a unique
metavariable name for later reuse (a \emph{repository}). As
subderivations can have free variables, we close them by a local
environment that must be instantiated by a \emph{substitution}.

Although this small example may seem anecdotal, the interest of such a
mechanism becomes clear on very large programs: then the cost of
rechecking well-typing of a small modification amounts to rechecking
the path in the term from the root to the changed part, and the
changed part itself.

%TODO figure?

Seeing type-checking as issuing a completely explicit typing
derivation enables thus to think of the relationship between the input
and the output. Feeding subterms of the output $\md_i$ back into
subterms of the input $P_i$ is a well-known technique for handling
changes in the input: combined with a way to automatically recognize
already-computed input, it is \emph{memoization}, a general-purpose
technique for incremental computation that has been extensively
studied and combined with other techniques \cite{pugh1989incremental,
  acar2003selective}. However, it is best understood in the case of
purely first-order data, and to our best knowledge remains unexplored
for higher-order structures containing binders, like programs and
proofs.

A memoized type-checker stores in a table bindings from its input (the
program) to its output (the derivation), and returns automatically the
stored output if the current input matches a binding in the table. It
thus performs two operations at once: output reuse and recognition of
already-seen input. Recognition --- that is from a modified program,
generate a delta from previous versions --- is in the higher-order
case a complex task that we hope to address in the future; we tackle
here the verification of output reuse: given a delta, construct a new
derivation and verify it.

\subsection{A language to write incremental type-checkers}

A few days earlier, the teacher is preparing the tool for the students
to train composing System \sysname{T} programs. He (or she) provides
the abstract syntax, the typing rules and the typing untrusted
algorithm in order to build the incremental type checker. How should
he write the algorithm? We know that we want to return a derivation,
and take advantage of a higher-order representation of the syntax, so
it shouldn't be necessary to thread any explicit environment. The
definition should then start with:

\begin{mathleft}
  \finfer{}\ :\ \prd M {\cst{tm}} \sig A {\cst{tp}} (\vdash M : A) =
\end{mathleft}

\noindent
It takes a term $M$ and producing the pair of a type $A$ and a
derivation that $M$ has type $A$. Its code will be untyped, but
the teacher provides this type so that the checker can verify that
each call is fed and returns values of the right type: it is its
\emph{contract} so to say. He first decomposes term $M$ and treats
the easy cases of $\cst{o}$, $\cst{s}$ and $\cst{app}$:

\begin{mathleft}
  \lamd M \match{M} \\
  \quad\case{\z} \pair {\cst{nat}}{\infer{ }{\vdash \cst o : \cst{nat}}} \\
  \quad\case{\s M}
  \letd {\pair {\cst{nat}} \md} {\finfer{M}}
  \pair {\cst{nat}}{\infer\md{\vdash\app{\cst{s}} M : \cst{nat}}} \\
  \quad\case{\app{M}N}
  \letd {\pair {A_1\to B} {\md_1}} {\finfer{M}} \\
  \quad\quad\letd {\pair {A_2} {\md_2}} {\finfer{N}} \\
  \quad\quad\letd {\cst{true}} {\feq {A_1} {A_2}}
  \pair {B} {\infer{\md_1 \and \md_2}{\vdash \app M N : B}}
\end{mathleft}

\noindent
We write $\pair{\cdot}{\cdot}$ the constructor of $\Sigma$-types; the
$\syntax{let}$ construct is just syntactic sugar for a one-branch
$\syntax{case}$; patterns don't have to be exhaustive since we are in
an untrusted setting; for conciseness, we use case failure for
signaling typing error; we use an auxiliary function $\!\!\feq{}{}\!\!
: \cst{tp}\to\cst{tp}\to\cst{bool}$ for syntactic comparison of types,
defined the same way.

Then comes the abstraction case $\tlam x A M$. The difficulty is that
the recursive call $\finfer{M}$ must be done in an enlarged
environment where $\vdash x : A$, otherwise the check that the
returned open derivation $\md$ is correct would fail ($x$ is
unknown). We thus introduce a new construct $\matchin M \Gamma \ldots$
and its $\syntax{let}$ counterpart ($\match M \ldots$ being syntactic
sugar for $\matchin M \envnil \ldots$). Even then, in the recursive
call, arriving to variable $\var x$ would fail since we would have no
way of telling its type anymore. Thus we substitute all occurrences of
$\var x$ with the \emph{output} of function $\finfer{}$ on it (since
it is known at this point), \ie\ a pair of $A$ and $\md_1$. For this
substitution to be well-typed, we now need to coerce this output to a
valid input (a term). We introduce an operator which is in some way
the \emph{inverse} of $\finfer{}$: operator $\fget{}$ maps a pair of a
type and a derivation to the term it types.

\begin{mathleft}
  \quad\case{\tlam x A M} \\
  \quad\quad
  \letdin{\pair B \md}{\env {$\md_{\var x}$} {(\vdash {\var x} : A)}}
  {\finfer {\gsubst M {\msubst x {\fget{\pair A {\md_{\var x}}}}}}}
\end{mathleft}

At this point, we get back $B$ and $\md$ from the recursive call. We
know that $B$ can't depend on $\md_x$ (because a type is always
closed), but $\md$ can: it is not \emph{per se} a hypothetical
derivation but an \emph{open} derivation where a special judgement
named $\md_x$ can occur. To return the complete derivation of
$\vdash\tlam A x M : A \to B$, we must then close, or ``hypothesize''
$\md$ by $\md_x$, and then apply rule \rulename{Lam}:

\begin{mathleft}
  \quad\quad
  \pair {A\to B}
  {\infer{
\mbox{$      \begin{array}{c}
        {[\md_{\var x}]} \\
        {\md}
      \end{array}
    $}    }{
    \vdash\lam x M : A\to B
  }}
\end{mathleft}

The case of $\cst{rec}$ is similar: we first compute recursively the
derivations $\md_M$ and $\md_N$ which should be well-typed in the
current environment, and then $\md_P$ in the enlarged environment,
with variables $\var x$ and $\var y$ in $P$ substituted with the
(future) result of $\finfer{}$.

\begin{mathleft}
  \quad\case{\recb M N x y P} \\
  \quad\quad
  \letd {\pair {\cst{nat}} {\md_M}} {\finfer{M}} \\
  \quad\quad
  \letd {\pair {A_1} {\md_N}} {\finfer{N}} \\
  \quad\quad
  \syntax{let} {\pair {A_2} {\md_P}} \syntax{in}
  {(\envcons {\env {$\md_x$} {(\vdash
        \var x : \cst{nat})}} {$\md_y$} {(\vdash\var y : A_1)})} = \\
  \quad\quad\quad
  {\finfer{\gsubst P {\msubstcons {\msubst x
          {\fget{\pair{\cst{nat}}{\md_x}}}} y
        {\fget{\pair{A_1}{\md_y}}}}}} \\
  \quad\quad\letd {\cst{true}} {\feq {A_1} {A_2}} \\
  \quad\quad
  \pair {A} {
    \infer{
      \md_M \and
      \md_N \and
      \mbox{$
        \begin{array}{c}
          [\md_x] [\md_y] \\
          \md_P
      \end{array}
        $}
    }{
      \vdash \recb M N x y P
    }
  }
\end{mathleft}

The type-checker is now done. There is no case for variables, as we
won't ever meet this case: we already substituted all variables with
their result derivation. We only need to make sure that in this case,
the derivation will be returned:
$$
\finfer{\fget{\pair A \md}} = \md
$$
Now $\fget{}$ can be derived automatically from the type of
$\finfer{}$: it is its inverse. It has definition:

\begin{mathleft}
  \fget{}\ :\ \prd{\{M\}}{\cst{tm}} \prd {A}{\cst{tp}} \prd{\md}{(\vdash
    M:A)} \cst{tm} =
  \\\quad \lamd M \lamd A \lamd \md M
\end{mathleft}

\noindent
where argument $M$ is implicit.

\paragraph{Discussion}

We exploit here an idea similar to run-time
\emph{contracts}: %TODO cite
there is no static guarantee that computations won't return ill-typed
values, but arguments and results are checked at run-time at the
boundaries of defined functions. This allows to omit formal
justifications, like the fact that $B$ can't depend on $\md_x$ in the
$\lambda$ case, but of course incurs on the performance. This is
compensated by the slicing mechanism described below in
\ref{sec:repr}.

Introducing the ``inverse'' of the derivation inference function has
two purpose: as we saw in \ref{sec:use-incremental}, it allows to
refer to already-constructed derivations when writing deltas. It also
permits to abstract from the concrete environment when writing the
type-checker itself, that one must thread throughout the process in a
traditional, first-order type-checker. It is a generalization of the
so-called \emph{context-free} typing presented in
\cite[chap. 4]{boespflug2011conception} where a special variable
construct $\{x:A\}$ denoting free variables is added to the syntax of
terms, and the rules
$$
\infer{
  \vdash \gsubst M {\msubst x {\{\var x:A\}}} : B
}{
  \vdash \tlam x A M : A\to B
}
\qquad\text{and}\qquad
\infer{ }{
  \vdash \{\var x : A\} : A
}
$$
replace the usual ones. The difficulty with this technique,
particularly in a dependent setting, is to ensure that term $\{\var
x:A\}$ reduces to $\var x$ so that they are considered equal, \ie\ we
can strip off the annotations, while not forgetting them too soon:
otherwise we would arrive to the unhandled case $\finfer{\var
  x}$. This requires evaluating the type-checker with a carefully
designed strategy.

For the sake of conciseness, we took a simple, syntax-directed system
as example; this scales nonetheless to declaratively presented
features like subtyping or conversion.

\section{Here is my idea}
%% Here is my idea (2 pages)

% lambda-lifting

\subsection{The representation language}
\label{sec:repr}

The emission of certificates by untrusted applications first raises
the question of the \emph{data structure} to use for these
certificates. The requirements are:
\begin{itemize}
\item conciseness of the certificates, since they are stored or even
  transferred remotely;
\item a well-understood, short and independent verification algorithm.
  This is known as the \emph{de Bruijn principle};
\item universality with respect to the particular problem: having a
  certificate language for each domain of application would just shift
  the problem of trust; having one universally accepted certificate
  language that can be tailored to many problems puts a much stronger
  trust on the unique verifier.
\end{itemize}

The Edinburgh Logical Framework \cite{harper1993framework}, or \LF\
for short, is a well-accepted solution to this question. It is a typed
lambda-calculus and can be viewed as a language for
\emph{representing} terms, derivations and statements of $\Pi_0^1$
meta-theorems (but not their proofs) in a given inference system
supporting hypothetical judgments \cite{pfenning2001logical}. For
these, it provides built-in support for encoding languages with
binders and hypothesis (a feature called $\lambda$-tree syntax or HOAS
\cite{pfenning1988higher}). The user gives a \emph{signature} defining
abstract syntax and inference rules of his \emph{object language}, and
gets an algorithm to verify terms and derivations expressed in the
\LF\ syntax.

For instance, the rule for System \sysname{T}'s recursor can be
encoded by the following constant:

\begin{mathleft}
  \cst{is\_rec}\ :\
  \prd{M}{\cst{tm}}\prd{N}{\cst{tm}}\prd{P}{\cst{tm}\to\cst{tm}\to\cst{tm}}
  \\\quad
  \prd A {\cst{tp}}
  \app {\cst{is}}\app M {\cst{nat}} \to \app{\cst{is}}\app N A \to \\\quad
  \left(
  \prd x {\cst{tm}} \prd y {\cst{tm}}
  \app{\cst{is}}\app x \cst{nat} \to \app{\cst{is}}\app y A \to
  \app{\cst{is}}\app {(\app P \app x y)} A
  \right)\to\\\quad
  \app{\cst{is}}\app{(\app{\cst{rec}}\app M \app N (\lam x\lam y \app
    P\app x y))} A
\end{mathleft}

This framework has been used to issue certificates for a
number of practical applications like proof-carrying code
\cite{necula1997proof}, authentication framework \cite{appel1999proof}
\ldots %TODO ATP

To write deltas by sharing subterms, we need to be able to address any
subterm and instantiate its free variables. For this, we define the
the \emph{slicing} of a well-typed \LF\ term: we extend \LF\ with
metavariables $\meta X$, $\meta Y$\ldots\ as in
\cite{nanevski2008contextual}. Metavariables stand for well-typed
terms defined in a repository, and are instantiated wrt. their free
variables. A repository also contains a distinguished metavariable
corresponding to the head of the term stored in it. For instance, the
repository
$$
\left(\begin{array}{llll}
    \meta X &\mapsto
    \envnil&\vdash
    \app{\cst{lam}} \lam x \smeta Y {\msubst u {\smeta T {\msubst w x}}} &:
    \cst{tm}
    \\
    \meta Y &\mapsto
    \env u {\cst{tm}} &\vdash
    \app{\cst{lam}} \lam y \smeta Z {\msubstcons {\msubst u
      {\var u}} v {\var y} } &:
    \cst{tm}
    \\
    \meta Z &\mapsto
    \envcons {\env u {\cst{tm}}} v {\cst{tm}} &\vdash
    \app{\app{\cst{app}}{\var v}}{\var u} &:
    \cst{tm}
    \\
    \meta T &\mapsto
    \env x {\cst{tm}} &\vdash
    \app{\cst{app}}\app{\var x}{\cst{o}} &:
    \cst{tm}

\end{array}\right), \smeta X {}
$$
is one of the possible way of slicing of the term
$$
\fbox{$\app {\cst{lam}} \lam x
\fbox{$\app {\cst{lam}} \lam y
\fbox{\app {\cst{app}} \app {\var y}
}$}
{(\fbox{\app
  {\cst{app}}\app{\var x}{\cst o}})
}
$}$$
where all boxed subterms are referable slices.

\subsection{The computational calculus}

% inverse

\section{The calculus}
%% My idea works (details, data) (5 pages)

\subsection{Value calculus}

The value calculus defines the \emph{values} we want to
manipulate. These contain constants, and --- contrarily to \eg\
\sysname{ML} values --- binders and variables enabling $\lambda$-tree
syntax. Thus, they embed a superficial notion of computation:
hereditary substitution. This calculus is an extension of \emph{spine
  canonical \LF} \cite{pfenning2007term} enriched with metavariables
which are references to open terms as in
\cite{nanevski2008contextual}. We present its typing algorithm
relatively to a signature. It features an explicit caching mechanism
using metavariables.

\paragraph{Syntax}

In this variant, only $\eta$-long and $\beta$-normal values have an
existence: canonicity for $\beta$-reduction is enforced in the syntax
and canonicity for $\eta$-expansion will be enforced by typing,
following \cite{hl07mechanizing}. Besides, application of a
\emph{head} (\ie\ a constant or variable) is $n$-ary (\ie\ to a
\emph{spine} of objects) as opposed to the more standard binary
application: this eases the definition substitution and
$\eta$-expansion, and corresponds to a focused sequent calculus known
as \sysname{LJT} \cite{herbelin1995λ}.

\begin{align*}
  K &\gequal
  \prd x A K \gor
  \type &
  \text{Kind}\\
  A &\gequal
  \prd x A A \gor
  \app {\cst a} S &
  \text{Type family} \\
  M &\gequal
  \lam x M \gor
  F &
  \text{Canonical object} \\
  F &\gequal \app H S
  \gor
  \smeta X \sigma &
  \text{Atomic object} \\
  H &\gequal
  \var x \gor
  \cst c &
  \text{Head}\\
  S &\gequal
  \spinenil \gor
  \spinecons M S &
  \text{Spine}\\
  \sigma &\gequal
  \msubstnil \gor
  \msubstcons \sigma x M &
  \text{Parallel substitution} \\
  \Gamma &\gequal
  \envnil \gor
  \envcons \Gamma x A &
  \text{Environment}
\end{align*}

As is customary, we adopt the Barendregt convention for naming bound
variables, write $A\to A'$ for $\prd x A A'$ when $\var x\notin \FV{A'}$,
$\meta X$ for $\smeta X \msubstnil$, and adopt indifferently list, map
or set notations for $\sigma$ and $\Gamma$. The identity substitution
$\msubstid{\Gamma}$ is a notation for $\{\msubst x {\var x}\ |\ \var x
\in\dom\Gamma\}$.

Metavariables $\smeta X \sigma$ stand for open objects with free
variables in $\dom{\sigma}$; To each use of them is attached a
substitution $\sigma$ defining these free variables. They are defined
in a repository (see below). We write $\FMV{M}$ for the set of
metavariables in $M$ and speak of a \emph{metaclosed} object $M$ if
$\FMV{M} = \emptyset$.

We distinguish syntactically between \emph{checkable} and
\emph{inferrable} objects (resp. \emph{canonical} and \emph{atomic}),
an idea dating back to \cite{pierce2000local}: canonical objects are
checked against a given type, whereas we can synthesize the type of
atomic objects; the coercion from the first to the second will be the
place where to verify equality between the synthesized type and the
given one.

\paragraph{Substitution}

We overload \emph{substitution} as a partial operation on objects,
given a parallel substitution $\sigma$. Since the syntax of object is
not stable by (textual) substitution, it is defined as
\emph{hereditary}: it is mutually recursive with a \emph{cut} function
$\gcut M S$ which can in turn trigger a chain of substitutions,
leading to a canonical objects or an error.
\begin{align*}
  \gsubst {(\lam x M)} \sigma &= \lam x {\gsubst M \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\cst c} S)} \sigma &= \app {\cst c} {(\gsubst S
    \sigma)}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \app {\var x} {\gsubst S \sigma} &
  \text{if $\var x\notin \dom{\sigma}$}
  \\
  \gsubst {(\app {\var x} S)} \sigma &=
  \gcut M {\gsubst S \sigma} &
  \text{if $\sigma(\var x) = M$}
  \\
  \gsubst {\smeta X {\sigma}} {\sigma'} &=
  \smeta X {\gcomp\sigma{\sigma'}}
  \\
  \gsubst\spinenil\sigma &=
  \spinenil \\
  \gsubst{\spinecons M S}\sigma &=
  \spinecons{\gsubst M\sigma}{\gsubst S\sigma}
  \\
  \gcut {\lam x M} {\spinecons N S} &=
  \gcut {\gsubst M {\msubst x N}} S
  \\
  % TODO cas meta/S ? rem: on en a pas besoin ds le proto.
  % \gcut {\smeta X\sigma} S &=
  % \smeta X{\gcomp\sigma\sigma'}
  % \\
  \gcut {\app H S} \spinenil &=
  \app H S
  \\
  \gcut {\smeta X\sigma} \spinenil &= \smeta X\sigma
  \\
  \gcomp \sigma \msubstnil &=
  \sigma
  \\
  \gcomp \sigma {(\msubstcons{\sigma'} x M)} &=
  \gcomp {\gsubst\sigma {\msubst x M}} \sigma'
  \\
  \gsubst {(\msubstcons{\sigma'} x M)} \sigma &=
  \msubstcons{\gsubst{\sigma'}\sigma} x {\gsubst M \sigma}
  \\
  \gsubst {\msubstnil}{\sigma} &= \msubstnil
\end{align*}

We extend this operation trivially on families $\gsubst A \sigma$ and
kinds $\gsubst K \sigma$. We also allow to abusively write
non-canonical application $\app M S$ as a shorthand for $\gcut M S$
if it is defined.

Note that this notion of substitution differs from hereditary
substitution found in \eg\ \cite{hl07mechanizing} as it presupposes
$\eta$-long normal forms: once a \emph{cut} starts, there must be
exactly as many formal and actual arguments. A non-empty spine cut
against another non-empty spine $\gcut {\app H S} {S'}$ blocks the
substitution instead of reducing to $\app H {(\gconcat S S')}$ (an
implicit $\eta$-expansion); conversely, exhausting actual but not
formal arguments as in $\gcut{\lam x M}{\gnil}$ blocks the
substitution too. For example, the reduction $\gcut {\lam x \app {\var
    x}{\var x}} {\lam x \app {\var x}{\var x}}$ gets stuck because the
second $\var x$ in the left part is substituted by a function yet is
not itself a function.

% TODO pourquoi cette def?
% TODO terminaison?
% TODO stabilité de la eta?

\paragraph{Typing algorithm}

Typing an object while storing intermediate derivations requires
threading and maintaining a data structure of named partial
derivations (\emph{slices}) throughout the typing process. We present
a bidirectional typing algorithm updating an input repository.

A \emph{signature} declares object and type constants to be used in
objects. To each object constant we assign an annotation $L$ setting
the slicing behaviour of the constant during type checking: either
sliceable $\annsliceable$ (each application of this constant will be
cached and given a name), non-sliceable $\annnonsliceable$ (we don't
create a name for applications of this constant), or defined
$\anndefined{T}$ (see \ref{sec:computational-calculus}).
\begin{align*}
  L &\gequal
  \annsliceable \gor
  \annnonsliceable \gor
  \anndefined{T}
  \\
  \Sigma &\gequal
  \gnil \gor
  \gcons \Sigma {\cst c :^L A} \gor
  \gcons \Sigma {\cst a : K}
  &\text{Signature}
\end{align*}
We write $\Sigma(\cst c) = A; L$ for $(\cst c :^L A) \in \Sigma$
suggesting the implementation of signatures as maps.

Our algorithm types exactly the well-typed objects of the usual
\LF, except that we thread a repository $\mr$ and store in it
all well-typed applicative subterms $F$ along with their classifier
$A$ and the environment in which they are well-typed.

% TODO parler d'effacer les R et retomber sur LF
% TODO expl. c'est tout comme LF sauf qqes règles...
% TODO parler de la redondance en LF normal

A \emph{repository} $\mr$ is a finite map from metavariable to
judgements of object well-typing, along with a distinguished
metavariable, the \emph{head}:
$$ \mr\;:\;(\meta X \mapsto (\Gamma\vdash\tp M A)), \smeta X \sigma $$
We maintain the invariant that a repository $\mr$ is
\emph{acyclic}, that is for all $\meta X$ in $\mr$, the fixpoint
of $\FMV{\meta X}$ does not contain $\meta X$. If $\mr =
(\Delta, X[\sigma])$, we write $\hat {\mr}$ for $X[\sigma]$,
abusively $\mr$ for $\Delta$ and $\mr (\smeta X \sigma)$ for
$\gsubst M\sigma$ if $\Delta(\meta X) = (\Gamma\vdash M : A)$.

We refer to cached subterms by metavariables. We force instantiation
of all variables in their environment by attaching a substitution to
them ($\smeta X \sigma$).

When checking large objects, the environment can grow fast. As an
optimization to reduce the size of the stored environments and
substitutions in slices $M$, we strengthen environments to their
minimum size (the free variables of $M$): We define the operation of
\emph{strengthening} from an environment $\Gamma$ and an object $M$
well-typed in $\Gamma$ to the minimal environment in which $M$ can
actually be typed, \ie\ containing only its free variables:
$$
\stren{\Gamma,M} = \{x : A\ |\ x\in \FV{M} \wedge \Gamma(x)=A\}
$$
% TODO lemme qui dit que c'est correct wrt variables libres des A et
% du type

The algorithm is presented in Fig. \ref{fig:obj-typing} as a
syntax-directed inference system. All judgments are of the form $\gjo
U V W$ where $U$ and $V$ are inputs, $V$ is the induction hypothesis
and $W$ the output. They are parameterized by a constant and implicit
signature $\Sigma$, and all premises are ordered in the algorithmic
order of computation.

\begin{figure*}

  \fbox{$\jm\mr\Gamma M A \mmr {M'}$}
  \qquad
  Canonical object

  \begin{mathpar}
    % Lambda
    \infer{
      \jm\mr{\envcons\Gamma x A} M {A'} \mmr {M'}
    }{
      \jm\mr\Gamma {\lam x M} {\prd x A {A'}} \mmr {\lam x M'}
    }

    % App
    \infer{
      \jf\mr\Gamma F \mmr {F'} {A'}
       \and
      \jea\mmr\Gamma {\app{\cst a} S} {A'}
    }{
      \jm\mr\Gamma F {\app{\cst a} S} \mmr {F'}
    }
    % TODO commenter type ground
  \end{mathpar}

  \fbox{$\jf\mr\Gamma F \mmr F A$}
  \qquad
  {Atomic object}

  \begin{mathpar}

    % Non-sliceable
    \infer{
      \jh\mr\Gamma H A \annnonsliceable
      \and
      \jl\mr\Gamma A S \mmr {S'} {A'}
    }{
      \jf\mr\Gamma {\app H S} \mmr {\app H {S'}} {A'}
    }

    % Meta
    \infer{
      \mr(\meta X) = (\Gamma'\vdash M:A)
      \and
      \js\mr\Gamma \sigma {\Gamma'} \mmr {\sigma'}
    }{
      \jf\mr\Gamma {\smeta X \sigma} \mmr {\smeta X {\sigma'}} {\gsubst{A}\sigma'}
    }

    % Sliceable
    \infer{
      \jh\mr\Gamma H A \annsliceable
      \and
      \jl\mr\Gamma A S \mmr {S'} {A'}
      \and
      \stren{\Gamma, \app H {S'}}=\Gamma'
    }{
      \jf\mr\Gamma {\app H S} {\mapadd\mmr{\meta X}{(\Gamma'\vdash
          {\app H {S'}}:A)}} {\smeta X{\msubstid{\Gamma'}}} {A'}
    }
    \quad\text{($\meta X$ fresh in $\mr$)}

    % Defined
    \infer{
      \jh\mr\Gamma H A {\anndefined T}
      \and
      \jl\mr\Gamma A S {\mmr} {S'} {A'}
      \and
      \jcc\mr\Gamma A T S \mmmr F
      \and
      \jm\mmmr\Gamma F {A'} {\repo {R'''} } {F'}
    }{
      \jf\mr\Gamma {\app H S} {\repo{R'''}} {F'} {A'}
    }

  \end{mathpar}

  \fbox{$\jh\mr\Gamma H A L$}
  \qquad
  {Head}

  \begin{mathpar}
    % constant
    \infer{
      \Sigma(\cst c) = A; L
    }{
      \jh\mr\Gamma {\cst c} A L
    }

    % variable
    \infer{
      \Gamma(\var x) = A
    }{
      \jh\mr\Gamma {\var x} A \annnonsliceable
    }
  \end{mathpar}

  \fbox{$\jl\mr\Gamma A S \mmr{S'}{A'}$}
  \qquad
  {Spine}

  \begin{mathpar}
    \infer{
      \jm\mr\Gamma M A \mmr {M'}
      \and
      \jl\mmr\Gamma {\gsubst{A'}{M'}} S \mmmr {S'} {A''}
    }{
      \jl\mr\Gamma {\prd x {A} {A'}} {\spinecons M S} \mmmr {\spinecons
      {M'}{S'}} {A''}
    }

    \infer{ }{
      \jl\mr\Gamma {\app{\cst a} S} \spinenil \mr \spinenil {{\app{\cst a} S}}
    }
  \end{mathpar}

  \fbox{$\js\mr\Gamma\sigma{\Gamma'}\mmr{\sigma'}$}
  \qquad
  {Substitution}

  \begin{mathpar}

    % Cons
    \infer{
      \js\mr\Gamma\sigma{\Gamma'}\mmr{\sigma'}
      \and
      \jm\mmr\Gamma M {\gsubst A {\sigma'}} \mmmr {M'}
    }{
      \js\mr\Gamma{(\msubstcons \sigma x M)} {(\envcons {\Gamma'} x {A'})}
      \mmmr {(\msubstcons {\sigma'} x {M'})}
    }

    % Nil
    \infer{ }{
      \js\mr\Gamma\msubstnil\envnil\mr\envnil
    }

  \end{mathpar}

  \caption{Typing algorithm for objects}
  \label{fig:obj-typing}
\end{figure*}

The main judgement is atomic object typing $\jf\mr\Gamma F \mmr {F'}
A$, which means that in repository $\mr$ and environment $\Gamma$, the
applicative term $F$ is well-typed of type $A$, and the slicing of $F$
produces a new repository $\mmr$. The object $F'$ is the
\emph{residual} of this operation: only certain kind of applications
are sliced, and it is possible that the ``tip'' of application $F$ is
not sliced (because it is declared as non-sliceable). In that case,
$M$ is this tip; if $F$ is directly sliceable, then $M$ is the
metavariable referring to it.

These typing rules use definitional equality judgement between two
families $\jea\mr\Gamma A {A'}$. This equivalence is usually defined
to be just $\alpha$-equivalence since objects are in
$\beta\eta$-canonical form. Here, objects can contain metavariables
defined in $\mr$, so we shall thread it. Moreover, we will see in the
next section how defined constants compute and thus influence
definitional equality. Definitional equality is defined later in Fig.
\ref{fig:def-eq}.

Signatures themselves need to be checked. We present the checking
algorithm in Fig. \ref{fig:sign-typing}.

\begin{figure*}
  \fbox{$\jsig\mr\Sigma\mmr{\Sigma'}$}
  \qquad
  {Signature}

  \begin{mathpar}

    \infer{
      \jsig\mr\Sigma\mmr{\Sigma'}
      \and
      \jfam\mmr\gnil {\!_{\Sigma'} A}\mmmr {A'}
    }{
      \jsig\mr{\gcons\Sigma{{\cst c :^L A}}}\mmmr{\gcons{\Sigma'}{{\cst c :^L A'}}}
    }

    \infer{
      \jsig\mmr\Sigma\mmmr{\Sigma'}
      \and
      \jkind\mr\gnil {\!_{\Sigma'} K}\mmr {K'}
    }{
      \jsig\mr{\gcons\Sigma{{\cst a : K}}}\mmmr{\gcons{\Sigma'}{{\cst a : K'}}}
    }

    \infer{ }{
      \jsig\mr\gnil\mr\gnil
    }
  \end{mathpar}

  \fbox{$\jfam\mr\Gamma A \mmr {A'}$}
  \qquad
  {Family}

  \begin{mathpar}

    \infer{
      \jfam\mr\Gamma {A_1} \mmr {A_1'}
      \and
      \jfam\mmr{\gcons\Gamma{\tp {\var x} {A_1'}}} {A_2} \mmmr {A_2'}
    }{
      \jfam\mr\Gamma {\prd x {A_1} {A_2}} \mmmr {\prd x {A_1'} {A_2'}}
    }

    \infer{
      \Sigma(a) = K
      \and
      \jfl\mr\Gamma K S \mmr {S'}
    }{
      \jfam\mr\Gamma {\app{\cst a} S} \mmr {\app{\cst a} {S'}}
    }
  \end{mathpar}


  \fbox{$\jfl\mr\Gamma K S \mmr {S'}$}
  \qquad
  {Family spine}

  \begin{mathpar}
    \infer{
      \jm\mr\Gamma M A \mmr {M'}
      \and
      \jfl\mmr\Gamma {\gsubst{K}{M'}} S \mmmr {S'}
    }{
      \jfl\mr\Gamma {\prd x {A} {K}} {\spinecons M S} \mmmr {\spinecons
      {M'}{S'}}
    }

    \infer{ }{
      \jfl\mr\Gamma\type\spinenil\mr\spinenil
    }
  \end{mathpar}

  \fbox{$\jkind\mr\Gamma K \mmr {K'}$}
  \qquad
  {Kind}

  \begin{mathpar}
    \infer{
      \jfam\mr\Gamma A\mmr{A'}
      \and
      \jkind\mmr{\envcons\Gamma x A} K \mmmr {K'}
    }{
      \jkind\mr\Gamma{\prd x A K}\mmmr{\prd x {A'}{K'}}
    }

    \infer{ }{\jkind\mr\Gamma\type\mr\type}
  \end{mathpar}

  \caption{Typing algorithm for signatures}
  \label{fig:sign-typing}
\end{figure*}

\paragraph{Commit}
% TODO commit

\paragraph{Example}

Let us work out an example.
% TODO example

\paragraph{Checkout}

We define the partial \emph{checkout} operation $\checkout{\mr}$
(resp. $\checkoutr{M}$, $\checkoutr{S}$) from a repository
(resp. object, spine) to a metaclosed object as unfolding the
definitions of all metavariables recursively:
\begin{align*}
  \checkout{\mr} &= \checkoutr{\hat{\mr}} \\
  \checkoutr{\smeta X \sigma} &= \checkoutr{\mr(\smeta X\sigma)}\\
  \checkoutr{\lam x M} &= \lam x \checkoutr{M} \\
  \checkoutr{\app H S} &= \app H {(\checkoutr{S})} \\
  \checkoutr{\spinecons M S} &= \spinecons {\checkoutr M} {\checkoutr S} \\
  \checkoutr{\spinenil} &= \spinenil
\end{align*}

A repository is said to be \emph{well-formed} if $\checkout {\mr}$
is defined.

\begin{theorem}
  If \/ $\jm\mr\Gamma M A \mmr {M'}$ then \/ $\jem\mmr\Gamma M {M'} A$
\end{theorem}

\begin{theorem}
  If \/ $\jm\mr\Gamma M A \mmr {M'}$ then \/ $\jlfm\Gamma {(\checkoutrr
    M)} (\checkoutrr A)$
\end{theorem}

\subsection{The computation calculus}
\label{sec:computational-calculus}

We now turn to the definition of our computation calculus. As we saw,
code is attached to special, \emph{defined} constants in the signature
that manipulates \LF\ objects. We have the ability to construct
objects out of other, and deconstruct them by case analysis:
\begin{align*}
  T &\gequal
  \lam x T \gor
  U \\
  U &\gequal
  F \gor
  \matchin U \Gamma C \\
  C &\gequal \casenobar P U \gor
  C\ \case P U \\
  P &\gequal
  \app H {\var x} \ldots\ {\var x}
\end{align*}

This language is un(i)typed, in that all \LF\ objects are viewed as
having the same implicit type $\cst{obj}$: there is no static
guarantee that terms produced by a constant $\cst c : A_1\to A_2$ will
have type $A_2$. It is also non-terminating: we could declare a
recursive constant $\cst c : \cst a\to \cst a = \lam x \app {\cst c}
{\var x}$. We expose here only the minimal building blocks of this
language, but really any language with algebraic datatypes would do
since we will be manipulating directly the first-order representation
of \LF\ objects. In fact, whatever the language is, you can think of
defined constants as mere black boxes taking and producing terms, and
being able to call other black boxes. We detail for example in section
\ref{sec:implem} how to embed \sysname{OCaml} as computation calculus.

Nevertheless, in order to maintain the well-typing of the generated
term, we want to maintain the invariant that each time a piece of term
goes in or out of a black box, it is checked against its specification
(\ie\ its type). We thus maintain \emph{dynamic typing} of
computations: arguments and results are type-checked at run-time.

\paragraph{Substitution}

\begin{align*}
  \gsubst {(\lam x T)} \sigma &= \lam x {\gsubst T\sigma} \qquad
  \text{if $\var x\notin\dom\sigma$}\\
  \gsubst {(\matchin U \Gamma C)} \sigma &=
  \matchin {\gsubst
    U\sigma} {\gsubst\Gamma\sigma} {\gsubst C\sigma} \\
  \gsubst {(\casenobar{\app H {\spinecons {\var x_1} {\spinecons \ldots {\var
          x_n}}}} U)} \sigma &= {\casenobar{\app H {\spinecons {\var x_1} {\spinecons \ldots {\var
          x_n}}}} \gsubst U \sigma} \quad \text{if $\var x_i\notin\dom\sigma$} \\
\end{align*}

\begin{figure*}
  \fbox{$\jcc\mr\Gamma T S A \mmr F$}
  \qquad
  {Computational cut}

  \begin{mathpar}
    \infer{
      \jm\mr\Gamma M {A_1} \mmr {M'}
      \and
      \jcc\mmr\Gamma{\gsubst {A_2} {\msubst x {M'}}}{\gsubst T {\msubst x {M'}}} S \mmmr F
    }{
      \jcc\mr\Gamma{\prd x {A_1} {A_2}}{\lam x T}{\spinecons M S}
      \mmmr F
    }

    \infer{
      \jcr\mr\Gamma U \mmr F
      \and
      \jm\mmr\Gamma F {\app {\cst a} S} \mmmr {F'}
    }{
      \jcc\mr\Gamma {\app {\cst a} S} U \spinenil \mmmr {F'}
    }
  \end{mathpar}

  \fbox{$\jcr\mr\Gamma U \mmr F$}
  \qquad
  {Computational reduction}

  \begin{mathpar}
    \infer{ }{
      \jcr\mr\Gamma{\app {\var x} S} \mr {\app {\var x} S}
    }

    \infer{
      \Sigma(c) = A; (\annsliceable\text{ or }\annnonsliceable)
    }{
      \jcr\mr\Gamma{\app {\cst c} S} \mr {\app {\cst c} S}
    }

    \infer{
      \Sigma(c) = A; \anndefined T
      \and
      \jcc\mr\Gamma T S A \mmr F
    }{
      \jcr\mr\Gamma{\app {\cst c} S} \mr {\app {\cst c} S}
    }

    \infer{
      \jcr\mr{\gconcat\Gamma{\Gamma'}} U \mmr {\app {H_i} {\spinecons
          {M_1}{\spinecons\ldots{M_{m_i}}}}}
      \and
      \jcr\mmr\Gamma {\gsubst{U_i}{\msubst {x$_1$} {M_1} \ldots\
          \msubst {x$_{m_i}$} {M_{m_i}}}} \mmmr F a
    }{
      \jcr\mr\Gamma{\matchin{U}{\Gamma'}{
          \casenobar{\app {H_1}{\spinecons {\var x_{11}} {\spinecons
                \ldots\ {\var x_{1m_1}}}}}{U_1}
          |\ \ldots\
          \case{\app {H_n}{\spinecons {\var x_{n1}} {\spinecons
              \ldots\ {\var x_{nm_n}}}}}{U_n}
        }}\mmr F
    }

  \end{mathpar}

  \caption{Weak head-reduction}
  \label{fig:whr}
\end{figure*}

\begin{figure*}
  \centering
  TODO
  \caption{Definitional equality}
  \label{fig:def-eq}
\end{figure*}

\section{Implementation}
\label{sec:implem}
\section{Related work}
% 1-2 pages

\section*{Conclusion and future work}
% 0.5 pages

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{abbrvnat}
\bibliography{../../english.bib}


% \begin{thebibliography}{}
% \softraggedright
% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
% P. Q. Smith, and X. Y. Jones. ...reference text...
% \end{thebibliography}

\end{document}

%  LocalWords:  subtype
