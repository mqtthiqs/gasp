Content-Type: text/enriched
Text-Width: 70

Thu Jul  7, 2011  3:53 PM


Je poursuis mon exploration des caractérisations possibles des
fonctions injectives. Une vue abstraite du problème s'énonce ainsi :


   M N₁ ≡ M N₂ ⇒ N₁ ≡ N₂


signifie que M est _inversible à gauche_ i.e.


   ∃ P. (P o M) = Id


C'est Roberto Amadio qui m'a suggéré une telle formulation.


Roberto Di Cosmo a écrit le papier suivant sur les termes inversibles

du lambda-calcul:


Second Order Isomorphic Types: A Proof Theoretic Study on Second Order

lambda-calculus with Surjective Pairing and Terminal Object.


Notre cadre est légèrement différent car on peut supposer que M est une

forme normale avec partage (i.e. une forme normale avec des définitions

non expansées représentées par des applications de fonctions elles-mêmes

injectives). Cela peut tout de même donner des idées...

Je le lis maintenant.


Tue Jul  5, 2011  9:42 AM


A lire :


"Efficient Representation and Validation of Logical Proofs"
George C. Necula
Peter Lee
October 1997


Dans ce papier, Necula se pose des problèmes d'efficacité de
représentation des termes LF. Sa réponse, si j'ai bien compris,
consiste à rendre implicite certaines parties des termes. Ils
optimisent ainsi la taille des preuves. La reconstruction de la partie
implicite semble être faite pendant la vérification. 


Notre réponse à l'efficacité de la représentation consiste à rendre
possible du partage en termes déjà vérifiés et termes à
vérifier. Notre solution est bien différente.


Fri Jul  1, 2011  8:47 AM


Quelques éléments pour expliciter les différents problèmes et

leurs motivations respectives.


Tout d'abord, sur le design de la syntaxe. La syntaxe doit être

suffisamment expressive pour représenter un ensemble arbitraire de
termes canoniques de SLF (Spine Form LF) partageant des fragments

de sous-termes. Elle doit aussi être compacte.


Ce dernier point justifie la non-utilisation des formes η-expansées.


Pour l'expressivité, je vois donc deux résultats à prouver :


a. Complétude vis-à-vis des formes canoniques de SLF.

Toute forme canonique de SLF (bien typée) est représentable en un
objet bien typé dans notre langage. Cela devrait être évident.


b. Complétude vis-à-vis des formes canoniques avec partage.


C'est l'aspect intéressant. Il faut d'une part caractériser simplement
le partage que nous souhaitons être capable d'exprimer (on doit
pouvoir fournir aux utilisateurs de S² une spécification simple des
deltas que l'on peut écrire et ceux que l'on ne peut pas). Il faut d'autre

part montrer que l'ensemble des objets bien typés de notre langage capture

effectivement bien ces termes avec partages.


Pour le moment, j'essaie de converger vers quelque chose comme ça
pour la spécification du partage que nous souhaitons capturer

(c'est encore à raffiner):


Si T est une forme canonique de SLF tel que

   T = C (F t₁ ... t_n)

avec:

   C, un contexte de forme canonique

   F, une fonction pris dans une classe de fonctions injectives (FI). 

   t_i, des formes canoniques


alors il existe un terme U de S² bien typé dont l'expansion est T.

   

Pourquoi choisir des fonctions F injectives? Parce que l'on souhaite

avoir un algorithme de vérification des types, et en particulier un

test de conversion, capable de prendre en compte le partage et de

n'expanser les définitions que lorsque que c'est nécessaire. En
particulier, on veut réduire 


     F (l) ≡ F (l') ⇒ l ≡ l'


Si un vérificateur incrémental de type a utilisé du partage
"pertinent", la vérification du bon typage du delta qu'il produit
devrait alors être "optimal". (Tout ceci est à formaliser dans

la définition du typage incrémental.)

   

Comment choisir la classe de fonctions injectives FI? On veut

certainement un critère simple, syntaxique, inspiré de ce que Pfenning

a proposé dans son papier sur les définitions. Cependant, nous sommes
dans un cadre légèrement différent. J'essaie donc actuellement de
converger vers un système de type capturant ces fonctions injectives
intéressantes et facilement spécifiables/explicables à nos
utilisateurs. Pour être plus précis, je suis actuellement en train de
creuser la piste des patterns à la Miller.



Fri Jun 24, 2011  4:00 PM


La syntaxe que je propose permet de représenter des termes plats du
spine calcul avec des définitions dans le même esprit que ce que fait
Pfenning dans le papier cité dans une note précédente. Les différences
principales sont :


- Dans Pfenning, les définitions sont fixées une fois pour toute 
  dans la signature. Dans notre cas, les définitions font parties
  des termes donc évoluent au fur et à mesure des commits.
  (Ceci n'est certainement qu'une différence superficielle.)

  
- Pfenning cherche à caractériser certaines définitions à l'aide d'un
  critère syntaxique pour s'assurer qu'elles sont injectives.  Dans
  notre cas, nous voulons nous assurer que *toutes* les définitions
  sont injectives de façon à garantir que le test de conversion
  mettant en jeu une partie partagée des deux côtés sera fait sans
  dérouler de définitions, i.e. :

  
  h (l) ≡ h (l') ⇒ l ≡ l'

  
  quelque soit "h".

  
  Nous pouvons trouver un tel critère (via du typage) car nous ne
  souhaitons représenter uniquement les formes canoniques du spine
  calcul, en introduisant du partage.

  
  On peut envoyer toute forme canonique du spine calcul vers un terme
  bien typé de notre langage (avec plus ou moins de
  partage). Réciproquement, un procédure d'expansion descendante des
  définitions produit une forme beta-normal du spine calcul (mais pas
  eta-long).



Thu Jun 23, 2011  2:18 PM


Un peu de remise en contexte. 


Les motivations pour faire le typage incrémentalement sont claires. Je
ne reviens pas dessus. J'essaie de donner une description rigoureuse
du cheminement qui mène à notre approche. On se place dans le lambda
calcul simplement typé à la Church.


Dans le STLC, supposons que l'on ait un ensemble de dérivations D_i :


         …
   ————————————–
    ∅ ⊢ t_i : τ_i 


et que l'on se donne un nouveau terme t.

    
Un première formulation informelle du problème du typage incrémental
est :


"Construire une dérivation de typage pour `∅ ⊢ t : τ' 
en réutilisant les dérivations D_i /le plus possible/."


J'essaie maintenant de préciser cette formulation. Je suppose
que je n'ai qu'une seule dérivation D parmi les D_i pour
simplifier la présentation.


** Que signifie l'expression "réutiliser une dérivation le plus possible"? **


a. Les différentes formes de réutilisation/partage.


Supposons un typeur "standard" pour le STLC. C'est une 
fonction récursive qui descend dans le terme "t" en maintenant
un environnement courant. En fonction de la forme de "t", 
la fonction choisit une règle du système de type pour réduire
la construction de la dérivation de "t" à la construction
des dérivations pour les hypothèses de cette règle correctement
instanciée. C'est en ce point qu'un programmeur fonctionnel 
pourrait appliquer la méthode de mémoisation pour chercher 
dans son ensemble de dérivations déjà connues celles 
correspondantes aux nouveaux problèmes produits. Avec 
cette stratégie, "réutiliser une dérivation" signifie
"réutiliser une sous-dérivation entière".


Un exemple qui marche :
——————————————————————–
t1 = λ(x : int). x + x
t2 = λ(x : int). (x + x) + x


Un exemple qui pourrait marcher :
——————————————————————–—————————–
t1 = λ(x : int). x + x
t2 = λ(y : int). y + y


(En canonisant la représentation des termes avec lieurs.)


Un exemple qui pourrait marcher avec beaucoup de travail :
————————————————————————————————————————————————————————––
t1 = λ(x : int). x + x
t2 = λ(x : int). λ (y : int). x + x


Ce deuxième exemple échoue car le sous-problème produit est :


   (x : int); (y : int) ⊢ (x + x) : int

qui n'est pas exactement celui connu. C'est rageant car la règle
de weakening est admissible. Une solution adhoc? 


(On suppose ici que l'environnement est explicité et que l'on ne travaille
 pas dans HOAS. En effet, pour le STLC en HOAS, il n'y a pas d'environnement
 de typage et donc les problèmes de weakening sont pris automatiquement.
 compte. On pourrait trouver un autre exemple pour lequel HOAS est aussi
 en défaut, mais il faut mieux commencer par quelque chose de simple.)

 
Un autre exemple qui fonctionnerait difficilement
——————————————————————————————————————–——————————
t1 = λ(x : int). x + x
t2 = λ(x : int). 0 + x


Cet exemple échoue d'une façon encore plus claire. Cependant, on
pourrait utiliser deux règles admissibles du STLC, la substitutivité
et la généralisation, pour argumenter la validation de la
réutilisation de la sous-dérivation de D :


	 (x : int) ⊢ x + x : int <<——— vient de D
         ————————————————————————————————————–——– [ Généralisation ]
            (x : int); (y : int) ⊢ y + x : int
            —————————————————————————————————– [ Substitutivité ]
  	    (x : int) ⊢ 0 + x : int

	    
Un exemple qui fonctionne mais n'est pas efficace :
——————————————————————————————————————————————————–
t1 = (... ((u1 u2) u3) ... uN )
t2 = (... ((v  u2) u3) ... uN )


Le typeur (mémoisé) va descendre N fois à gauche le long des
applications et reconstruire une dérivation filaire de taille N. Si N
est grand - et on le suppose potentiellement très grand - on a
perdu. On aimerait réutiliser la dérivation de t1 dans son ensemble
sauf là où on trouve la sous-dérivation de "u1", sans avoir
reconstruire ce qui est réutilisé, ou même seulement un objet
proportionnel à la profondeur de ce qui est réutilisé. En d'autres
termes, on veut pouvoir se donner une représentation en espace
constant, un nom, pour n'importe quel contexte de dérivation.


Un exemple de réutilisation en toute généralité :
————————————————————————————————————————————————–
t1 = (+ (+ a (+ b c)) d)
t2 = (+ (+ (+ a b) c) d)


Ici, on aimerait réutiliser à la fois la dérivation pour le contexte
(+ ... d) et les sous-dérivations de a, b, et c. La seule chose
qui change ici, c'est l'arrangement de ces trois dérivations. Pour être
plus précis, la dérivation de t1 est de la forme :

            b     c
            ———————
    a       (+ b c)
    ———————————————
     (+ a (+ b c))              d
     —————————————————————————————
         (+ (+ a (+ b c)) d)

tandis que celle de t2 est : 

     a    b
     ——————*
     (+ a b)    c
     ————————————–*
     (+ (+ a b) c)              d
     —————————————————————————————
           (+ (+ (+ a b) c) d


Le partage que l'on a envie d'avoir ici est indiqué avec des *.

	 
(Cela répond à la question de Matthias sur l'intérêt des abbréviations
 paramétrées par des fonctions : ici, on veut abstraire par rapport à 
 la partie étoilée. Cela signifie, en gros, que le partage idéal, en
 espace et en temps, est de la forme :

      D = λG. app (G a b c) d
      Delta_t1 = λ x y z. app (app x y) z
      Delta_t2 = λ x y z. app x (app y z)
      Derivation_t1 = D Delta_t1
      Derivation_t2 = D Delta_t2
)

      
Dans notre cas, nous disons donc :


- que "réutiliser une dérivation" signifie "extraire n'importe quel
fragment de cette dérivation et s'en servir pour construire une
nouvelle dérivation" ;


- qu'un fragment d'une dérivation D est une sous-dérivation de D dont
certains sous-arbres ont été éventuellement abstraits ;


- que l'on peut instancier un fragment de dérivation et le "brancher"
  comme hypothèse d'une règle ou encore l'utiliser comme argument d'un
  autre fragment de dérivation, instancier ou non.

  
b. Réutiliser "efficacement" une dérivation

  
Un langage de "définitions d'ordre supérieur" semble approprié pour
exprimer la réutilisation via le partage. On a ainsi une
représentation *efficace en espace*. Est-ce une réprésentation 
efficace en temps?


En d'autres termes, est-ce que la vérification d'un terme représenté à
l'aide de partages tire elle-même partie de ce partage pour ne
vérifier que la partie "nouvelle", et ce d'une façon efficace?


Cette propriété est difficile à formaliser. Je propose la définition
(expérimentale) suivante:


Soit "d" un dépôt et "u" un delta. 
On définit le "commit" comme suit : commit d u ≡ (old = d in [ u ]_old)


Soient T1, le terme LF issu de l'expansion d'un terme d1 et D1 la
dérivation de typage de T1 dans LF.  Soient T2 un terme de LF, D2 sa
dérivation de typage dans LF.


Supposons un oracle capable de trouver le meilleur partage entre D1 et
D2, alors il existe un "u" tel que:


(i)  l'expansion de "commit d1 u" est T2 ;


(ii) et la vérification de "commit d1 u" correspond à la construction
de la partie de D2 qui n'est pas partagée.



Thu Jun 16, 2011  5:17 PM


Notons que l'application binaire n'est pas réaliste en pratique pour
implémenter une application n-aire. Un noeud n-aire se réprésente
beaucoup efficacement par un vecteur et l'unification d'un noeud
réprésentant une application n-aire se faire en temps constant alors
que, avec une application unaire, l'unification est proportionnelle
à l'arité.


Habituellement, ce genre d'argument ne tient pas car on peut supposer
que l'arité des constructeurs est bornée. Cependant, dans notre cas,
ce n'est pas vrai : l'ensemble des abbréviations est voué à grossir
et donc leur arité n'est pas bornée.




Thu Jun 16, 2011  4:49 PM


Je viens de remarquer que, dans notre cadre, il n'est pas vraiment
malin, du point de vue du partage du moins, d'imposer des applications
maximales des constructeurs de données. Voici un exemple de forme
beta-normale de LF :


k (f a b) (f a c)


où k et f sont des constructeurs d'arité 2. 


Si on a le droit de "couper" les applications n-aires, un bon partage,
économe en espace, est :


X = f a
Y = X b
Z = X c
V = k Y Z


Ce choix me semble meilleur que cela imposé par une application n-aire
totale : 


F X = f a X
Y = F b
Z = F c
V = k Y Z


parce que si "f" était d'arité N alors F contiendrait une application de 
taille N alors que l'information essentielle de F est la partie "f a".


On pourrait argumenter pour un retour à une application unaire, mais dans
ce cas là, on nomme vraiment tout et on est alors trop gourmand en espace :


X = f a
Y = X b
Z = X c
V = k Y
W = V Z


Comment maintenant réinjecter les formes eta-long dans ce contexte? Si on 
reprend notre potentiel choix de répresentation (applications n-aires mais 
on autorise les applications partielles) alors :


X = f a
Y = X b
Z = X c
V = k Y Z


On voit qu'on peut être assez naïf lorsque l'on est face à des
problèmes de conversion. En effet, si je suis par exemple face au
problème suivant :


X b = f a b

La règle actuelle interroge l'environnement pour connaître la forme
eta-long de tête de X. On obtient : 

Γ (X) = λ x. f a x

Comme on obtient une lambda-abstraction, le système va se mettre en
mode "réduction" et rajouter un binding [x -> b] dans l'environnement
puis poursuivre avec le problème "f a x = f a b", ce qui réussira. 

Dans ce cas précis, le système de type des abbréviations sait que "X" est
une donnée : la mise en forme eta-long et la substitution semble un peu
overkill. On pourrait ici se contenter de consommer les arguments
les plus à droites (les deux "b"s ici) et de partir en récursion à gauche
avec le problème "X = f a". 


Dans tous les cas, remarquons que la mise en forme eta-long de tête des

abbréviations préserve le bon typage des abbréviations.


Ces dernières remarques sont, il me semble, de l'ordre de l'optimisation
du test de conversion. On va les ignorer pour le moment mais je garde 
ces notes pour mémoire.



Thu Jun 16, 2011  4:20 PM



Sans surprise, je réponds à la négative à la question précédente. Il existe
deux abbréviations bien typées (sans rédexs dangereux, injectives et 
canonisées au sens de l'entrée précédente) qu'on ne peut pas traiter 
comme des constantes : 



F = \(x, y). k x x y
G = \(x, y). k x y y



On a : 



   F 0 0 = G 0 0 


   
mais 



   F <<> G 


   
(et rien ne justifierait un dépôt égalisant ces deux abbréviations). 



Le caractère intentionnelle de la béta-réduction se retrouvent donc même
dans un cadre aussi simple que les abbréviations. Dit autrement : il y a 
toujours plusieurs façons de partager ou d'abstraire certaines parties
d'un terme.



Thu Jun 16, 2011  3:29 PM


Quelles sont les applications rencontrées lors du test de conversion? 


       F X =?= T


Hypothèse : 
F et X sont des noms pour des abbréviations bien typées.


Deux cas principaux :

- "F X" est bien typée. 
  Cela signifie qu'on est dans un des trois sous-cas suivants :
  1. X est une donnée, F est une abbréviation avec un trou pour y 
     brancher une donnée. On est vraiment dans le cas premier ordre.

  2. X est une constante fonctionnelle (un constructeur), l'argument
     de "F" est potentiellement utilisée en position fonctionnelle,
     mais ce n'est pas grave. 
     On peut raisonner comme au premier ordre, ce n'est pas grave.

  3. X est une lambda-abstraction mais F promet de ne l'utiliser 
     qu'en position d'argument.
     On peut encore raisonner comme au premier ordre. Il n'y a 
     toujours pas de calcul ici. C'est une donnée avec des lieurs.


- "F X" est mal typée.
  Cela signifie que X est une lambda-abstraction et que F peut l'utiliser
  en position fonctionnelle. Il faut faire la réduction. 
  On va réduire tant que ce terme est mal typé (par appel récursif au 
  test de conversion). 



Thu Jun 16, 2011  2:45 PM



Une autre remarque, cette fois-ci pour tirer partie au maximum de
la notion d'abbréviation dans la conversion (et dans une unification
potentielle), on pourrait enrichir le typage des abbréviations par
une notion d'*injectivité*. Cette idée est mentionnée dans le papier
de Pfenning référencé plus bas. J'en avais eu l'idée il y a quelques
temps dans mes premières formulations du système de types pour 
les abbréviations.



Une abbréviation F est injective ssi F X = F Y <<=> X = Y. Dans 
un langage où F peut calculer, il faut faire une analyse de 
strictness pour s'assurer que F ne jette pas son argument. 



L'intérêt d'une telle notion est d'améliorer drastiquement le test de
conversion en ne se sentant pas obligé de dérouler les définitions
tout en restant complet. Dans notre cas, cette propriété est garantie
car les corps des lambda-abstractions ne représentent qu'une forme
normale (avec partage, certes, mais dans laquelle plus aucun calcul en
suspens ne peut transformer la forme du résultat en fonction de la
valeur des arguments). On peut donc éviter de dérouler les
abbréviations pendant le test de conversion, chouette. Mieux, on
pourrait marquer les arguments par "utilisé" ou "non utilisé" alors on
serait complets en ne testant que l'égalité des arguments utilisés.



Une telle annotation des identificateurs introduits pour les
définitions serait utile en Coq. J'en ai parlé à Hugo qui s'est montré
très intéressé car actuellement le traitement de la delta-expansion
dans Coq est très naïve. (En gros, on expanse beaucoup trop souvent
les définitions pendant l'unification ou la conversion.) J'en ai aussi
profité pour parler à Hugo de l'utilisation de l'anti-unification pour
résoudre les problèmes d'inférence de la clause de retour de certains
pattern-matching dépendants. On en reparlera lors d'un GT sur l'unification
dans Coq.



Pour terminer sur cette remarque, notons évidemment que l'on doit toujours
dérouler les définitions quand on est dans le cas F X = G Y. Cependant,
si on maintient un partage "maximal" dans le dépôt pour s'assurer que 



      F x = t /\ G y = t' /\ t =_\alpha t' => F = G


      
alors on devrait déjà se limiter à des expansions d'abbréviations "utiles". 



On peut aller encore un peu plus loin en théorie. Si on a dans le dépôt : 

      F x y = k x y 
      G x y = k y x


      
alors on peut remplacer toutes les occurrences de 'G x y' par 'F y x'
et oublier la définition de G. La fonction de canonisation ici revient
à réordonner les variables dans l'ordre dans lequel elles apparaissent
dans le corps de l'abbréviation (pour un certain parcours de
l'AST). Après application de cette fonction de normalisation, la règle
précédente s'applique car les abbréviations sont alors
alpha-equivalentes. On peut même exiger, sans perte de généralité, que
les abbréviations définies par l'utilisateur soient normalisées (on peut
fournir une fonction de normalisation dans l'API à l'aide smart-constructeurs
qui laissent des annotations sur les identificateurs des abbréviations pour
savoir dans quel ordre les applications doivent être formées de façon
interne, sans que le programmeur ne s'en rende compte).



Encore un autre cas, plus tordu : 

     F x = (y = x, z = k x x |- y)
     G x = (z = k x x |- z)


     
sont clairement équivalents ...



Existe-t-il une définition de partage maximal (sans utiliser de
généralisation) justifiant le traitement des abbréviations comme
des constantes de constructeurs? (i.e. justifiant la seconde partie
de la règle 'F x =?= G y -> ⊥' si F <<> G). 




Thu Jun 16, 2011  1:32 PM



Quelques notes supplémentaires sur la notion d'abbréviations que
j'essaie d'introduire et formaliser pour réaliser le partage. 



D'abord, je dois préciser ce que j'entends par "une abbréviation 
bien typée ne doit pas introduire de nouveaux rédexes". Évidemment,
en toute rigueur, c'est une assertion inexacte :

  def f x = x
  def g x = (f x, f x)
  g 0


  
n'est pas rejetée par mon système de type alors que concrètement, si
on déplie la dernière application, on obtient : 



  def f x = x
  def g x = (f x, f x)
  (f 0, f 0)


  
donc, en toute rigueur, il y a un redex de plus.



Ce que j'entends par "non introduction de nouveaux redexes", c'est
qu'il n'y a pas de nouvelles lambda-abstractions de créées par les
expansions des définitions. Dans le cadre général, i.e. dans une
chaîne de réductions standard du lambda-calcul simplement typé, il y a
un nombre de nouvelles lambda-abstractions qui doit être
proportionnel à la longueur de la chaîne.



En particulier, cette propriété de bon typage des abbréviations me
permet de calculer *statiquement* le nombre d'expansion de définitions
(i.e. de beta-reduction) à effectuer pour déplier toutes les
définitions et obtenir la forme beta-normale de LF correspondante.



(Remarque: Cette quantité est d'ailleurs probablement une bonne mesure
du partage réalisée par cette représentation particulière de la forme
normale. Cette mesure pourrait être utilisée pour aider à quantifier
l'incrémentalité du vérificateur de types.)



Si j'autorise des utilisations d'abbréviations potentiellement
dangereuse comme dans l'exemple suivant :



   def mul   = λn.λf.λx. f (n f x)
   def exp   = λm.λn. n (mult m) 1
   def three = λf.λx. f (f (f x))
   exp three three


   
alors, sauf erreur de ma part, on ne peut plus calculer
automatiquement et statiquement le nombre d'étapes pour obtenir la
forme beta-normal du programme en LF.



Cette discussion est importante car nous sommes en train de concevoir
une structure de donnée. Si on veut argumenter sur l'efficacité des
opérations (d'accès) sur cette structure, alors il faut donner une
quantification précise du coût de ces opérations. 



Wed Jun 15, 2011  1:21 PM


J'étais étonné qu'il n'existe pas de LF avec définitions. Après une
discussion avec Jeff, le doute est levé : 


Algorithms for Equality and Unification in the Presence of Notational
Definitions -- Pfenning and Schurmann


Ils ont une notion de "strictness" pour forcer les arguments
fonctionnels des définitions à n'apparaître qu'en position d'argument
(qu'ils appellent rigides). Ma notion de typage semble contenir cette
notion (et être un tout petit peu plus générale). 


Ces explications viennent de Jeff. Je vais lire le papier pour
m'assurer que ce qu'il m'a raconté correspond à ce qui est écrit dans
ce papier.



Wed Jun 15, 2011 10:23 AM



J'essaie de préciser l'assertion "notre structure de donnée est
efficace en temps et en espace". 

Pour l'efficacité en espace, il faut argumenter autour de la notion
de partage. Si je prends un terme de canonical LF, alors il existe
toujours un terme plus petit dans notre système qui
le dénote modulo une expansion des lets. 



Pour l'efficacité en temps, on peut avancer deux points: les
opérations sur la structure de données sont efficaces et le commit
fait "au mieux". Pour le premier point, la syntaxe parle d'elle-même 
(choix de lambda-bar pour faire des applications par bloc,
des lets à plat (i.e. des records) pour avoir un accès aléatoire à
une abbréviation, le maintien du partage à travers
la réduction quand c'est possible, etc). 



Le second point est plus subtil. On avait l'idée approximative que la
complexité du commit, c'est-à-dire de la vérification du bon typage
d'un delta, devait être proportionnel à la taille de ce delta. Si on
l'observe précisément, cette idée est une cause perdue : les tests de
conversion peuvent demander des tests d'égalité proportionnels à la
taille du dépot. Est-ce à dire que l'incrémentalité est un illusion?
En fait, non : on reste proportionnel à la taille de la dérivation de
typage du delta, c'est-à-dire proportionnel à la taille de la
dérivation de typage du terme LF obtenu par expansion des
abbréviations moins la taille des dérivations partagées. Les tests de
conversion issus de la partie partagée sont bien partagés. Par contre,
les tests de conversion introduit par le delta ne le sont pas et
peuvent être assez gros, mais on ne peut rien y faire. 



Maintenant, supposons que nous laissions de véritables rédexes, et non
de simples utilisations d'abbréviations, dans le dépôt. Cela signifie
que des calculs qui n'ont rien à voir avec le "delta" sont laissés en
suspens dans le dépôt, peuvent être référencés à travers le partage,
et, surtout, dupliqués pendant les tests de conversion apparaissant
pendant la vérification du "delta". Le temps nécessaire à la réduction
de ces rédexs seraient alors proportionnels à la taille du dépôt et
aux nombres de rédex dupliqués (un nombre potentiellement exponentiel
si je ne dis pas de bêtise).


Tue Jun 14, 2011  5:52 PM


Une journée bonus avec Matthias. Je lui ai présenté le système actuel
avec une syntaxe un peu plus lâche et des choix de design un peu
différents. Il n'est pas convaincu qu'une généralisation de l'ancien
système (moralement, NLF avec des boîtes) ne ferait pas l'affaire. 
C'est une question à creuser. 


Une remarque importante, c'est qu'il existe peut-être deux formes
d'application : l'application symbolique, inerte, qui créé de la
donnée et l'application qui code une expansion de définition. Dans
la nouvelle syntaxe, les deux sont confondues, c'est peut-être 
dommage.


Une autre question à creuser (exercice!) : trouver un exemple
d'utilisation qui justifie la généralisation de la paramétrisation
des boîtes à l'ordre supérieur. 



Thu Jun  9, 2011  3:16 PM



Une critique éventuelle sur notre approche : si il s'agit d'avoir
une syntaxe pour implémenter du partage, le lambda calcul suffit.
En effet, un beta-redex implémente du partage. Effectivement, 
nous avons un langage de "let"s, et on pourrait se demander si 
les remplacer par des beta-redexes (non dangereux) ne seraient pas
équivalents tout en se passant d'introduire une nouvelle syntaxe.


L'argument ici est l'efficacité des accès : on veut mettre à 
plat donc une imbrication de beta-redexes ne nous sied guère.


Pourquoi alors ne pas travailler dans λbar? Parce que de toute façon,
dans la machine abstraite, on veut faire une différence entre 
les let et les vrais rédexes qui calculent pour optimiser le
test de conversion : on veut s'arrêter sur deux variables égales
syntaxiquement sans comparer leur définition. (Je ne suis pas trop
sûr pour ce dernier point.)


Thu Jun  9, 2011  1:42 PM


Je réfléchis à la règle de commit des boîtes. Voici la règle
actuellement écrite au tableau :


      (H0)  (Γ, v)|p = λ x : A. o
      (H1)  Γ, v ⊢ u ⇝ (σ' ⊢ v')
      (H2)  o, Γ σ' [ x = u : A ] ⊢ t : B ⇝ (σ'' ⊢ v'')
      —————————————————————————————————————————————————— [BOX]
         Γ, v ⊢ [ t ] u p : B ⇝ ([ x = u : A ] σ'' ⊢ v'')


	 (Attention aux hypothèses fraîcheurs : dom(σ) # dom(Γ))

	 

Cette règle est sympathique car elle règle le problème numéro (i) 
décrit plus tôt. Malheureusement, nous avons sciemment mis de 
côté une incohérence avec la syntaxe : u peut être une lambda
abstraction et donc, l'hypothèse H2 n'est pas dans la syntaxe.


Deux solutions : l'une est brutale mais j'ai peur qu'elle fasse
perdre de l'expressivité à XLF (cependant, j'aimerais être convaincu
du contraire) ; l'autre semble maintenir l'expressivité de XLF 
mais remets en cause les notes précédentes affirmant que nommer
les lambdas ne sert à rien.


□ Première solution : 
————————————————————–

On fixe "u ≡ h l" dans la règle BOX, on obtient
BOX1 :


       (H0)  (Γ, v)|p = λ x : A. o
       (H1)  Γ, v ⊢ u ⇝ (σ' ⊢ h l)
       (H2)  o, Γ σ' [ x = h l : A ] ⊢ t : B ⇝ (σ'' ⊢ v'')
        ——————————————————————————————————————————————————[BOX1]
        Γ, v ⊢ [ t ] u p : B ⇝ ([ x = h l : A ] σ'' ⊢ v'')

	

Tout va bien ici, syntaxiquement, on est dans la syntaxe. 

Cependant, est-ce que l'on capture toujours l'ensemble de toutes les
formes canoniques? Je pense que non. En effet, si je pars du dépot :


       (∅ ⊢ λ (f: (Π y : A. B)). (y = App (f, 0) ⊢ y))


et que je veux travailler sur l'objet suivant:

       

      App (λ x. x, 0)


alors j'aimerais écrire le terme XLF suivant pour représenter
l'intégration de cet objet au dépot :


      [ y ] (Lam (λ x. Var x)) •


mais je ne suis pas dans la syntaxe de XLF est j'ai l'impression qu'il
n'existe pas un terme équivalent dans cette syntaxe. Qu'en pense Matthias?

Une solution serait d'aller substituer directement dans la définition
de la boîte mais on perd alors vraiment et pour toujours le partage
des boîtes.


□ Deuxième solution : 
————————————————————–

On relâche la syntaxe générale pour autoriser n'importe quel "u" mais
on utilise un argument de typage pour s'assurer que l'on dénote
toujours uniquement des formes canoniques. On annote les variables
par des +/- et moins dans tous les bindings (sur les lambdas, 
dans les Γ, dans les σ...). Le '+' signifie que la variable ne
peut apparaître qu'en position applicative et le '-' signifie que 
la variable apparaît aussi en position fonctionnelle. (Evidemment,
les extrémistes de la syntaxe pourront choisir d'introduire un
autre espace de nom pour représenter les variables + et les 
variables -, ceci implique une duplication de nombreux constructeurs
de la syntaxe – les λ, π, constructeur de Γ ... — ce que l'on 
préférera éviter). 


Dans cadre, la règle BOX devient la règle BOX2 suivante :


      (H0)  (Γ, v)|p = λ x^δ : A. o
      (H1)  Γ, v ⊢ u ⇝ (σ' ⊢ v')
      (H2)  o, Γ σ' [ x = u : A^δ ] ⊢ t : B ⇝ (σ'' ⊢ v'')
      (H3)  Γ σ' ⊢ v' ⋄ δ 
      ——————————————————————————————————————————————————————— [BOX2]
         Γ, v ⊢ [ t ] u p : B ⇝ ([ x = u : A^δ ] σ'' ⊢ v'')


	 où "δ ::= + | -" et le jugement "Γ ⊢ v ⋄ δ" est défini par :

                   
      ——————————   
      Γ ⊢ v ⋄ +

      

      ——————————–
      Γ ⊢ h l ⋄ -

      

      Γ(x) = A^δ    δ << δ'
      ————————————————————
      Γ ⊢ x ⋄ δ'


La dernière règle sert à propager la prétention δ d'une certaine
variable liée plus haut à la variable qu'elle substitue
(formellement).


Il faut également mettre à jour les règles pour vérifier que ce que
prétend le δ d'un binding est bien respecté par le terme de XLF. J'ai
l'impression que ce n'est pas très dur.



Thu Jun  9, 2011 12:21 PM


D'une manière générale, cette question de nommer explicitement les
lambdas, apparaissant en position non fonctionnelle, est peut-être
une fausse question.


En fait, tous les lambdas sont déjà nommés dans NLF : ils sont nommés
par une position. En quoi un nom "plat" serait-il plus efficace? Si 
on néglige l'indirection induite par l'extraction d'un argument d'un
terme applicatif alors je ne vois plus d'intérêt à cette question.


Thu Jun  9, 2011 12:10 PM


Matthias m'a réexpliqué le rôle des têtes. Voici ce que j'ai compris.

Les têtes servent à implémenter le "checkout", c'est-à-dire l'opération
de réification d'un terme XLF extrait du dépot. Il est donc très important
de particulariser la tête pour retrouver la structure des lambda-termes. 


Jusque là, pas d'arguments en défaveur de ma modification.

Là où ça se complique, c'est que les σ ne lient actuellement que des 
termes applicatifs et non des lambdas. De cette facon, on a une garantie
syntaxique que les termes NLF sont en forme canonique. Si on veut étendre
la syntaxe des σ en maintenant cette contrainte, il faut introduire un
espace de nom différents pour lier les fonctions et les termes applicatifs. 


Ceci implique aussi éventuellement des modifications de la syntaxe des valeurs
(mais à y réfléchir un peu, pas forcément). 


Ce qui est sûr, c'est que ce que je proposais ne simplifie pas la syntaxe
des positions : on doit avoir deux constructeurs différents pour atteindre
une abstraction en fonction de l'endroit où elle se trouve : à toplevel ou 
comme argument d'un terme applicatif. 


En résumé donc, la suppression des têtes de la syntaxe serait superficielle
car on obtiendrait une syntaxe équivalente, mais moins pratique que celle
que nous avons maintenant (on rajouterait des indirections non nécessaires
en nommant les lambdas.).


Thu Jun  9, 2011  8:26 AM


Retour sur la discussion technique avec Matthias sur le système pour le typage
incrémental. Matthias est arrivé avec deux problèmes : 


     (i) Le système accumulait abusivement les définitions du contexte à l'intérieur 
     des sous-composantes. Cela semblait stupide car l'interprétation des sous-
     composantes du termes NLF se fait toujours sous les définitions de son
     contexte, pourquoi les répéter?


     (ii) L'instanciation des boîtes duplique le contenu des lambda-abstractions 
     qu'elles dénotent. C'est dommage : on pourrait imaginer de laisser les
     boîtes dans NLF pour éviter cette duplication-là.

     

Lors de notre discussion, nous avons, je pense, réglé le point (i) mais
pas le point (ii). 


La confusion qui régnait dans l'ancien système semblait venir d'une mauvaise
implémentation de l'importation du dépot R initial dans le nouveau dépot R'
produit par le commit. Matthias implémentait cela via une sorte de "fold" 
accumulant de façon systématique le contenu de R dans R'. Cela partait
de la constatation que les méta-variables utilisées dans le terme XLF doivent
être embarquées dans le nouveau dépot. D'ailleurs, n'ayant pas bien pris 
conscience de cela, ma première proposition de solution à ce problème a
été une sorte de "map" de "t" vers "R'" oubliant totalement le précédent
dépot "R". 


La solution au problème (i) a consisté en une prise de conscience que
la méthode "fold" n'était pas très économe car elle ferme de façon
top-down tous les sous-composantes à l'aide de la totalité des
définitions de leurs contextes! Une méthode économe consiste à ne
fermer les sous-composantes qu'en des points bien précis correspondant
à des ouvertures de scope instanciés, i.e. au niveau de la traduction
des boîtes de XLF. 


Il est alors apparu qu'il n'existait en vérité qu'une uniqu ensemble Γ
de déclarations/définitions pendant la traduction qui est peuplé des objets
importés de l'ancien répo et des instanciations faites par les boîtes. 
Ainsi, dès que l'on se déplace dans l'ancien dépot, on importe toute ce 
qu'il y a dans son scope et on ne garde que la tête qui va permettre des
navigations ultérieures dans un nouveau scope interne si la tête est un
lambda. 


Les jugements de l'ancien système étaient de la forme :


    R, Γ ⊢ t : A ⇝ u
  
Nous les avons changés en

    

     Γ, v ⊢ t : A ⇝ u


Plus j'y pense et plus je pense que la notion de tête n'est peut-être
pas très pertinente dans les termes de NLF et introduisent une
irrégularité pas vraiment justifié, il faut que j'en parle avec Matthias. 

Pourquoi ne pas passer de "t ::= σ ⊢ v" à "t ::= σ" où la valeur "v" a
été inclue dans "σ" à l'aide d'un nom frais? Éventuellement, on
rajouterait à ce nom une marque spéciale pour se souvenir que c'était
l'appel terminal du terme XLF de façon à pouvoir implémenter le
checkout. J'ai l'impression que l'on simplifierait alors le jugement
et la définition des positions "p".


Par ailleurs, je pense que le système serait plus élégant si on résolvait
le point (ii). 


Tue Jan 25 18:51:48 CET 2011


== Composition de langages


Aujourd'hui, la traduction de SLF vers NLF se fait par raffinements
successifs:

  SLF (LF non stratifié)

  LF (stratifié)

  XLF (applications n-aires)

  XLFa (annotation des spines, nommage des arguments)

  XLFe (forme η-longues)

  NLF (Π et λ n-aires)

La traduction totale est la composition de ces traductions
intermédiaire. Certaines ont prennent en argument des informations
supplémentaires sur le langage cible, dans leur mouture récursive:

- SLF -> LF prend un environnement LF (pour déterminer l'entité d'un
terme SLF)

- LF -> XLF prend une liste d'arguments

- XLF -> XLFa prend un environmment XLFa (pour déterminer le nom et
nombre d'arguments d'une fonction)


On veut comme traduction totale une fonction

down : SLF.term -> NLF.env -> (NLF.obj | NLF.fam | NLF.kind)

Comment l'obtenir à partir des traductions successives?


La réponse apportée (temporaire) est:

- d'ajouter un constructeur Meta(x) dans chaque langage (excepté NLF),
qui est un nom de constante NLF.

- Arrivé à XLF->XLFa, on prend en plus comme argument de la traduction
un environnement NLF qui sert à trouver le type de ces Metas. Ce type
est "détraduit" (?) de NLF vers XLFa pour pouvoir générer l'annotation
de type.


Cela a l'inconvénient qu'un gros type (le type d'une dérivation Γ ⊢ t
: A contenant un gros terme t) sera extrait de NLF (c'est linéaire en
la taille du terme).


Une autre solution proposée par Yann pour que cette opération soit en
temps linéaire est la suivante:

- changer la suite des traductions: tout non annoté (applications
n-aires, Π et λ aussi), c'est à dire tout ce qui peut se faire par
induction sur le terme et pas sur le type, puis on arrive dans une
syntaxe "à la NLF" (problème: et les noms des arguments? ils sont
déterminés par induction sur le type non?).

- la syntaxe à la NLF contient la place pour les annotations mais
elles sont en option. La deuxième phase consiste à annoter, et faire
les expansions. De cette façon, la deuxième phase peut réinjecter des
termes annotés sans avoir à les "détraduire": on les pousse juste tels
quels, en supposant qu'ils sont annotés.


== TC comme traduction vers les dérivations


On a aussi remarqué qu'un certain nombres de conditions sont vérifiées
sur le bon typage d'un terme SLF par la traduction successive. Par
exemple, SLF->LF vérifie le bon "sortage" (bonne application de la
règle produit), XLF->XLFa vérifie que les fonctions ne sont pas
sur-appliquées. Que vérifie le test de cyclicité des environnements de
NLF? Est-ce une autre façon d'écrire un typechecker? Quid de la
convertibilité?


== What's next

- mettre en place l'infrastructure de commit/checkout

- implémenter un TC pour LF (pour tests et comparaison)

- continuer l'implem du TC de NLF avec tests comparatifs

- en particulier comprendre la compatibilité des environnements
(qu'est-ce que ça implique, comment le faire efficacement…)



Thu Sep 16 19:04:58 CEST 2010


On continue la discussion d'hier et fixe des objectifs concrets pour
la suite.


Il semble se dessiner que le graphe non-orienté de dépendance des
informations dans les dérivations (voir tableau d'hier) est une
structure à investiguer pour notre problème. J'espérais (M.) pouvoir
trouver une structure d'arbre avec partage (DAG) qui se superposerait
aux dérivations et indiquerait pour chaque sous-terme d'un jugement
son effet sur le reste de la dérivation. Yann remarque à juste titre
que cette information est sûrement contenue dans le graphe qu'on a
dessiner, le DAG n'en serait qu'un arbre de couverture. D'autre part,
il semble important de ne pas maintenir une direction (cause -> effet)
dans ce graphe, puisque ceci induirait une dissymétrie arbitraire de
certaines règles. Yann fait remarquer le cas de l'application: ce
n'est ni l'argument qui impose le type du domaine de la fonction
(vision 'checking'), ni le contraire (vision 'inference' dans les
algos de TC bidirectionnel): les deux sont inter-dépendants. Deux
exemple ont étés étudiés:

- le sempiternel (λx. x+1) 42 ——> (λx. x+1) tt

- (λx. π₁ (¬x)) <<tt, 42> ——> (λx. π₁ (¬x)) <<42, tt>


La version actuelle du noyau a deux problèmes majeurs:

- il ne prend pas *vraiment* en compte les "dépendances" comme on les
entend depuis le début. Exemple: faisons une modification à une
sous-dépendance qui ne change syntaxiquement pas le jugement, tout va
bien; si par contre une jugement change d'une façon qui n'influe pas
sur le reste de la preuve (weakening par exemple, mais aussi
suppression d'un binding non utilisé — strengthening), ça ne se
branche plus du tout à l'endroit voulu, on doit reconstruire une
nouvelle dérivatione.

- On ne sait toujours pas comment reconstruire automatiquement une
vraie dérivation à partir d'un terme, même à la Church: l'utilisateur
doit fournir les deux lui même. Trois pistes:

     - fournir un outil externe réentrant qui calcule incrémentalement
     la dérivation à partir du terme (algo de checking ou
     d'inférence... paramétré par une fonctionnelle qui fait le
     memoizing)

     - réflechir à un système de vues: le terme est une version
     incomplète de la dérivation, c'est lui qu'on modifie et qui
     implique des changements automatiques sur la dérivation.

     - toutes les règles du système sont visible par le kernel: on
     peut donc faire de la proof search pour reconstituer la
     dérivation à partir du terme (type ? : (Γ ⊢ t :
     ?)). Inconvénient: suivant comment on a spécifié le système, la
     PS peut ne pas terminer.    

On a conclu que la dernière solution était la plus simple et sera
implémentée.


Une refonte du métalangage est aussi à prévoir. Au menu des
changements:

- les termes devraient être profonds, l'applatissement et le nommage
de tous les sous-termes réservé à l'environnement. Cela évite d'avoir
à utiliser des "bouchons" à la fin des déclarations de termes.

- le jugement correspondant au commit devra renvoyer le nom (hash) de
la tête du terme committé.


On se fixe donc deux versions successives:

- v1: refonte du kernel avec termes profonds, renvoi des noms et
surtout recherche de preuves pour la reconstruction des dérivations.

- v2: prise en charge du graphe de dépendance et vérification
"minimale" des sous-termes.


Tue Sept 14 15:26:00 CEST 2010


Refamiliarisation avec le système actuel, Revue des objectifs "big
picture", nouvelles idées sur les dépendances intra-programme.


Le système actuel est une implémentation d'un type-checker pour un
méta-langage, dérivé des PTS, permettant d'exprimer non seulement des
logiques et dérivations objet, mais aussi leur incrémentalité: en
nommant toutes les applications intermédiaires et en les annotant avec
leur type, on permet de maintenir plusieurs arbres de preuves dans le
même environnement (un répository), en partageant les sous-preuves
égales. Une grande limitation de ce système est que pour pouvoir être
partagées, les sous-preuves en question doivent voir l'annotation de
type de leur noeud racine s'écrire syntaxiquement exactement comme
celle attendue. Une conséquence est qu'un type correct mais dans un
environnement différent par exemple ne passe pas du tout. Du coup on
perd tous les avantages de l'incrémentalité dans le cas où le
changement effectué influe sur le type d'une variable: le reste du
programme est à retyper.


En interlude de cette difficile question, quelques remarques générales
sur l'architecture du système et les objectifs concrets. Dans la
version actuelle (MIPS), on a spécifié une seule opération Γ ⊢ t : A ⇒
Δ, qui correspondrait plus ou moins à commit. Plus ou moins parce que:
commit devrait prendre un répo, un terme du langage objet, c'est à
dire non-typé et profond. En revanche, le terme t dans le système
appartient au langage méta et est aplati. Bref, pas très cohérent me
semble-t-il.

Yann – a raison – insiste sur la nature interactive du système: il
faut pouvoir interagir avec les erreurs du noyau pour les corriger, de
façon manuelle ou automatique (stratégies de correction).

De plus, il faut dégager une architecture plus claire pour le
noyau. La proposition de Yann, discutée ci-dessous, est de séparer le
processus de typage actuel en deux parties:

- grâce au répo actuel Γ et au contenu édité t, générer un "patch"
comme la différence syntaxique entre les termes t et celui de Γ. C'est
ce que faisait le partage bottom-up de notre typeur avec son système
de noms unique et de partage maximal de sous-dérivations.

- puis vérification des types pour ce "patch" (et les endroits où il
se branche dans Γ.


Un exemple où cette architecture pêche est le suivant: changeons le
programme [ let x = false in ...] en [let x = 1 in ...] (les ... c'est
le reste t du programme, très long). le "patch" généré syntaxiquement
est [let x = 1 in t], qui ne type pas parce qu'au moment de vouloir
typer le nouveau t, on cherche une dérivation de x : nat ⊢ t : unit
alors qu'on en a une de x : bool ⊢ t : unit. Résultat, on devrait sans
doutes retyper l'intégralité du t, ce qui est trop (il suffirait de
retyper au moins aux occurences de x dans t p.ex.). Moralité: le
système actuelle utilise un DAG pour représenter les différentes
versions d'un même arbre de preuve/dérivation de programme, mais n'y
stocke pas les dépendances entres les objets, qui permettraient le
typage plus fin promis.


Première proposition/réplique: oui mais le let est une structure
spéciale qui nécessite un traitement particulier puisqu'il n'est pas
nécessaire de voir une série de lets à toplevel comme des "peignes"
séquentiels: on peut les voir comme des définitions simultanées, en
réduisant le contexte à son minimum dans chaque définitions et en
s'assurant juste que le graphe des appels est acyclique (pas de
dépendance circulaire). Réponse: le problème se pose déjà dans
l'exemple suivant, un peu artificiel mais repris dans la suite: on
veut transformer (λx. x+1) 2 en (λx. x+1) true. Evidemment la version
cible est mal typée, mais on veut pouvoir le vérifier rapidement en
type-checkant à partir de l'occurence de x dans le corps de la
fonction: x a le type bool dans le contexte x : bool, ok, maintenant
on applique ce x a +, et c'est là que ça échoue car + attend deux
nats. Se dégage une idée de causalité *à l'intérieur* d'une dérivation
donnée, qui donnerait une bonne base pour tracker les dépendances à
l'intérieur d'un terme. Encore faut-il trouver un bonne représentation
des preuves, qui intègre cette notion de dépendance.


Un arbre de preuve est un arbre dont chaque noeud est une instance
d'une règle logique. Il est facile de vérifier que chaque application
est bien une instance d'une règle existante, mais difficile d'évaluer
l'impact du changement d'une sous-dérivation: cela nous obligerait
potentiellement à modifier d'autres parties de la preuve et il est
difficile de savoir lesquelles sans revérifier l'intégralité de la
preuve. Il semblerait que ceci est dû au fait que la preuve de départ
nous est donnée de façon statique, sans l'historique de la
vérification de chacune de ses parties et le flot d'information qui en
découle. En annotant les arbres de preuve avec cette information, qui
semble pouvoir prendre la forme de contraintes accumulées et
vérifiées, et chaque contrainte par la sous-dérivation qui l'a généré,
on retrace la *dynamique du type-checking*. Cette information peut
être lue aussi comme un rapport de causalité entre différentes parties
d'une preuve: si tel atome est A, c'est parce qu'il a été
contraint par l'application d'une règle à l'être, peut-être à un tout
autre endroit de sa preuve. Ainsi, si on change cet atome en B, on
peut observer son effet on suivant cette chaîne de causalité: si on
découvre une incohérence, le changement est impossible et on saura
précisément pourquoi. Ceci devrait aider à implémenter l'interactivité
du vérificateur incrémental.


On s'écarte donc un peu de l'implémentation actuelle mais on a
l'espoir de résoudre un problème grave (comparaison syntaxique, trop
stricte pour observer finement les modifications, dans l'environnement
notamment).


Mon Jul 26, 2010  5:39 PM


Je poursuis les expérimentations sur le prototype de façon à exercer
la première version de notre formalisme. Je vais maintenant écrire un
vérificateur de type pour le STLC dans le cadre d'une architecture du
système _avec_ métathéorèmes constructifs.


Le type à donner à ce métathéorème TC (comme Type-Checking) ne me
semble plus évident dans notre formalisme à la PTS. En effet,
intuitivement, j'aimerais écrire :


  TC :: (Γ : env) (t : term) → ∃ (τ : type). (Δ : Γ ⊢ t : τ)


(Ce qui est clairement une réminescence de l'ancien formalisme et qui
montre que je n'ai pas encore intégré le nouveau.)


Est-ce que je devrais plutôt décomposer ce méta-théorème en deux? Par
exemple, je pourrais avoir type_of :


  type_of :: (Γ : env) (t : term) → type


et ensuite :


  TC :: (Γ : env) (t : term). (Γ ̌⊢ t : type_of (Γ, t))


ce qui rentre dans notre formalisme si je ne me trompe pas.


Par contre, d'un point de constructif, ça semble peu élégant : le type
checking serait implémenté dans "type_of" et réimplémenté, dans une
version plus "verbose" - avec construction explicite de la dérivation
-, dans "TC". 


J'ai dû manquer quelque chose.


M: Tentative de réponse: pourquoi pas ajouter la définition de la
somme dépendante, et travailler avec elle? En Twelf ça donnerait:


  sig : {A:type} (A -> type) -> type.

  exist : {A:type} {P:A -> type} {x:A} P x -> sig A P.

  proj1 : {A:type} {P:A -> type} sig A P -> A.

  proj2 : {A:type} {P:A -> type} {sg : sig A P} P (proj1 A P sg).


Puis le type de l'algo de type-checking:


  error : type.

  typeError : error.

  union : {A B : type} type.

  inl : {A B : type} A -> union A B.

  inr : {A B : type} B -> union A B.

  

  type_of : {Γ : env} {t : term} union (sig term (λτ. Γ ⊢ t : τ)) error.


A condition de réintégrer le λ ça marche chez nous, en
aplatissant tout ces termes composés... la question est évidemment
comment le réintégrer :) 

  

Mon Jul 12, 2010  9:48 PM

[extrait d'un mail de Matthias après l'exposé MIPS 2010]


Il y a eu deux questions on-line, une de S. Autexier sur comment
réaliser les définitions alternatives qu'on promettait au début (et
faisant référence au lambda-bar-mu-mu-tilde, à regarder): réponse
facile. L'autre était plus critique, de M. Kohlhase qui a dit qu'il
avait fait un travail très similaire, et qu'on avait sans doutes un
approche trop ad-hoc (?), et que leur projet bénéficiait déjà d'une
implémentation qu'il suffirait de réutiliser.


J'ai eu d'assez bons échos off-line: 3 personnes sont venu me dire que
c'était très intéressant, 2 ont posé des questions très pertinentes
qui m'ont fait réaliser plus précisément le boulot qu'il restait à
faire. En particulier Andrei Paskevich qui a beaucoup aimé le projet
et ses applications (on a parlé du coqtop idéal au déjeuner)


J'ai aussi discuté assez longuement avec le post-doc de Kohlhase à
propos de ce projet similaire (dont le nom m'échappe maintenant). En
fait je crois que Kohlhase n'avait pas bien compris l'exposé, il me
semble justement que leur projet est moins général que le nôtre :)
Bon, c'est à première vue, il faudra lire précisément leurs papiers...


Fri May 21 17:44:10 CEST 2010


Cher journal,

Discussion téléphonique aujourd'hui sur plusieurs aspects tournant
autour du noyau actuel (alagit), et directions pour la suite:

- explications de Yann sur l'embryon de système de "mémoisation de
calculs",

- mise en évidence des différences conceptuelles de ce système avec
  celui du noyau programmé,

- clarification sur la différence entre un système qui calcule et un
  système purement syntaxique,

- décisions sur la direction des travaux.


1/ Le système de Yann (mail du 12/5/2010):


Il s'agit d'une variante du système précédent, où le typage est vu
comme la vérification d'une solution vis-à-vis d'un problème. Les deux
différences fondamentales sont:

- qu'il prend en compte le fait que la réduction d'un problème à un
autre se réduit potentiellement vers une solution "canonique".

- et qu'un problème n'est pas une seule entité syntaxique, mais une
classe d'équivalence, au moyen une égalité ad-hoc donnée.

Il est paramétré par:

- un ensemble P de problèmes (les atomes dans le système
d'avant). Appliqués à d'autres problèmes ils forment des instances τ.

- un ensemble F de réduction de problèmes (procédures de décision ou
règles de typage du langage objet)

- Pour chaque problème, deux fonctions Ψ et φ, de décision d'égalité
entre deux instances et de destruction d'une solution en ses
sous-problèmes.

- Un prédicat singleton τ → bool, décidant si une instance a une
unique solution (autrement dit si toutes les solutions sont
équivalentes, sorte de proof-irrelevance).

Le typage consiste à vérifier si un ensemble de déclarations et
définitions de solutions à des instances de problème est valide, et en
même temps si ces instances ont déjà des solutions définies plus tôt,
de façon à les réutiliser. Si ce n'est pas le cas, on cherche une
solution équivalente (pour φ), ou sinon on évalue un pas la solution
proposée et en recommence récursivement le processus. Si et seulement
si la solution est totalement inconnue, on l'ajoute à l'environnement
courant.


2/ Système syntaxique / système avec calcul.


Le système actuellement implémenté a deux caractéristique disjointe et
indépendantes:

- il est purement syntaxique, parce que les objets déclarés sont de
purs noms. Un exemple: définissant le λ-calcul simplement typé dans
notre système (fichier stlc.v), rajouter une déclaration Weak de type
(Γ⊢t:A → Γ,B⊢t:A), c'est rajouter une règle supplémentaire, on ne peut
pas montrer qu'elle est admissible. On ne pourra pas détruire Weak(Γ,A
⊢ t u : B) en App(Γ⊢t:CB, Γ⊢u:C).

- il implémente le partage maximal. Ceci est fait en "découpant" les
termes profonds en leur noeuds applicatifs, et donnant des noms (et
des types) à chacuns de ces résultats intermédiaires.

Découper c'est n'autoriser que des termes applicatifs plats, nommer
les résultats intermédiaires c'est introduire un produit spécial
contenant ces termes plats.


Il a été dit qu'il existe plusieurs façons d'introduire du calcul dans
le système:

- mode externaliste: En dehors du système, une déclaration (un nom)
est associé à un programme récursif – un client, en caml par exemple –
qui construit un terme du système automatiquement. Ce programme est
paramétré par son appel récursif, de façon à ce qu'à chaque appel
récursif, le système puisse vérifier s'il n'a pas déjà une solution
calculée pour le problème en cours. Dans ce sens, c'est la solution
qui se rapproche le plus du système de Yann.

- mode internaliste: On étend le système avec du vrai calcul, à la
façon des PTS. Par exemple, on prend un PTS qu'on restreint avec une
application seulement à des variables, et qu'on étend avec le produit
singleton (règles de typage habituelles, y compris conversion):

t ::= x | (x:t).t | (x=t:t).t | λx:t.t | t x | s

On pourrait y définir un terme dont le type est celui du weakening,
dont l'application à une dérivation se réduirait vraiment vers une
autre dérivation.


Remarque: Dans ce cadre intéressant, il me semble que hash-consing et
mémoïsation se confondent élégemment: traditionellement, le
hash-consing est le partage de sous-termes égaux, et la mémoïsation
est le partage de résultats de calculs. Dans un PTS comme décrit plus
haut, un sous-terme devra être indexé (hash-consé) modulo son calcul,
c'est-à-dire que s'il réapparaît plus tard, on aura sa valeur
normale. Ce point est à développer quand on y arrivera.


3/ Directions des travaux


Il a été conclu de s'intéresser d'abord au système sans calcul, qui
présente déjà des difficultés:

- Que doit-on représenter comme un terme dans notre système:
uniquement de la syntaxe? des "patches"? des commits? des branches?
Quel choix d'univers adopter (conséquence des choix précédents)?

- Que nous apportera le bootstrapping? A-t-on besoin de constructions
spéciales (p.ex. de hashing) pour le définir à l'intérieur de
lui-même?

- Comment définir l'inférence de type d'un langage "à la Curry"? Si
toutes les informations de typages ne sont pas dans la syntaxe, un
algorithme doit être en charge de les inférer.

- Quelle interface utilisateur pour le système?


Deux directions parallèles donc:

- Reflexion sur l'interface utilisateur du système, pour avoir une
spécification claire. Dans synthèse.tex (?)

- Implémentation d'un langage objet "à la Church", le lambda-calcul
simplement typé p.ex. Cela comprend: parseur/pretty-printer,
conversion des termes dans l'AST du système, et après voir...



Mon Mar  8, 2010 11:35 AM


Aujourd'hui, nous avons discuté de différentes formulations du langage
de patchs, dont le niveau d'abstraction varie. 


Matthias a mis le doigt sur une sorte d'incohérence entre deux

propriétés du système actuel :


1. On veut maximiser le partage et donc s'assurer que l'on a un

nom unique pour chaque élément du domaine d'interprétation de

la syntaxe. Ceci permet de vérifier les contraintes facilement

par une simple unification à la Huet.


2. Il y a plusieurs façons de construire un élément du domaine

d'interprétation : par exemple, le type-checker ou le weakening

sont deux façons d'obtenir la même dérivation de typage. Du coup,

il semble difficile de vérifier une contrainte de la forme

"X1, ..., XN = E" sans aller voir le code des transformers. Il

faut sortir de son chapeau une preuve d'égalité qui peut être
arbitrairement complexe pour quotienter les noms de façon à maintenir
la propriété de partage optimal.


Le type-checking du langage de patchs semble être compromis par

cette tension. 


Du coup, Matthias propose de se restreindre l'ensemble des transformers

aux constructeurs de données. Ainsi, les objets qui apparaissent dans

le langage de patchs sont exclusivement des données, c'est à dire

des descriptions canoniques d'éléments du domaine d'interprétation

et non des descriptions (non canoniques) caractérisées comme des

résultats d'un calcul. Le système obtenu est très proche de git dans

le sens où il est orienté-données. On ne parle plus de la façon dont

les objets sont construits mais des objets eux-mêmes. C'est une sorte

de "sémantique de trace" du développement. Les étapes du calcul sont

caractérisées comme les transformations d'une donnée en une autre

donnée. Il existe certainement un langage de bas-niveau permettant

de décrire ces transformations de façon canonique : il se peut

même que ce soit tout simplement un langage formé des listes

d'égalités de la forme "X = F(X1, ..., XN)" assurant le partage

maximal.


Le type-checking dans ce langage orienté données peut s'exprimer

de nouveau comme une unification à la Huet mêlé à un type-checking

de types dépendants. L'interprétation des répositoires est définie

par un point fixe mémoisant de fonctionnelles de production

des constructeurs (des procédures de décision en somme), qui auraient

le type :


((forall x, F x) -> (forall x, F x)) -> (forall x, F x)


Comme on se place à un niveau d'abstraction

très bas, on ne peut pas internaliser grand chose dans ce

langage de patchs. Matthias a fait remarqué que, dans ce cadre,

le weakening nécessite une déconstruction/reconstruction

explicitée dans le patch (on voit la trace de la déconstruction

et la trace de la reconstruction apparaître sous la forme de

résultats intermédiaires nommés).


Yann, sans trop en avoir conscience :-), a proposé de travailler

"modulo" des procédures de recherche de preuve. Par exemple,

dans le cas du weaking, on peut supposer que l'on se donne,

en plus des procédures de décision évoquées plus tôt, des

"fonctionnelles d'incrémentalité" qui auraient le type :


((forall x, F x -> G x) -> (forall x, F x -> G x)) ->

(forall x, F x -> G x)


Le weakening tombe dans ce cadre, puisqu'il s'agit de

construire une dérivation qui parle d'un terme "t"

à partir d'une autre qui parle du même terme "t". 


On pourrait les utiliser lors des appels récursifs des procédures

de décision : on se donne la possibilité d'utiliser

un résultat intermédiaire issu de l'application d'une fonction

pour déduire le résultat final et non uniquement l'appel

récursif sur un sous-terme de x.


Ainsi, du coup, quand on a un objet syntaxique dans le patch,

son interprétation est multiple : elle représente tout ce

que l'on peut déduire par application des fonctionnelles. On

travaille "modulo" ces fonctionnelles, comme l'a suggéré Matthias.


Vu à la lumière du memoizing, j'ai l'impression qu'on est

en train de se donner la possibilité de piocher des résultats

intermédiaires d'évaluation d'une fonction F soit en observant

ce que l'on déjà calculé à l'aide de F, soit en utilisant

une sorte de coercion des résultats d'une fonction G déjà

calculés eux aussi.


Tout ceci est très intéressant!


Thu Feb 25, 2010  6:10 PM


Reprise du développement Coq: même après deux jours, je n'arrive pas

à comprendre ce que j'ai fait :^). La bonne explication, c'est que

c'est trop compliqué.


Du coup, j'ai imaginé une autre façon d'attaquer le problème:


1. On simplifie un peu l'algèbre de type des transformers.

On sépare clairement dans la syntaxe des type les binders et

les égalités/bornes. Ca donne quelque chose comme ça :


    o     ::= Sigma { Eq* }

    Sigma ::= sigma (x : theta). Sigma

    i     ::= Pi { Eq* } . o

    Pi    ::= pi (x : theta). Pi

    Eq    ::= X1, ..., XN = E

    E     ::= X

           |  F (X1, ..., XN)


Voici un type de transformer :

Pi (X : theta) (Y : theta') { X = F(Y1, Y2), Y = G(Y1) }.

Sigma (Z : theta'') { Z = H(Y3) }


On peut voir les égalités comme des pré/post-conditions

associés à la fonction de type


Pi (X : theta) (Y : theta'). Sigma (Z : theta'')


2. Pré-interprétation des types


On se donne ensuite une pré-interprétation des types qui

fonctionne un peu comme la première interprétation que nous avions

définie sur les types sans égalités (moralement, Pi = forall,

Sigma = exists).


On se débrouille "juste" pour que cette interprétation

soit paramétrée par une interprétation des égalités, abstraites

pour le moment. 


3. Pré-interprétation des expressions


On définit une pré-interprétation des expressions qui est

paramétrée par l'interprétation future des égalités IH, qui

attend un environnement d'objets et un environnement de

transformers contenant des pre-interprétations des transformers

appliquées sur IH.


La pré-interprétation d'une variable est un lookup dans

l'environnement d'objets.


La pré-interprétation d'une application de transformers

"F(Y1, ..., YN)" va chercher la pré-interprétation de F

dans l'environnement des transformers et l'applique

aux pré-interprétation des Yi. Cela produit un

terme Coq en attente des preuves d'égalités. 


4. On ferme la boucle.


C'est le point le plus délicat. Il faut maintenant instancier

les pré-interprétations sur des familles d'égalités plus

petites au sens où elles utilisent des pré-interprétations définies

plus tôt (sur des environnements d'objets plus petits).


On obtient alors des familles imbriquées d'interprétations mutuelles

entre types et expressions. Ca doit avoir des liens avec

"Coq en Coq" d'ailleurs ...


J'ai l'intuition mais de là à le faire passer en Coq... Au secours :-)


Tue Feb 23, 2010 12:01 PM


Aujourd'hui, je me suis souvenu de la remarque de Matthias qui

suggérait qu'il serait plus simple de définir l'interprétation

d'un type de transformer sous la forme d'une relation plutôt

qu'une fonction. Effectivement, grâce à cette idée, je suis allé une
étape plus loin dans la formalisation en Coq.


Hier, j'étais gêné par le fait que la fonction d'interprétation

des types de transformer devait être paramétrée par un environnement

contenant les transformers. Or, cet environnement devait contenir

l'interprétation des transformers dont le type était défini

par la fonction que l'on était en train de définir.


En fait, j'ai relâché cette dernière contrainte temporairement:

l'environnement des transformers contient maintenant des valeurs

Coq _closes_ de type "sigT (fun (T : Type) => T)".

Dans un second temps, on a invariant de bonne formation qui nous assure

que lorsque l'on extrait un transformer de cet environnement, le

type de la valeur Coq obtenu est en fait une interprétation

du type de transformer attendu.


En résumé, cette coercion a posteriori, qui est autorisée par
l'invariant de bon typage des expressions de patch, permet de casser
la récursion mutuelle entre le type des environnements de transformer et

l'interprétation des types de transformer.


J'ai l'impression que ca va fonctionner.


Un nouveau problème est apparu pour définir l'interprétation

des applications de transformer "F (Y1, ..., YN)".


On a un type Coq T ainsi qu'une valeur FC Coq de type T issue

de l'extraction de F de l'environnement des transformers.


On aimerait former l'application Coq :

<center>

FC YC1 ... HEQ1 ... HEQM ... YCN


</center><flushboth>où les YC1s sont les interprétations des Yk et les HEQi sont

les preuves des égalités attendues par F.


Mon intuition, c'est que l'on doit pouvoir calculer à partir

du type (de transformer) de F et des Yk, un terme Coq de type


T -> YT_1 -> ... -> YTN -> HEQ1 -> ... -> HEQN -> type_de l_application


En gros, cela nous dit que l'on peut différer la preuve de HEQi

à plus tard. Or, ces HEQi peuvent être des arguments du constructeur

de l'inductif d'interprétation (on doit certainement utiliser une

liste de propriétés, de type Coq "list Type" pour rendre cela
possible).

</flushboth><nofill>		   

C'est encore un peu flou. Je vais laisser décanter tout ça et retourner

à la préparation de mes cours. </nofill><flushleft>

</flushleft>

Mon Feb 22, 2010  6:40 PM


Je viens de passer une heure et demie à essayer de dérouler les
dépendances du système actuel. Je vais certainement trop vite

mais pour le moment, je ne vois pas comment écrire la fonction

d'interprétation d'un type de transformer en un Type Coq.


Le problème vient de la dépendance "expression -> type" que

nous avons introduit. L'interprétation des expressions et

des types sont maintenant mutuellement récursives. Or, pour

ces interprétations doivent maintenant attendre un environnement

fournissant la spécification des transformers mais le type de

ces spécifications fait appel à l'interprétation des types!


Je n'arrive vraiment pas à briser ce cycle ... et pourtant,

je sens bien qu'il n'y a pas de problèmes très profonds car

on n'applique que des transformers localement clos.


Bref, ce n'est pas clair aujourd'hui. Cela le sera peut-être

plus demain.


Sat Feb 20 16:18:54 CET 2010


Une idée à propos de l'α-conversion: Jusque la on a pensé la
formalisation du langage objet comme toujours _nommée_. Et aussi, on a
laissé implicite le fait que pour l'utilisateur final qui veut écrire
du code, il faut toujours _parser_ avant de laisser la main à
p.ex. l'algo de diff.


Quitte à faire ce prétraitement, on peut aussi passer en DeBruijn de
façon interne, mais en gardant l'info de nom, à la façon Coq. exemple:


type term =

| Var of int

| Lam of string * term

| App of term * term


De cette façon, on a tous les avantages des DeBruijn, en gardant les
noms, et l'α-conversion se réduit à changer le nom seulement au niveau du
binder. En plus on peut comme ça réutiliser toutes les formalisation
off-the-shelf (peut-être).


    -m


Tue Feb 16, 2010  9:09 PM


Après avoir commencé à spécifier ce qui précède en Coq, j'ai remarqué

qu'il n'y a pas d'interprétation statique des expressions mais seulement

une interprétation dynamique. Les interprétations des expressions et des types

sont définies de façon mutuellement récursives.


La suite demain ...


Tue Feb 16, 2010 12:18 PM


Quelques notes sur l'algèbre de type. 


Nous avons convergé vers la syntaxe suivante pour les types de métathéorème 
(le suffixe "s" signifie que l'on a un vecteur en ASCII) :


  parameter θ :: le type des objets syntaxiques


  ζ ::= Π Σ
  Π ::= ∀ (Xs : θs := e). Π
     |  ∀ (X : θ). Π
     |  •
  Σ ::= ∃ (Xs : θs := e). Σ
     |  ∃ (X : θ). Σ
     |  •


  e ::= X
     |  F (Xs)


Cette syntaxe est dépendante de deux façons. Par la faute des lieurs ∀
et ∃, on introduit des objets auxquels on peut faire référence dans la
suite du type dans deux contextes :


  1. Dans les types θ, comme dans le type 
     "Γ ⊢ t : τ" 
  2. Dans les bornes e, comme dans la borne 
     "∀ (Γ' : env := cons (x, τ, Γ)). …" 


Ca complique quelque peu l'interprétation des types ζ en termes de
types Coq (mais pas trop). On se donne une syntaxe des environnements
de typage des objets syntaxiques et des métathéorèmes Δ :


  Δ ::= • | Δ (X : θ) | Δ (F : ζ := F̃) 


et on définit, de façon mutuellement récursive, <italic>l'interprétation d'un
type ζ sous un environnement Δ </italic>et <italic>l'interprétation statique d'une
expression e</italic> <italic>sous un environnement Δ</italic> de la façon suivante :


<underline>I</underline>nterprétation statique d'une expression e<underline> </underline>sous un environnement Δ


Signature :

  〚 • 〛_Δ : (e : expr) → (〚 Δ 〛 → 〚 type_of e 〛_Δ)


— Cas "variable"

  〚 X 〛_Δ = fun (σ : 〚 Δ 〛) ⇒ σ (X)


— Cas "application de transformer"

  〚 F (Ys) 〛_Δ = fun (σ : 〚 Δ 〛) ⇒ F̃ (〚 Ys 〛_Δ σ)


<underline>I</underline>nterprétation d'un type ζ sous un environnement Δ


Signature :

  〚 • 〛_Δ : (ζ : spec) → (〚 Δ 〛 → Type)


— Cas "quantification universelle bornée" 

  〚 ∀ (Xs : θs := e). Π 〛_Δ =

  fun (σ : 〚 Δ 〛) ⇒ ∀ (Xs : 〚 θ 〛_Δ). Xs = 〚 e 〛_Δ → 〚 Π 〛_{Δ (Xs : θ)} Xs


– Cas "quantification universelle non bornée" 

  〚 ∀ (Xs : θs). Π 〛_Δ =

  fun (σ : 〚 Δ 〛) ⇒ ∀ (Xs : 〚 θ 〛_Δ). 〚 Π 〛_{Δ (Xs : θ)} Xs


(règles similaires pour les quantifications existentielles)


L'interprétation statique des expressions n'a de sens que pour les
expressions bien typées (celles pour lesquelles la fonction typeof

est définie). Du point de vue de l'implémentation en Coq, on

voudra sûrement indicer le type des expressions par leur type). 


Maintenant, l'interprétation dynamique des expressions de patch

s'opère dans un répositoire de type Δ qui se définit ainsi :


  ρ ::= • | ρ (X = X) | ρ (Xs ↦ e)


avec les invariants de bon typage suivants:

- Si X₁ = X₂ dans ρ, alors Δ (X₁) = Δ (X₂)

- Si Xs ↦ e dans ρ, alors Δ (Xs) = typeof e


L'interprétation dynamique d'un répositoire sous un environnement Δ

est une fonction qui calcule une paire formée d'une structure de
donnée associative entre des variables et des objets (internalisés en
Coq) et d'une preuve d'(une conjonction de) égalité(s) entre des
objets (internalisés en Coq).


  《•》_Δ :

  (ρ : repositoire de type Δ) →

  { X ↦ 〚 Δ (X) 〛_Δ } × (⋀_{X₁ = X₂} (〚 Δ (X₁) 〛_Δ = 〚 Δ (X₂) 〛_Δ))


Voilà pour un gros tas de symboles qui n'ont sûrement pas encore beaucoup

de sens. En tout cas, ça donne une base pour reprendre le développement Coq

avec nos dernières idées. 


Tue Feb 16, 2010 12:19 PM

Forwarded conversation
Subject: Petite question.
------------------------

From: Yann Régis-Gianas <<yann.regis-gianas@pps.jussieu.fr>
Date: 2010/2/13
To: matthias <<puech@cs.unibo.it>


Salut Matthias,

j'espère que ton déménagement se passe bien. Juste une toute petite
question : je ne me souviens plus de ta dernière remarque au sujet des
types d'entrée des méta-théorèmes.

Pour donner le type de l'inversion du lambda, par exemple, dans la
nouvelle syntaxe des types de méta-théorème, il faut écrire quelque
chose comme :

forall env x t tau1 tau2.
delta tau3 := tau1 -> tau2 in
forall d1 : env |- lambda (x : tau1). t : tau3
delta env' := env; x : tau1 in
sigma d2 : env' |- t : tau2

Je ne sais plus ce que l'on avait dit : est-ce le typechecking de
l'application de ce méta-théorème nécessite de l'unification? J'ai un
doute car j'avais l'impression que nous avions répondu "non" à cette
question mais je ne vois plus du tout pourquoi.

A plus,
--
Yann

----------
From: Matthias Puech <<puech@cs.unibo.it>
Date: 2010/2/15
To: Yann Régis-Gianas <<yann.regis-gianas@pps.jussieu.fr>


Salut Yann,
Ça y est, installé!

Alors... de ce que j'ai retenu, on s'était dit que le type [deriv]
prenant plus des vrais termes,types,envs mais plutôt des variables (de
patch). Pour moi l'inversion ressemble plus à ça du coup: delta tau3
:= arrow(tau1,tau2) in // construction du nouveau type delta t2 :=
lambda(x,tau1,t) in // du nouveau terme forall d1 : deriv(env,t2,tau3)
// on a une deriv des deux dans env delta env' := cons(env,x,tau1) in
// alors on a une deriv dans env sigma d2 : deriv(env',t,tau2) // des
objets initiaux

Tu es d'accord? Laisse-moi penser à haute voix: ce transformer sera
appliqué à 8 arguments (c'est beaucoup). Les deux avant-derniers, tau3
et t2 devront être *strictement* construits par application du
transformer arrow (resp. lambda) aux arguments 4 et 5 (resp. 1, 3 et
2). Et on ira pas voir ce que c'est que arrow ou lambda.  Comme tu as
dit, on compare les noms des transformers seulement.  Donc pour moi,
non, pas besoin d'unification, mais par contre on a des contraintes
d'égalité entre arguments (c'est des types dépendants quoi...). A un
état du repository donné, on a donc un lot de "contraintes" comme tu
le suggérais, qu'on a accumulé à chaque delta dans un sigma (des
définitions dans la conclusion) et qui pourront nous servir dans la
suite à appliquer des transformers qui nécessitent une forme
particulière.

Est-ce que cette approche est complète? Selon moi non, parce que ces
arguments correspondant à des deltas peuvent avoir été crées par
d'autres transformers plus... haut niveau (transformation CPS par
exemple). Ces derniers doivent donc refléter dans leur signature qu'il
créent bien des objets de type e.g. arrow(A,B) pour la CPS (par un
delta à la fin).

En fait, je commence a me poser la question du bien-fondé de nos deltas.
En toute généralité, n'importe quel transformer a besoin de
potentiellement n'importe quelle forme d'hypothèses (aussi profonde
soit-elle). Donc chaque transformer doit préciser exactement la forme de
ses hypothèses, et pas moins. Ça exclue donc les transformers comme la
CPS sur les types par exemple (A-A-traduction): sa signature devrait
donner la forme exacte de tous les types de sortie en fonction du type
d'entrée, donc contenir l'algo tout entier! (c'est assez clair ce que je
viens de dire?).

Bref, on a un problème, qui est en lien à celui-ci: on veut pouvoir
typer un patch sans rien savoir du contenu calculatoire des
métathéorêmes qu'on utilise (juste leur signatures). Mais certains
patches ne s'appliquent que sur des programmes d'une certaine forme,
même s'ils sont en soi corrects (transforment un programme bien typé
en un programme bien typé). La question est: comment faire respecter
ces contraintes sans exécuter les transformers?

Je vois deux alternatives:

- Ou bien on se dit que de toutes façon on n'a besoin que des règles
de typage et de leur inversion. On exclue donc les transformers qui
font des choses variée en fonction de leur entrée, on ne garde que
constructeurs et destructeurs (ils ont tous un seul niveau de
profondeur dans leurs deltas). Dans ce cas je peux imaginer que c'est
complet, et en plus, c'est peut-être juste ça qu'on veut: un algo de
transformation, ad-hoc dans le sens qu'il doit être réécrit pour
chaque programme d'entrée. Pas de CPS, pas de patchs génériques
d'alpha-conversion ("transforme tous les f en g dans n'importe quel
programme d'entrée"), pas de récurseurs, uniquement des chaînes de
transformations locales. Peut-être que l'on veut juste ça.

- Ou bien on peut executer les transformers et se rammener à des
  objets plus "primitifs" (les constructeurs du langage), mais de
  façon lazy. Dans l'exemple précédent, la A-A-traduction prendrait un
  type, en renverrait un, opaque, mais si l'on doit vérifier que le
  type résultant est une flèche, on déroule juste un coup sa
  définition (on ne calcule pas au typage toute la traduction du type
  argument, juste ce qui nous faut pour typer la suite). C'est un gros
  changement par rapport à ce qu'on s'est dit avant, peut-être qu'il
  faudrait bien investiguer le premier point avant de s'y résoudre.

Voilà mes quelques remarques!

Au fait, j'ai parlé à Andrea des patches aujourd'hui, et aussi de
l'autre versant de mon projet de recherche (recherche de preuve par
focusing dans CC). Il a été très critique sur les deux, et me propose
de bosser sur un theorem-prover premier-ordre d'égalité à interfacer
avec Matita. Dur dur la thèse... Mais je vais quand même lui présenter
plus en détail notre boulot vendredi, ainsi qu'à Claudio sans
doutes. Je te tiens au courant!

A bientôt,
       -m

----------
From: Matthias Puech <<puech@cs.unibo.it>
Date: 2010/2/15
To: Yann Régis-Gianas <<yann.regis-gianas@pps.jussieu.fr>


PS: J'ai parlé de notre projet à un copain, un vrai codeur lui, qui
travaille dans une boîte de supply-chain, parle de web-services et de
facturation. Il m'a fait cette remarque: "super, alors plus de
conflits quand un mec committe un patch de refactoring trivial et un
autre une feature dans le même fichier". J'ai pas vraiment su quoi lui
répondre mais je garde cet exemple dans un coin de la tête pour le
moment ou on parlera de concurrence :) a+

----------
From: Yann Régis-Gianas <<yann.regis-gianas@pps.jussieu.fr>
Date: 2010/2/16
To: Matthias Puech <<puech@cs.unibo.it>


Salut Matthias,

merci pour tes remarques. Je te réponds au fil du texte...

2010/2/15 Matthias Puech <<puech@cs.unibo.it>:
> Ça y est, installé!

Quelle rapidité! Passe un bon séjour.
> [...]

On a la même chose en tête. Je vais essayer d'écrire des notes un petit peu
formelle pour fixer ça.
> [...]

Je ne sais pas trop ce que tu entends par "complète". Est-ce que c'est
lié aux problèmes que tu avais illustrés avec la mise en forme CPS?

La forme d'incomplétude que nous avons, il me semble que c'est notre
capacité à déduire à l'intérieur du langage de patchs des implications
de la forme : "Si tel type a été produit par la transformation CPS,
alors c'est une flèche." Du coup, il n'est pas possible d'appliquer
directement un transformer qui attend une flèche sans préalablement
repasser au niveau du dessous en appliquant un lemme qui paraphrase
exactement l'implication dont nous avons besoin.

C'est une excellente remarque.

En fait, j'étais en train de me poser une même question similaire en
réfléchissant au type-checking : on peut très bien définir un
algorithme de type-checking pas trop compliqué en récoltant des
contraintes d'égalité entre variables (une sorte d'unification en
fait, je le maintiens :-)).

Le langage typé obtenu décrit des patchs de bas-niveau, que
j'appellerai P_low, dans le sens où on explique vraiment toutes les
étapes d'application des transformers. Ces explications nécessitent un
grand nombre de transformer (toutes les implications permettant
moralement de déduire d'une égalité X := F(Ys) une autre égalité de la
forme X := G(Ws).)  Note en passant que je généralise encore la notion
de répositoire, qui est maintenant formé d'un ensemble de
multi-équations mettant en relation des termes du premier ordre qui ne
sont pas nécessairement des variables. Tant que ces termes ont des
types similaires, ça semble avoir encore du sens.

Le langage P_low est un langage cible idéal pour notre diff mais il
est peu pratique pour le programmeur! Il y a deux raisons à cela.
D'abord, le programmeur doit constamment prouver de nouveaux
métathéorèmes pour pouvoir composer les métathéorèmes qu'il a sous la
main (l'effet "vilaine incomplétude). En effet, comme tu le dis, si on
veut se passer de ce travail, il faut que les spécifications de
métathéorèmes ne s'expriment qu'à l'aide d'un ensemble fini de
constructeurs/destructeurs, naturellement la syntaxe des objets
sous-jacents, mais dans certains cas, il y a de grandes chances pour
que cela nécessite d'écrire la spécification du métathéorème sous une
forme très (trop) précise (en gros, son code exprimé sous une forme
équationnelle). La deuxième raison, c'est que c'est un langage de trop
bas niveau de toute façon : on veut donner les grandes lignes des
transformations et pouvoir compléter les parties "stupides"
automatiquement.

Ca nous amène à un langage de plus haut niveau, que j'appelerai
P_high, qui inclut un algorithme de sous-typage qui produit des
obligations de preuve. L'algorithme de sous-typage réussit si les
préconditions qui ne sont pas résolues trivialement (par unification)
sont démontrées comme vraies par une procédure de décision donné par
le module de description du langage de programmation. Dans un premier
temps, une simple preuve par réflexivité en Coq doit être une bonne
procédure de décision. :-)

Est-ce que je suis assez clair?
> [...]

Hum, la deuxième solution semble être proche de l'idée décrite plus haut. Non?

On peut commencer par développer P_low, l'algorithme de diff et
s'attaquer à P_high un peu plus tard.
Oui, tiens moi au courant ... Ta situation n'est pas très confortable.
J'espère que tu pourras trouver un terrain d'entente avec Andrea et
Claudio ...

Je continue sur mes notes ...

A plus,
--
Yann

----------
From: Yann Régis-Gianas <<yann.regis-gianas@pps.jussieu.fr>
Date: 2010/2/16
To: Matthias Puech <<puech@cs.unibo.it>


Oui, c'était un exemple que j'avais en tête :-).

L'alpha-conversion commute bien avec beaucoup de patchs par exemple.

--
Yann Régis-Gianas


Thu Feb 11, 2010 14:00 PM


Discussion éclair, partant d'un exemple simple (λx.x → λf.λx. fx), sur les thèmes:
- maximisation du partage dans les patches et hash-consing
- adaptation des signatures de métathéorêmes


Le problème de départ est le suivant: Pour transformer D : (x:A ⊢ x:A)
en (x:A, f:AA ⊢ f x : A), on doit appliquer weak sur D d'un côté (on
obtient D₁ : (x:A, f:AA ⊢ x:A)), et créer de l'autre la dérivation D₂
: (x:A, f:AA ⊢ f:AA), pour finalement créer l'application, qui exige
deux environnements identiques. Question: les deux environnements
sont-ils: 
- structurellement identiques, ce qui oblige effectuer une
  vérification d'égalité structurelle au typage du patch?  
- ou identique au niveau de leur localité (représentés par la même
  variable dans le patch)? C'est plus contraignant pour les patches,
  puisque deux objets "structurellement égaux" ne pourront pas être
  utilisés indifféremment; par contre le typage est beaucoup moins
  coûteux, et on a une garantie du partage maximal.


La deuxième solution a été retenue parce qu'elle est plus élégante.
On en a tiré une leçon/question plus générale à propos du partage dans
le langage: Est-il possible de créer deux objets (types, dérivations,
termes), structurellement égaux? Oui, sans aucun doutes. Mais pour
chacune de ces situations, y a-t-il un moyen d'exprimer le patch
différemment pour que ces objets partagent la même variable? Sans
doutes (mais pas sûr). Si c'est le cas, alors pas besoin d'embarquer
une procédure d'unification (ou égalité syntaxique) dans le
typeur. Sinon, on peut imaginer permettre la comparaison, mais fournir
une procédure de hash-consing sur les patches qui maximise le partage.
Sous réserve que ça soit le cas, alors on adapte la théorie:


Le constructeur d'atome [deriv] n'est plus paramétré par un triplet
(env, terme, type) mais par trois variables (X,Y,Z). De cette façon on
garantit que les env, terme et type sont bien déclarés plus
haut. Maintenant, si un transformer a une [deriv] dans sa conclusion,
il faut pouvoir donner une valeur a (X,Y,Z) ses arguments. On change
donc le type des métathéorêmes, de ΠΣ en ΠΔΣ. Δ c'est une série de
let-in's qui bindent (du verber binder) les noms nécessaire à leur
définitions en terme de transformer. Par exemple:


weak :: Π (E:env) (t:term) (A:type) (D:deriv(E,t,A)) (x:var) (B:type). 
        Δ (E'=cons(E,x,B)). Σ (D':deriv(E',t,A))


Attention, les variables de Δ font moralement partie de la conclusion
d'un transformer, et on mémorise leur valeur implicitement.


(E',D') = weak(E,t,A,D,x,B)


doit être compris comme (expansé en):


E' = cons(E,x,B)
D' = weak(E,t,A,D,x,B,E')


De cette façon, on a récupéré E' dont on peut parler. On a proposé un
exemple pour lequel il est assez naturel de permettre des définitions
embarquant des transformers plus complexes que [cons]: la
transformation CPS. On a trois transformers:


cps_terms :: term -> term
a_a_trans :: type -> type
typed_cps :: Π (D : deriv(Γ,t,A)).
             Δ (t' = cps_term t) (A' = a_a_trans t).
             Σ (D' : deriv(Γ,t',A'))


Problème: Cette approche semble permettre un typage des patches en ne
connaissant que la signature de chaque transformer. Est-ce vraiment le
cas? En vérité je ne crois pas. Imaginons un transformer qui prend en
argument une dérivation d'une certaine forme (deriv(Γ;x:A, t, A→A) par
exemple). 


1/ ça ne rentre plus dans notre syntaxe! Faut-il des let-in entrelacés
dans les arguments aussi?


2/ si on lui passe le E' plus haut, pour vérifier le bon typage du
transformer, on va être obligé de calculer cons(E,x,B). On perd donc
notre propriété de non-execution des transformers. Doit-on autoriser
une forme lazy de calcul sur les transformers (dans l'exemple
ci-dessus, calculer juste assez pour pouvoir garantir que E' n'est pas
vide?)


A suivre…


Fri Feb  5, 2010  7:06 PM 


Aujourd'hui, nous avons discuté de :
1. Explicitation de points obscurs sur la syntaxe du langage de patch.
2. Amélioration de cette syntaxe de patch.
3. Quelques réflexions sur le système auto-appliqué.


Matthias a précisé quelques notions au sujet de la dénotation d'un
patch comme une transformation de programme. En fait, la sémantique du
langage de patch donne une place centrale au répositoire. La transformation
d'un programme peut être extraite du répositoire (c'est une sous-séquence
de définitions de dérivation menant du programme source au programme
cible). 


Matthias a proposé une amélioration de la syntaxe des patchs. Un
patch est une séquence de définitions de la forme :


  d11, ..., d1N = f d'11 ... d'1N'
  d21, ..., d2M = g d'21 ... d'2M'
  ...


On "atomise" ainsi les spécifications. Yann avait une erreur de
compréhension à ce sujet car il pensait qu'atomiser les spécifications
impliquait une atomisation des dérivations. Ce n'est pas le cas: par
exemple, si on appelle un méta-théorème [typecheck] alors on peut (on
doit) très bien atomisé les différentes composantes objets nécessaires
à la formulation du jugement tandis que la dérivation en elle-même
peut rester internalisée en Coq. 


Enfin, Yann a parlé un peu des idées de méta-théorème concernant le
système appliqué une fois et qui apparaitrait comme des patchs de
répositoire si on auto-applique. Parmi eux, on trouve la jointure, qui
ne pose a priori pas de problème. Un méta-théorème plus intéressant
consiste à exhiber les dépendances induites par les définitions du
répositoire. Si on a un méta-théorème qui transforme le répositoire
sans en modifier le sens (moralement, on ne change les connaissances
de ce répositoire) alors on doit avoir une relation d'ordre entre les
ensembles de dépendances. Typiquement, on peut devenir plus ou moins
précis mais pas contradictoire. Or, avoir des dépendances fines permet
de normaliser le répositoire et d'autoriser des transformations plus
locales donc distribuer plus facilement le développement. 


Pour finir, Matthias a vérifié qu'on s'intéressait plus volontier aux
méta-théorèmes de :
[forall X. Proprietes sur X => proprietes sur F(F(X)]
Ils vont faire mal à la tête mais ils sont définitivement les points
les plus intéressants de cette étude!


Prochaine étape : mettre au clair [F(STLC)]. Baby steps!


Fri Jan 22, 2010 6:45 PM


Aujourd'hui, la discussion a porté sur :
1. une explication du système décrit moralement dans meta.v
2. une réflexion sur la forme du langage de patch
3. l'expressivité de l'algèbre de types
4. des questionnements sur l'aspect logique/programmation.
5. l'interface à fournir au programmeur.


A propos de 1, pas grand chose à dire, si ce n'est que les
noms des concepts ont été très mal choisis. 


Au sujet de 2, Matthias a fait remarquer une distinction que je
n'avais pas saisi: quand on développe, c'est vraiment monotone. En
effet, on ne veut surtout pas perdre l'historique du développement. Le
langage de patchs est donc pure : il nomme des expressions de patchs
mais ne modifie pas les expressions existantes. L'interprétation d'une
série d'expressions de patchs nommés est claire : il s'agit finalement
juste de mémoiser les résultats de fonctions Coq.


C'est le niveau du dessus qui décrit des transformations sur des
répositoires (les expressions de patch nommées). C'est donc en
auto-appliquant le système qu'on voit apparaître la notion de 
développement (concurrent ou du moins, local vs global). Il faut
alors définir des méta-théorèmes sur le langage de patchs qui 
décrivent comment on peut ajuster, par exemple, les clients
d'une dérivation pour ne pas perdre la propriété d'être
bien formé pour un répositoire. Matthias a suggéré qu'on
se focalise sur le niveau "langage de patchs" avant de
s'intéresser au "langage de développement". Il a aussi
remarqué une ressemblance très forte avec Git. 


Au sujet de 3, Matthias a proposé qu'on autorise des 
types d'ordre supérieur pour donner un type aux récurseurs. 
C'est possible. On a ensuite essayé quelques exemples qui
nous ont à peu près convaincu de l'expressivité de la chose. 
Il faudra se comparer exactement aux types de LF. Cette
précision nous a permis de spécifier le type de module
permettant de décrire un langage objet et sa métathéorie. 
Ca a l'air ok. 


Au sujet de 4, Matthias était gêné par une redondance entre
les termes du lambda-calcul simplement typé et des dérivations
de preuve. Effectivement, quand on travaille dans un système
logique qui satisfait curry-howard, il n'y a pas de raison
de se donner une syntaxe pour les termes: ils sont extractibles. 
Au contraire, en programmation, les programmes ont une vie en
dehors des preuves de typage. Notre système est compatible
avec les deux approches. 


Au sujet de 5, il n'est pas évident de définir une 
interface pour l'utilisateur lambda, subsumant les 
pratiques habituelles d'édition textuelle. Un algorithme
de diff, construisant un patch à partir de deux termes
devra être fourni. Il sera aussi nécessaire de préciser
les langages de description de chemin pour accéder à une
dérivation précise, etc. A mon avis, cela s'éclairera plus tard. 
Pour l'instant, nous allons méta-programmer.


La prochaine étape est la réimportation de ces idées dans
le code Coq. 


Thu Jan 21, 2010  4:35 PM


Matthias a découvert les travaux d'une étudiante de Frank Pfenning,
très proche de ce que nous faisons. Son nom est Penny Anderson.


A lire.


Sun Jan 17, 2010  9:47 AM


Au sujet de la visualisation d'une dérivation à offrir au programmeur,
ce n'est pas évident. A première vue, je pensais qu'il suffisait de se
donner une (ou plusieurs) syntaxe(s) pour visualiser les jugements
mais en fait, ce n'est pas suffisant. En effet, éditer les jugements
permettrait de faire de la programmation, mais pas de la
méta-programmation.


Il faut vraiment se donner une syntaxe pour le langage des expressions
du langage de patchs, incluant une syntaxe pour les transformateurs
ainsi que l'ensemble des objets syntaxiques considérés (jugement,
atom, type, etc).


Si on veut définir le "diff", il se passe quelque chose de bizarre
puisque l'on calcule une différence syntaxique entre (deux visualisations
de) deux expressions du langage de patchs qui doit être interprétée
elle-même comme une expression du langage de patchs! Pour ce qui
est de changement syntaxique sur le terme du programme, je suis 
d'accord mais si on change une application de transformateur
en l'application d'un autre transformateur, on n'a pas de langage 
pour exprimer cette transformation! 


Peut-être que je me pose la mauvaise question: lorsque l'on modifie
un noeud correspondant à la modification d'un transformateur, on
est dans le langage calculatoire des patchs (le petit langage
impératif du début). Il se peut quand produisant non pas une 
expression du langage de patchs mais un programme du langage de
patchs, on puisse capturer correctement la différence entre 
deux expressions du langage de patchs. 


Sun Jan 17, 2010  9:38 AM


A propos du related work, il faudra se comparer à Twelf, Delphin et 
Cocinnelle. Pour ce dernier, c'est juste le nom de "semantic patch"
qui est surchargé mais ça n'a pas grand chose à voir, il me semble.


Sun Jan 17, 2010  9:37 AM


Quelque chose d'amusant serait de traiter des commentaires
structurés pouvant contenir du code, à mettre à jour aussi
à travers les patchs. D'une façon générale, on peut même 
interagir avec le programmeur pour que tout commentaire 
qui a été impacté par un patchs puisse être mis-à-jour
intéractivement.


Sun Jan 17, 2010  9:30 AM


En parlant des points de comparaison avec les systèmes de compilation
séparée, il faut noter que notre approche fonctionne uniquement 
dans un cadre open source. Pour traiter l'opacité des dérivations,
ça ne devrait pas être trop dur.


Sun Jan 17, 2010  9:24 AM


A propos des systèmes de modules, je pense qu'il suffirait de définir
un jugement de la forme "env |- decls => env" pour les traiter. Une
question intéressante serait de déterminer différents moyens de
visualiser l'utilisation de modules importés au sein d'un autre
module.


En effet, on peut écrire :
import M
val f x = ...


Pour représenter le méta-programme qui utilise la dérivation de module
M pour construire une nouvelle dérivation de module. Mais on peut
aussi vouloir visualiser dans le buffer, l'ensemble des déclarations
de M (ça évite d'avoir à regarder dans la documentation.).


Comment représenter un foncteur? Peut-être ainsi :


uses Msig
val f x = ...


qui signifie qu'on définit une dérivation de module qui peut-être
jointe à une autre qui remplit la signature Msig.


Comment représenter des contraintes entre les modules? Peut-être ainsi:


uses M1 : M1sig
uses M2 : M2sig
requires M1.t = M2.t


Il faut noter que cette façon de coder les modules les cantonnent à
être des objets de seconde classe. Une question ambitieuse serait de
définir un langage de méta-programmation qui intégrerait dynamiquement
ces constructions. On pourrait ainsi créer des dérivations
dynamiquement et donc charger des modules dynamiquement. Ca reste une
perspective sur du long terme. A mettre dans le Future work.


A propos maintenant d'inférence de type, j'avais des doutes sur la
possibilité de l'intégrer au système mais en fait, il n'y a pas de
raison : on peut très bien définir un jugement de la forme "env |- t
=> t' : tau où t est implicitement typé tandis que t' est
explicitement typé.



Sun Jan 17, 2010  1:15 AM


Un des points de comparaison avec Twelf, c'est tout
simplement que nous n'utilisons la syntaxe d'ordre
supérieure pour représenter les jugements mais
pas pour représenter la syntaxe objet. 


Je ne sais pas trop si c'est très profond comme
comparaison puisqu'on pourrait très bien faire 
de la syntaxe du premier ordre en Twelf ... qui
peut le plus, peut le moins. 


Sun Jan 17, 2010 12:19 AM


Quelques réflexions supplémentaires. 


Le type "dlist", qui sera renommé en "spec", correspond quasi
exactement aux types de LF (i.e. λπ). Plus généralement, l'inductif
"signature" semble correspondre à la définition de théorie LF. 


Le langage de patchs est donc un langage qui correspond à
un sous-ensemble de Delphin[1] (la version constructive et
fonctionnelle de Twelf). Il faudra se comparer à ce langage
et expliquer en quoi la restriction que nous considérons
est digne d'intérêt. D'ailleurs, est-ce qu'on ne pourrait
pas directement (meta)-programmer en Delphin?


La sémantique que j'ai en tête pour le langage de patchs
se décrit comme une transformation d'un "répositoire",
qui est toujours bien formé. Cette transformation
est décrite de façon impérative mais on pourrait très bien
se placer dans une monade et rester fonctionnel. D'ailleurs,
je me demande si une monade de transaction ne serait pas
pertinente pour modéliser la modification concurrente
de répositoire. 


Ensuite, il serait vraiment amusant et intéressant de
voir le langage de patchs et son système de type comme
une entrée possible à lui-même. Je ne sais pas encore
bien expliquer ce qu'on obtient mais à première vue,
c'est un langage de patchs sur des patchs, donc un
moyen de parler de l'historique d'un projet et de
le modifier. 


Enfin, il serait intéressant de voir en quoi le 
langage de patchs se compare aux systèmes de module
et aux systèmes de compilation incrémentale (compilation
séparée). Une question à se poser pourrait être : 
comment construire un ensemble de modules O'Caml
à partir d'un répositoire? Si on se débrouille bien
cette construction pourrait être incrémentale : on
ne génèrerait les modules correspondant uniquement
aux dérivations de typage mis-à-jour. 


Voilà, c'est tout pour aujourd'hui.


[1] http://www.cs.yale.edu/homes/delphin/delphin.htm


Sat Jan 16, 2010  7:21 PM 


Plus je réfléchis aux patchs sémantiques et plus je me dis que le bon
cadre de développement est effectivement Coq. 


Si on prend le point de vue très général qui consiste à voir
les dérivations de typage comme n'importe quel inductif dans
Type (de façon à en avoir une représentation enregistrable 
sur un support physique) et qu'on voit les patchs atomiques
comme des appels à des lemmes écrits en Coq (dont le
type est un transformateur), alors la difficulté de la 
conception d'un système de patchs consiste essentiellement
à la définition dans langage de programmation capturant
les transformations utiles au développement d'un programme
(qui a un aspect un peu plus "méta" que la programmation). 


Voici une proposition simple pour ce langage mais qui
pose déjà des problèmes intéressants. On voit
un programme dans ce langage comme un transformateur
de répositoire/mémoire associant des dérivations à des
emplacement. On a alors l'interprétation suivante 
des trois opérations standards: 


- ref e  : alloue un emplacement frais contenant
  la dérivation de typage ∇.
- ! e    : lit un emplacement.
- x := e : écrit dans un emplacement.


C'est la troisième opération qui est intéressante puisque, pour
préserver le fait que la mémoire peut être interprétée comme une
dérivation de typage correcte, il faut s'assurer que les utilisateurs
de 'x' sont toujours satisfaits. Pour cela, on peut produire des
sous-buts, à la manière des tactiques de Coq. On obtient une sorte de
jeu.  Pour traiter ces sous-buts automatiquement, on peut imaginer
comparer l'ancienne et la nouvelle version de "x" et déduire des
transformations canoniques, lorsqu'elle existe.


Par exemple, si on rajoute un nouveau constructeur dans un ADT, on
peut vouloir rajouter une branche avec un lancement d'exception sur
tous les patterns matchings de ce type. Si on rajoute un nouvel
argument à une fonction, tous les utilisateurs de cette fonction
peuvent être appliqué à une valeur par défaut. Il y a aussi cette
transformation de passage de "map" à "foldmap" (qui doit avoir une
définition catégorique):


let map f = function
| [] -> []
| x :: xs -> (f x) :: map f xs


en 


let fold f accu = function
| [] -> accu, []
| x :: xs ->
  let x', accu = f x accu in
  let accu, xs' = fold f accu xs in
  (accu, x' :: xs')


C'est le passage du type "a -> b" au type "a * c -> b * c"
dont parlait Paul-Andre l'autre fois. L'argument "c" 
suit le flot d'exécution. Quelle est la transformation
similaire qui préserve le tail-call?


On doit pouvoir programmer un interprète pour ce langage en Coq.


La mémoire Δ contiendrait des sommes dépendentes de la forme
Σ T: S. φ (T) où φ est une fonction qui transforme S,
la signature de l'algèbre des objets syntaxiques de 
la théorie (ça peut être un jugement, un environnement,
un terme, peu importe). Une procédure de décision sur
ces objets est du type D ≡ list T -> bool. 


Du coup, les transformateurs ont des types de la forme Θ ≡ list T *
list D * list T, qu'on peut facilement interpréter dans Type. De cette
façon, on élimine le problème d'extensibilité: il suffit de
dynamiquement intégrer un nouveau méta-théorème dans l'environnement
du langage de patchs pour que cela fonctionne. 


Le système de type du langage de patchs obtenu est simple. 


Fri Jan  8, 2010  3:51 PM 


Matthias a présenté son système à base de deux jugements : Δ | t, qui
signifie "je peux construire une dérivation t à partir de mes faits Δ"
et Δ | t → u, "je peux construire une dérivation pour u à partir de
la dérivation de t et de la base de faits Δ". 


Le système est joli mais on a remarqué une certaine redondance : il y
a par exemple une règle d'introduction de la partie gauche d'une
application pour le jugement qui a une flèche et une règle de typage
pour l'application pour le jugement "Δ | t". 


En fait, se focaliser sur une dérivation particulière introduit
une certaine asymétrie : on travaille sur une dérivation particulière
qui n'a rien de plus qu'une autre dérivation de la base Δ. Pourquoi
ne pas travailler directement sur plusieurs dérivations à la fois 
et se permettre d'en déduire plusieurs dérivations? On a donc
adapté le système avec cette idée en introduisant un unique 
jugement : Δ ⊧ Δ' où Δ représente les dérivations qu'on a déjà 
sous la main et Δ' les dérivations que l'on déduit. 


Les règles de typage sont des axiomes de ce système, ainsi que les
métathéorèmes (en particulier, les très utiles "lemmes d'inversion").
La théorie des patchs (paramétrée par le langage) a donc une mission
d'administration des dérivations de typage. Le langage des patchs
décrit des constructions de dérivations (à partir d'autres). A priori,
l'expressivité des constantes est peu limitée, en termes opérationnels
(on peut itérer sur une dérivation pour réécrire les environnements de
jugement). Par contre, l'expressivité du langage de patchs est limitée
puisque c'est un langage dont seule la sémantique statique nous
intéresse vraiment (On pourrait imaginer associer une sémantique
opérationnelle qui réaliserait réellement les constructions
de dérivation de typage de bas-niveau mais si on travail dans Coq,
on a la garantie qu'on ne fait rien de mal.)


Il devrait exister un système de types/une analyse statique
intéressant(e) pour le langage des patchs qui permettrait de donner un
sens à des transformations mais non pas sur des dérivations de typage
sur des patchs eux-mêmes. En effet, les transformations tels que
l'évolution d'un répositoire (qui est un patch) en une nouvelle
version de ce répositoire est un patch! On doit donc, d'une certaine
façon, pouvoir bootstrapper le système. 


Revenons à des choses plus concrètes. Matthias a remarqué que le
jugement "Δ ⊧ Δ'" gagne, certes, en symétrie mais on voit moins bien
ce sur quoi s'applique le patch. En particulier, lorsque les patchs
avec une unique conclusion, on pouvait les décrire à l'aide d'un terme
du langage objet (éventuellement étendu par plongement des
métathéorèmes comme des opérateurs constants). Il semble plus
difficile de jouer avec la syntaxe de cette façon dans le nouveau
système.


Quels sont les prochains objectifs? Je pense qu'il faut avoir ce que
l'on a dit en tête mais se focaliser sur un langage particulier (STLC
par exemple). Je ressens vraiment le besoin d'obtenir un prototype
(soit écrit en Caml ou extrait de Coq) et de pouvoir réellement
(méta-)programmer avec ce système. 


Mon Jan  4, 2010  3:22 PM [semantic patch]


Discussion avec Matthias à propos des patchs sémantiques. Il a
formalisé plusieurs systèmes (en Coq et en Agda) qui énumèrent des
transformations possibles sur des dérivations de typage (préservant la
bonne formation de celles-ci).


Le premier système contient des transformations sur la syntaxe basées
sur les règles dirigées par la syntaxe ainsi que des règles de passage
aux contextes.


Le système final contient des transformations avec mémoire,
c'est-à-dire permettant de nommer des dérivations (et les réutiliser
plus tard). Ce système subsume le précédent.


Ce système semble prometteur : en le codant en Coq et en définissant
une base de hints, Matthias a été capable de vérifier des théorèmes
admissibles automatiquement. Il a pensé rajouter des récurseurs (ou
des opérateurs similaires à ceux des langages de stratégies, comme
Stratego).


Matthias a aussi pu constater une certaine méthodologie pour déduire
automatiquement des règles de transformations à partir des règles
dirigées par la syntaxe. La notion de "méta-théorème" constructif
lui semble aussi de plus en plus naturelle.


Les prochaines étapes :
- vérifier que les bindeurs fonctionnent bien ;
- prendre en compte le contexte et la spécification (i.e. le jugement
 tout entier) ;
- valider un système permettant d'écrire les règles d'affaiblissement
 et de renforcement du contexte.




Fri Dec 18, 2009  3:39 PM


Matthias a posé la question de la vérification de l'application
d'un métathéorème. 


Par exemple, le renommage R[x -> y] s'applique bien sur le 
terme [let x = e1 in e2] mais pas sur le theme [e1].


Est-ce qu'on peut trouver un système de types intéressant pour
vérifier la bonne application de R ?


Yann répond : il suffit d'utiliser Coq mais, c'est un peu facile
comme réponse :). 


Mon Dec 14, 2009  6:29 PM


Une transformation non triviale qui apparait souvent :


let rec f x = 
  ... g y ...
and g y = 
  ... f x ...


devient :


let rec f env x = 
 ... g env y ...
and g env y = 
 ... f env x ...


(ou bien interaction a l'aide d'un '?')

