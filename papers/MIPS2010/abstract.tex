\documentclass{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{mathpartir}
\usepackage{color}
\usepackage[backref=page,colorlinks=true]{hyperref}
\usepackage{amsmath, amstext, amsthm, amsfonts}
\usepackage{stmaryrd}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage[all,cmtip]{xy}
\usepackage[protrusion=true,expansion=true]{microtype}

\definecolor{bwgreen}{rgb}{0.183,1,0.5}
\definecolor{bwmagenta}{rgb}{1,0.169,0.909}
\definecolor{bwblue}{rgb}{0.317,0.161,1}
\hypersetup{
  linkcolor=blue,
  citecolor=blue
}

\newcommand{\sort}{\textsf{s}}
\newcommand{\gor}{\ |\ }
\newcommand{\gdecl}[2]{{#1}\ &::=\ {#2}}
\newcommand{\subst}[2]{\{#1/#2\}}
\newcommand{\nat}{\mathbb N}

% Commandes à désactiver avant la soumission.
% Remarques sur la structure.
\newcommand{\remplan}[1]{\noindent\textcolor{bwblue}{$\triangleright$ \textbf{#1}}}
% Remarques sur le contenu.
\newcommand{\remtext}[1]{\textcolor{bwgreen}{$\triangleright$ \textsl{#1}}}

\renewcommand{\remplan}[1]{}
\renewcommand{\remtext}[1]{}

\title{Towards typed repositories of proofs}

\author{Matthias Puech \and Yann Régis-Gianas}
\date{}

\begin{document}

\maketitle

\paragraph{Motivations}

\remplan{Motivations générales}

Practical efforts in the formalization of mathematical results have
naturally led to the questions of how to manage large repositories of
proofs, i.e. what is the \emph{daily workflow} of users of a proof
assistant. How does one elaborate a formal proof? What kind of \emph{a
  posteriori} modification is he prone to doing? What do these
modifications imply on the validity of the whole edifice or, the other
way around, how do one rely on existing work to build up new results?
Many of these questions remain largely unanswered, but the tendency
seems to be to adapt existing methods coming from software
development, as illustrated for example by the introduction of
modules, file-based scripts and separate compilation in proof
assistants like \textsf{Coq} ({\cite{CoqDocWeb}}) or
\textsf{Matita}~{\cite{Asperti06userinteraction}}, the use of
dependency management tools (\textsf{Make}) or version control system
(\textsf{GIT}, \textsf{Subversion}) to build and manage versions of a
project. Both for the development of proofs or programs, these tools
attempt to cope with the fact that most of a mathematician or
programmer's time is actually spent \emph{editing}, not
\emph{writing}.

We believe that these tools are not adapted for the new, demanding
requirements of proof developments. Indeed, whereas compilation of a
program is usually fast enough for the programmer to rely on the usual
interaction loop ((edit; compile)*; commit)*, the operation of proof
checking is usually too expensive computationally to mimic this
workflow. But even besides the time factor, this ``traditional'' way
of formalizing mathematics hinders the process of mathematical
discovery process: once a concept contained in a file is compiled, it
is considered frozen and any changes to it require the recompilation
of the whole file; the linearity of the development also gives no room
for alternate, inequivalent definitions. This fact has nonetheless
been shown to be of crucial to the mathematical discovery
process~\cite{lakatos1964proofs}, and we believe that they should be
taken into account in the realization of mathematical assistants.

In fact, although dedicated tools exist to formalize the description
of languages and their metatheory (e.g. \textsf{Twelf},
{\cite{pfenning1999system}}), and substantial formalizations have been
undertaken~[]\cite{lee07}, we still use legacy tools
based on text representation to manage our developments. The general
goal exposed here is to replace this tool chain and make it
language-aware, both on a syntactic side (AST) and on a semantic side
(typing).

\remplan{Sujet du papier : on focalise sur un problème bien précis}

\remtext{Fri Jun 18, 2010 10:00 AM.
  Un peu de liant. Pour arriver à notre but, il y a une
  question importante à résoudre: si on te donne une méta-théorie
  formalisée, es-tu capable de décrire l'historique d'un développement
  dans cette théorie? Cela induit deux questions : dans quel langage
  et est-ce que ce langage est utilisable (parce qu'a priori, ce n'est
  pas gagné car le développeur travaille en mode ``instantané'' et ne
  veut pas se trimballer ses casseroles tout le temps\ldots).}

We propose to discuss a small part of these questions, namely the
enhancement and adaptation of version control paradigms to the
management of mathematical repositories, to witness with more
precision the \emph{impact of changes}. In proof assistants based on
the propositions-as-types paradigm, Type Theory offers a powerful tool
to witness the effect of changes finely: types, and we base our work
on these.

We will describe some of the possible directions to develop a tool to
analyze the impact of change through types. It involves at its core a
typed description language for repositories, and is strongly related
to incremental type-checking: only differences between versions are
type-checked and not the entire development. In the first iteration of
this project, we focus on a static, or data-driven model for
repositories inspired by the repository model of \textsf{GIT}.

\paragraph{A core language to describe typed repositories}

The kernel of our system is a type-checker algorithm for a typed
meta-language. In this language, we will declare both the syntax of
the object (proof-)language and its typing rules, and define pieces of
syntax (our proofs) and their derivations. Describing transformations
among syntax objects is done by sharing common subterms or
subderivations.

Representing syntax and logics is nicely done in a \emph{logical
  framework} like LF: both the syntactic
elements and the typing derivations can sit in the same tree
structure, and both can be rechecked at the same time, thanks to
dependent types. For the purpose of incremental type checking though,
our needs are a little bit different: first, we need to record, that
is to name all intermediate values of our developments, so as to be
able to address and reuse them multiple times. Secondly, we need to
make sure that those intermediate values (sub-terms) are not recorded
twice, so as to not type-check them twice: we are looking to represent
syntactic and typing objects as a DAG rather than a tree.

Our system will have these properties w.r.t. LF:
\begin{itemize}
\item In this first iteration of the project, we do not need
  computations to take place within our DAGs. Our syntax will then be
  restricted to product types and applications;
\item Every term should be a flat application of variables, so that we
  don't introduce compound terms without naming them;
\item Finally we need a way to record intermediate definitions: we
  introduce a new kind of binder, the equality binder. We maintain the
  invariant that not two same definition can sit in the context while
  typing.
\end{itemize}

We describe the state of a repository at a given moment by a type in
the following syntax. Well-typed types in the repository meta-language
guarantee that it contains only well-typed proofs in the object
language.
\begin{align*}
 \gdecl{a}{x \gor a\ x } \\
 \gdecl{t}{a \gor \sort \gor (x:t)\cdot t \gor (x=a:t)\cdot t} \\
 \gdecl{\Gamma}{\cdot \gor \Gamma[x:t] \gor \Gamma[x=a:t]}
\end{align*}

% \label{kernel}

% The kernel of our system is a type-checker algorithm for a typed
% meta-language. In this language, we will declare both the syntax of
% the object (proof-)language and its typing rules, and define pieces of
% syntax (our proofs) and their derivations. Describing transformations
% among syntax objects is done by sharing common subterms or
% subderivations.

% \subsection{How to describe a development?}

% Representing syntax and logics is nicely done in a \emph{logical
%   framework} like LF (\cite{harper1993framework}): both the syntactic
% elements and the typing derivations can sit in the same tree
% structure, and both can be rechecked at the same time, thanks to
% dependent types. For the purpose of incremental type checking though,
% our needs are a little bit different: first, we need to record, that
% is to name all intermediate values of our developments, so as to be
% able to address and reuse them multiple times. Secondly, we need to
% make sure that those intermediate values (sub-terms) are not recorded
% twice, so as to not type-check them twice: we are looking to represent
% syntactic and typing objects as a DAG rather than a tree.

% Our system will have these properties w.r.t. LF:
% \begin{itemize}
% \item In this first iteration of the project, we do not need
%   computations to take place within our DAGs. Our syntax will then be
%   restricted to product types and applications;
% \item Every term should be a flat application of variables, so that we
%   don't introduce compound terms without naming them;
% \item Finally we need a way to record intermediate definitions: we
%   introduce a new kind of binder, the equality binder. We maintain the
%   invariant that not two same definition can sit in the context while
%   typing.
% \end{itemize}

% \subsection{Formalization}

% We describe the state of a repository at a given moment by a type in
% the following syntax. Well-typed types in the repository meta-language
% guarantee that it contains only well-typed proofs in the object
% language.

% \paragraph{Syntax} There are three syntactic categories: applicative
% terms $a$, types $t$ and environments $\Gamma$.

% \begin{align*}
%  \gdecl{a}{x \gor a\ x } \\
%  \gdecl{t}{a \gor \sort \gor (x:t)\cdot t \gor (x=a:t)\cdot t} \\
%  \gdecl{\Gamma}{\cdot \gor \Gamma[x:t] \gor \Gamma[x=a:t]}
% \end{align*}

% Following the standard syntax, we denote by $(x:t)\cdot u$ the
% dependent product (elsewhere written with a $\Pi$). The new construct
% $(x=a:t)\cdot u$ is a definition binder, whose value is $a$. We write
% as usual $(x_1:t_1)(x_2:t_2)\ldots(x_n:t_n)\cdot t$ for
% $(x_1:t_1)\cdot(x_2:t_2)\cdot\ldots\cdot(x_n:t_n)\cdot t$, $t \to u$
% for $(x:t)\cdot u$ where $x\notin u$, and $(x\ y : t)\cdot u$ for
% $(x:t)(y:t)\cdot u$. The lookup in the environment is written either
% $\Gamma(x):t$, denoting ``$\Gamma$ contains a declaration $[x:t]$
% \emph{or} a definition $[x=a:t]$'', or $\Gamma(x)=a:t$ denoting
% ``$\Gamma$ contains a definition $[x=a:t]$''. Capture-avoiding
% renaming of variables in a term $t$ is written $t\subst{x}{y}$ for
% ``replace all occurences of $x$ by $y$'' and is defined the usual way.

% \paragraph{Typing} Typing is parameterized by:
% \begin{itemize}
%   \item$\mathcal S$ a set of sorts \sort,
%   \item$\mathcal A \in \mathcal S^2$ the axioms of the system,
%   \item$\mathcal R \in \mathcal S^3$ the allowed products.
% \end{itemize}
% It relies on two distinct judgments:
% \begin{itemize}
% \item $\Gamma\vdash a : t$ ($a$ has type $t$ in $\Gamma$),
% \item $\Gamma\vdash t : \sort$ (the repository $t$ is well-typed of
%   sort $s$ in $\Gamma$).
% \end{itemize}
% The rules are:
% \begin{mathpar}
%   \infer[Axiom]{
%   }{\Gamma\vdash\sort_1:\sort_2}
%   \quad(\sort_1,\sort_2)\in\mathcal A
%   \and
%   \infer[Init]{
%     \Gamma(x):t
%   }{\Gamma\vdash x:t}
%   \and
%   \infer[Prod]{\Gamma\vdash t:\sort_1 \and 
%     \Gamma[x:t]\vdash u:\sort_2
%   }{\Gamma\vdash(x:t)\cdot u : \sort_3}
%   \quad(\sort_1,\sort_2,\sort_3)\in\mathcal R
%   \and
%   \infer[Let1]{\Gamma\vdash a:t \and
%     \Gamma\vdash t:\sort_1 \and
%     \Gamma[x=a:t]\vdash u:\sort_2
%   }{\Gamma\vdash(x=a:t)\cdot u : \sort_3} 
%   \quad (x=a:t)\notin\Gamma
%   \and
%   \infer[Let2]{\Gamma\vdash t:\sort_1 \and
%     \Gamma\vdash u:\sort_2
%   }{\Gamma\vdash(x=a:t)\cdot u : \sort_3} 
%   \quad(x=a:t)\in\Gamma
%   \and
%   \infer[App]{
%     \Gamma\vdash a:(y:t)\cdot u \and \Gamma(x):t
%     }{\Gamma\vdash a\ x : u\subst{x}{y}}
%   \and
%   \infer[Skip1]{
%     \Gamma\vdash a:(y=b:t)\cdot u \and \Gamma(x)=b:t
%     }{\Gamma\vdash a : u\subst{x}{y}} \and
%   \infer[Skip2]{
%     \Gamma[x=b:t]\vdash a:(y=b:t)\cdot u
%     }{\Gamma\vdash a : u\subst{x}{y}}
% \end{mathpar}

% The first four rules are standard. As application is only performed on
% variables, the \textsc{App} rule looks up directly in the
% environment. Rules \textsc{Let1} and \textsc{Skip1} show the
% verification and the use of the equality binder in case the definition
% is not already known. Rule \textsc{Let2} and \textsc{Skip2} allow
% incremental typing in our system: if the definition was already known
% in the environment, we do not need to recheck it. If $\cdot\vdash
% t:\sort$, then the repository $t$ is well-typed.

% \paragraph{Example} 

% Here is a well-typed repository describing the simply-typed
% $\lambda$-calculus and constructing two $\lambda$-terms sharing some
% sub-derivations: 
% {\small
% \begin{align}&
% (\nat : *).
% (o : \nat).
% (s : \nat \to \nat).
% (type : *).
% (base : type).
% (arr : type \to type \to type). \\&
% (env : *).
% (nil : env).
% (cons : type \to env \to env). \\&
% (term : *).
% (var : \nat \to term).
% (lam : term \to term).
% (app : term \to term \to term). \\&
% (lookup : env \to \nat \to type \to *).
% (lookup_o : (e : env).(a : type).(f = cons\ a\ e : env). lookup\ e\ o\ a). \\&
% (lookup_s : (e : env).(a b : type).(n : \nat).(sn = s\ n : \nat).(f =
% cons\ a\ e : env). lookup\ e\ n\ b \to lookup\ f\ sn\ b). \\&
% (of : env \to term \to type \to *). \\&
% (of_v : (e : env).(a : type).(n : \nat).(v = var\ n : term).
%           lookup\ e\ n\ a \to of\ e\ v\ a). \\&
% (of_l : (e : env).(t : term).(a\ b: type).(c = arr\ a\ b : type).
%           (u = lam\ t : term).(f = cons\ a\ e : env).
%           of\ f\ t\ b \to of\ e\ u\ c). \\&
% (of_a : (e : env).(t\ u: term).(a\ b: type).
%           (c = arr\ a\ b : type).(v = app\ t\ u : term).
%           of\ e\ t\ c \to of\ e\ u\ a \to of\ e\ v\ b). \\&
% (1 = s\ o : \nat).(v_0 = var\ o : term).(v_1 = var\ 1 : term).
% (t_3 = lam\ v_0 : term).(ch_0 = lam\ t_3 : term).\\&
% (t_1 = app\ v_0\ v_1 : term).(t_2 = lam\ t_1 : term).
% (t_4 = lam\ t_2 : term).
% (bb = arr\ base\ base : type). \\&
% (e_0 = cons\ bb\ nil : env).
% (e_1 = cons\ base\ e0 : env).
% (d_0 = lookup_o\ e_1\ base : lookup\ e_1\ o\ base). \\& %        (* a→a, a ⊢ 0 : a *)
% (d_1 = of_v\ e_1\ base\ o\ d0 : of\ e_1\ v_0\ base). %	   (* a→a, a ⊢ 0 : a *)
% (d_2 = of_l\ e_0\ v_0\ base\ base d_1 : of\ e_0\ t_3\ bb). \\& %   (* a→a ⊢ λ0 : a → a *)
% (bbb = arr\ bb\ bb : type).
% (d_3 = of_l\ nil\ t_3\ bb\ bb\ d_2 : of\ nil\ ch_0\ bbb). \\& %  (* ⊢ λλ0 : (a→a)→(a→a) *)
% (g_1 = lookup_o\ e_0\ bb : lookup\ e_0\ o\ bb).
% (g_2 = lookup_s\ e_0\ base\ bb\ o\ g_1 : lookup\ e_1\ 1\ bb). \\&
% (g_3 = of_v\ e_1\ bb\ 1\ g_2 : of\ e_1\ v_1\ bb).	          % (* a→a, a ⊢ 1 : a→a *)
% (t_5 = app\ v_1\ v0 : term). \\&
% (g_4 = of_a\ e_1\ v_1\ v_0\ base\ base\ g_3\ d_1 : of\ e1\ t5\ base). % (* ... ⊢ 1 0 : a *)
% (t_6 = lam\ t_5 : term). \\&
% (g_5 = of_l\ e_0\ t_5\ base\ base\ g_4 : of\ e_0\ t_6\ bb).  %  (* a→a ⊢ λ(1 0) : a→a *)
% (t_7 = lam\ t_6 : term). \\&
% (g_6 = of_l\ nil\ t_6\ bb\ bb\ g_5 : of\ nil\ t_7\ bbb). *	 %  (* ⊢ λλ(1 0) : (a→a)→a→a  *)
% \end{align}
% }


\paragraph{Related work}
\label{rw}

The \textsf{Twelf} project (\cite{pfenning1999system}) is an
implementation of the Logical Framework (LF,
\cite{harper1993framework}). It was used in \cite{anderson1993program}
to devise transformations of proofs in order to extract efficient
programs.

The problems of managing a formal mathematical library have been dealt
with in various proof assistant and mathematical repositories. The
HELM project (\cite{asperti2000content}, \cite{asperti2006content})
was an attempt to create a large library of mathematics, importing
\textsf{Coq}'s developments into a searchable and browsable database.
Most ideas from this project were imported into the \textsf{Matita}
proof assistants (\cite{asperti2007hop}), especially a mechanism of
\emph{invalidation and regeneration} to ensure the global consistency
of its library w.r.t changes, with granularity the whole definitions
or proofs and their dependencies. The MBase project
(\cite{kohlhase2001mbase}) attempts at creating a web-based,
distributed mathematical knowledge database putting forward the idea
of \emph{development graph} (\cite{hutter2000management},
\cite{autexier2000towards}) to manage changes in the database,
allowing semantic-based retrieval and object-level dependency
management.

This idea, generalized over structured, semi-formal documents gave
birth to \texttt{\it locutor} (\cite{muller2008fine}), a fine-grained
extension of the \texttt{svn} version control system for XML
documents, embedding ontology-driven, user-defined semantic knowledge
which allows to go across the filesystem border. It embeds a
\emph{diff} algorithm, operating on the source text modulo some
equality theory to quotient the syntax. On the same line of work, we
should mention the \emph{Coccinelle} tool
(\cite{padioleau2008documenting}). It is an evolution over textual
patches, specialized on the C language, allowing more flexibility in
the matching process, and was developed to deal with the problem of
\emph{collateral evolutions} in the Linux kernel. It embeds a
declarative language for matching and transforming C source code,
operating on text modulo defined isomorphisms.

Our approach to the ``impact of changes'' problem seems novel on
several aspects: first, it applies uniformly on proofs and programming
languages by virtue of the Curry-Howard isomorphism, and because we
operate at the AST level. Secondly, by taking \emph{types} as a
witnesses for the evolution of a development, we refine the usual,
dependency-based approach for a finer granularity.

% \section{Further work}
% \label{fw}

% \remtext{Extension vers une version constructiviste?}

% \remtext{Parler du bootstraping?}

% \section{Conclusion}

%% In the light of the propositions-as-types paradigms where
%% proof-checking boils down to type-checking, we abstract from whether
%% we are talking about proofs or programs, so that our development could
%% eventually also be used in traditional software development.

%% \section{Methodologies}

%% We propose here to devise a system for \emph{semantic repositories}. It
%% % TODO changer le nom?
%% substitutes the idea of textual transformation by:
%% \begin{itemize}
%% \item First preferring an abstract syntax tree representation instead
%%   of plain text;
%% \item Secondly embedding semantic, or typing data into the
%%   transformations, in order to be able to reason on them.
%% \item Finally, perform type-checking in an incremental way, that is
%%   type only the syntactic difference and reuse derivation for the
%%   already known subprograms.
%% \end{itemize}

%% We want to design a \emph{language of repositories}, able to represent
%% % TODO changer le nom?
%% these transformations and their semantic properties, thus capturing
%% local syntactic changes in programs (the abstract syntax tree), as
%% well as their global effect on the whole project (the typing
%% derivations). We can see this goal as a refinement of the former idea
%% of ``dependency''.

\bibliographystyle{plainnat}

\bibliography{english}

\end{document}
