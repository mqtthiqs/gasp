\documentclass{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{mathpartir}
\usepackage{color}
\usepackage[backref=page,colorlinks=true]{hyperref}
\usepackage{amsmath, amstext, amsthm, amsfonts}
\usepackage{stmaryrd}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage[all,cmtip]{xy}
\usepackage[protrusion=true,expansion=true]{microtype}

\definecolor{bwgreen}{rgb}{0.183,1,0.5}
\definecolor{bwmagenta}{rgb}{1,0.169,0.909}
\definecolor{bwblue}{rgb}{0.317,0.161,1}
\hypersetup{
  linkcolor=blue,
  citecolor=blue
}

\newcommand{\sort}{\textsf{s}}
\newcommand{\gor}{\ |\ }
\newcommand{\gdecl}[2]{{#1}\ &::=\ {#2}}
\newcommand{\subst}[2]{\{#1/#2\}}

% Commandes à désactiver avant la soumission.
% Remarques sur la structure.
\newcommand{\remplan}[1]{\noindent\textcolor{bwblue}{$\triangleright$ \textbf{#1}}}
% Remarques sur le contenu.
\newcommand{\remtext}[1]{\textcolor{bwgreen}{$\triangleright$ \textsl{#1}}}

%\newcommand{\remplan}[1]{}
%\newcommand{\remtext}[1]{}

\title{Towards Semantic Patches}
  % pas sûr que le nom soit encore adapté

\author{Matthias Puech, Yann Régis-Gianas}
\date{}

\begin{document}

\maketitle

\section{Motivations}

\remplan{Motivations générales}

Practical efforts in the formalization of mathematical results have
naturally led to the questions of how to manage large repositories of
proofs, i.e. what is the \emph{daily workflow} of users of a proof
assistant. How does one elaborate a formal proof? What kind of \emph{a
  posteriori} modification is he prone to doing? What do these
modifications imply on the validity of the whole edifice or, the other
way around, how do one rely on existing work to build up new results?
Many of these questions remain largely unanswered, but the tendency
seems to be to adapt existing methods coming from software
development, as illustrated for example by the introduction of
modules, file-based scripts and separate compilation in proof
assistants like \textsf{Coq}~{\cite{CoqDocWeb}} or
\textsf{Matita}~{\cite{Asperti06userinteraction}}, the use of
dependency management tools (\textsf{Make}) or version control system
(\textsf{GIT}, \textsf{Subversion}) to build and manage versions of a
project. Both for the development of proofs or programs, these tools
attempt to cope with the fact that most of a mathematician or
programmer's time is actually spent \emph{editing}, not
\emph{writing}.

We believe that these tools are not adapted for the new, demanding
requirements of proof developments. Indeed, whereas compilation of a
program is usually fast enough for the programmer to rely on the usual
interaction loop ((edit; compile)*; commit)*, the operation of proof
checking is usually too expensive computationally to mimic this
workflow. But even besides the time factor, this ``traditional'' way
of formalizing mathematics hinders the process of mathematical
discovery process: once a concept contained in a file is compiled, it
is considered frozen and any changes to it require the recompilation
of the whole file; the linearity of the development also gives no room
for alternate, inequivalent definitions. This fact has nonetheless
been shown to be of crucial to the mathematical discovery
process~\cite{lakatos1964proofs}, and we believe that they should be
taken into account in the realization of mathematical assistants.

In fact, although dedicated tools exist to formalize the description
of languages and their metatheory (e.g. \textsf{Twelf},
{\cite{pfenning1999system}}), and substantial formalizations have been
undertaken \remtext{TODO citer, mais quoi?}, we still use legacy tools
based on text representation to manage our developments. The general
goal exposed here is to replace this tool chain and make it
language-aware, both on a syntactic side (AST) and on a semantic side
(typing).

\remplan{Sujet du papier : on focalise sur un problème bien précis}

\remtext{Fri Jun 18, 2010 10:00 AM.
  Un peu de liant. Pour arriver à notre but, il y a une
  question importante à résoudre: si on te donne une méta-théorie
  formalisée, es-tu capable de décrire l'historique d'un développement
  dans cette théorie? Cela induit deux questions : dans quel langage
  et est-ce que ce langage est utilisable (parce qu'a priori, ce n'est
  pas gagné car le développeur travaille en mode ``instantané'' et ne
  veut pas se trimballer ses casseroles tout le temps\ldots).}

We propose to discuss a small part of these questions, namely the
enhancement and adaptation of version control paradigms to the
management of mathematical repositories, to witness with more
precision the \emph{impact of changes}. In proof assistants based on
the propositions-as-types paradigm, Type Theory offers a powerful tool
to witness the effect of changes finely: types, and we base our work
on these.

We will describe here some of the possible directions to develop a
tool to analyze the impact of change through types. It involves at its
core a typed description language for repositories -- described in
\S\ref{kernel} -- driven by a user interface that we sketch in
\S\ref{devel}. In the first iteration of this project, we focus on a
static, or data-driven model for repositories; in \S\ref{rw}, we
quickly review related approaches, and hint a possible generalization
in \S\ref{fw}.

\section{A core language to describe typed repositories}
\label{kernel}

The kernel of our system is a type-checker algorithm for a typed
meta-language. In this language, we will declare both the syntax of
the object (proof-)language and its typing rules, and define pieces of
syntax (our proofs) and their derivations. Describing transformations
among syntax objects is done by sharing common subterms or
subderivations.

\subsection{How to describe a development?}

Representing syntax and logics is nicely done in a \emph{logical
  framework} like LF (\cite{harper1993framework}): both the syntactic
elements and the typing derivations can sit in the same tree
structure, and both can be rechecked at the same time, thanks to
dependent types. For the purpose of incremental type checking though,
our needs are a little bit different: first, we need to record, that
is to name all intermediate values of our developments, so as to be
able to address and reuse them multiple times. Secondly, we need to
make sure that those intermediate values (sub-terms) are not recorded
twice, so as to not type-check them twice: we are looking to represent
syntactic and typing objects as a DAG rather than a tree.

Our system will have these properties \emph{wrt.} LF:
\begin{itemize}
\item In this first iteration of the project, we do not need
  computations to take place within our DAGs. Our syntax will then be
  restricted to product types and applications;
\item Every term should be a flat application of variables, so that we
  don't introduce compound terms without naming them;
\item Finally we need a way to record intermediate definitions: we
  introduce a new kind of binder, the equality binder. We maintain the
  invariant that not two same definition can sit in the context while
  typing.
\end{itemize}

\subsection{Formalization}

We describe the state of a repository at a given moment by a type in
the following syntax. Well-typed types in the repository meta-language
guarantee that it contains only well-typed proofs in the object
language.

\paragraph{Syntax} There are three syntactic categories: applicative
terms $a$, types $t$ and environments $\Gamma$.

\begin{align*}
 \gdecl{a}{x \gor a\ x } \\
 \gdecl{t}{a \gor \sort \gor (x:t)\cdot t \gor (x=a:t)\cdot t} \\
 \gdecl{\Gamma}{\cdot \gor \Gamma[x:t] \gor \Gamma[x=a:t]}
\end{align*}

Following the standard syntax, we denote by $(x:t)\cdot u$ the
dependent product (elsewhere written with a $\Pi$). The new construct
$(x=a:t)\cdot u$ is a definition binder, whose value is $a$. We write
as usual $(x_1:t_1)(x_2:t_2)\ldots(x_n:t_n)\cdot t$ for
$(x_1:t_1)\cdot(x_2:t_2)\cdot\ldots\cdot(x_n:t_n)\cdot t$, $t \to u$
for $(x:t)\cdot u$ where $x\notin u$, and $(x\ y : t)\cdot u$ for
$(x:t)(y:t)\cdot u$. The lookup in the environment is written either
$\Gamma(x):t$, denoting ``$\Gamma$ contains a declaration $[x:t]$
\emph{or} a definition $[x=a:t]$'', or $\Gamma(x)=a:t$ denoting
``$\Gamma$ contains a definition $[x=a:t]$''. Capture-avoiding
renaming of variables in a term $t$ is written $t\subst{x}{y}$ for
``replace all occurences of $x$ by $y$'' and is defined the usual way.

\paragraph{Typing} Typing is parameterized by:
\begin{itemize}
  \item$\mathcal S$ a set of sorts \sort,
  \item$\mathcal A \in \mathcal S^2$ the axioms of the system,
  \item$\mathcal R \in \mathcal S^3$ the allowed products.
\end{itemize}
It relies on two distinct judgments:
\begin{itemize}
\item $\Gamma\vdash a : t$ ($a$ has type $t$ in $\Gamma$),
\item $\Gamma\vdash t : \sort$ (the repository $t$ is well-typed of
  sort $s$ in $\Gamma$).
\end{itemize}
The rules are:
\begin{mathpar}
  \infer[Axiom]{
  }{\Gamma\vdash\sort_1:\sort_2}
  \quad(\sort_1,\sort_2)\in\mathcal A
  \and
  \infer[Init]{
    \Gamma(x):t
  }{\Gamma\vdash x:t}
  \and
  \infer[Prod]{\Gamma\vdash t:\sort_1 \and 
    \Gamma[x:t]\vdash u:\sort_2
  }{\Gamma\vdash(x:t)\cdot u : \sort_3}
  \quad(\sort_1,\sort_2,\sort_3)\in\mathcal R
  \and
  \infer[Let1]{\Gamma\vdash a:t \and
    \Gamma\vdash t:\sort_1 \and
    \Gamma[x=a:t]\vdash u:\sort_2
  }{\Gamma\vdash(x=a:t)\cdot u : \sort_3} 
  \quad (x=a:t)\notin\Gamma
  \and
  \infer[Let2]{\Gamma\vdash t:\sort_1 \and
    \Gamma\vdash u:\sort_2
  }{\Gamma\vdash(x=a:t)\cdot u : \sort_3} 
  \quad(x=a:t)\in\Gamma
  \and
  \infer[App]{
    \Gamma\vdash a:(y:t)\cdot u \and \Gamma(x):t
    }{\Gamma\vdash a\ x : u\subst{x}{y}}
  \and
  \infer[Skip1]{
    \Gamma\vdash a:(y=b:t)\cdot u \and \Gamma(x)=b:t
    }{\Gamma\vdash a : u\subst{x}{y}} \and
  \infer[Skip2]{
    \Gamma[x=b:t]\vdash a:(y=b:t)\cdot u
    }{\Gamma\vdash a : u\subst{x}{y}}
\end{mathpar}

The first four rules are standard. As application is only performed on
variables, the \textsc{App} rule looks up directly in the
environment. Rules \textsc{Let1} and \textsc{Skip1} show the
verification and the use of the equality binder in case the definition
is not already known. Rule \textsc{Let2} and \textsc{Skip2} allow
incremental typing in our system: if the definition was already known
in the environment, we do not need to recheck it. If $\cdot\vdash
t:\sort$, then the repository $t$ is well-typed.

\section{Development through semantic patches}
\label{devel}

\remplan{Fri Jun 18, 2010 10:50 AM. L'objectif ici, c'est de décrire
  une (possible) architecture pour rendre utilisable le langage 
  décrit dans la section précédente. En gros, on énumère les 
  opérations qui interviennent dans l'utilisation et on les décrit
  informellement à l'aide d'un exemple.}

The kernel language described in the previous section is clearly not
adapted to human interaction, for several reasons~: 
\begin{itemize}

\item Every single object is flattened, which makes unobvious the
  structure of a program or a proof. 

\item A term of this language denotes the entire history of a
  development. Most of the time, the user is more interested in a
  instantaneous snapshot of his object of study.

\item Even though we were able to provide a snapshot of a particular
  state of the development, the repository represents this state 
  using proof terms, which contain too many syntactic details from the
  user point of view and are not suitable for edition.  

\end{itemize}

\remtext{Fri Jun 18, 2010 10:52 AM. Par contre, je ne sais pas quel
  exemple utilisé. Le STLC? Est-ce que ça fait ``assez'' système
  logique pour ce workshop?}

\subsection{A layered architecture}

\remtext{Fri Jun 18, 2010  1:32 PM. Ici, on décrit la figure en suivant
  le modèle de flot du système. On décrit le rôle des fonctions, 
les problèmes posés.}

% À mettre dans une figure. 
\begin{figure}
\begin{center}
% FIXME: il y a un problème avec la flèche double phi/psi.
\xymatrix{
\textrm{History of metatheory objects}
&R 
\ar@{->}[d]_{\phi}
& \Delta \ar@{->}[r]^{\theta(R)} & R' \\
\textrm{Metatheory objects} 
&t \ar@{->}[d]^{\pi}
\ar@{->}[u]_{\psi}
& t' \ar@{->}[u]_{\nabla(t)}\\
\textrm{Concrete views on metatheory objects} 
&w \ar@{.>}[r] & w' \ar@{->}[u]_{\sigma(t, w)}
}
\end{center}
\label{fig:architecture}
\caption{A layered architecture for a semantic patch framework.}
\end{figure}

With the previous remarks in mind, we designed a generic framework of semantic
patches that can be described using three layers which are summarized in 
Figure~\ref{fig:architecture}. We now describe the role of each layer.

The most internal layer focused on the history of metatheory
objects. It is composed of a type-checker for the kernel language. The
purpose of this layer is to maintain a repository~$R$ and the
invariant that it is well-typed. Through the layer interface, a client
can ask for the \textit{integration} $\theta(R)$ of a patch $\Delta$
to obtain an extended version $R'$ of the repository $R$.

The middle layer is a mechanized formalization of a metatheory. This
module provides a syntax for the objects of the metatheory (formulae,
judgments, \ldots) as well as the admissible rules (that we also call
metatheorems).  An \textit{internalization} function $\phi$ translates
the constructors of these objects and the specification of the
admissible rules as terms of the kernel language. As a consequence,
every object~$t$ of the theory can be injected into the
repository~$R$. An \textit{externalization} function $\psi$ is
implementing the inverse operation. Given a modified version $t'$ of
an object~$t$, we are able to compute the patch $\Delta$ using
the \textit{differentiation} function $\nabla(t)$. 

The external layer offers views on the objects of the theory. A
view~$w$ is a human readable, and editable, partial representation of
an object~$t$. A view is obtained from an object through a
projection~$\pi$.  The user transforms the view $w$ into a view $w'$
which must be $\textit{put back}$ into the metatheory as a modified
object $t'$ using a $\textit{putback}$ function $\sigma(t, w)$.  In
concrete syntax, an object~$t$ may be denoted simply using its
syntactic definition but also, more interestingly, by a reference to this 
object into the repository.

The implementation of this architecture raises some interesting
technical difficulties that can be handled, but only partially, 
by existing technology. Indeed, even if internalization and 
externalization are easily implemented thanks to the expressivity
of the kernel language, the projection, putback, differentiation
and integration operations are more intricate to realize. 

\paragraph{From an abstract proof tree to a concrete view, and back}
\-
\remtext{Fri Jun 18, 2010  1:36 PM. Un arbre de preuves, ce n'est
  pas très pratique. Pourtant, c'est moralement sur cet objet 
  que s'opère le développement et non sur le code source, qui
  n'en est qu'une représentation. On justifie la nécessité de
  projection et on fait référence aux lentilles de Pierce pour
  expliquer $\sigma$ et $\pi$.}

The problem of updating some data according to a modification that was
done on a partial view of it is a well-known issue in database
literature called the \textit{view-update} problem. Pierce \textit{et
  al} have recently attacked it from a language design approach. Their
idea is to develop projection and putback functions simultaneously by
writing a so-called lens in a bidirectional programming
language~\cite{pierce-bidirectional}. Roughly speaking, a lens is a
software component that can be used to produce output from input but
also input from output (hence the bidirectionality). A lens is able
to keep track of the information that was erased by a projection to
produce a projected output from the initial input. When we supply a
modified output to the lens, it can elaborate, from the initial input
and under certain conditions, the missing part of the modified input. 

We plan to design a bidirectional language to define the
projection~function~$\pi$ and the putback~function~$\sigma$ as a
lens. Yet, we cannot just import existing lenses combinators from the
Harmony system that already works on trees. Indeed, most of the time,
our trees have extra structural constraints induced by the presence of
binders. 

\remtext{Fri Jun 18, 2010 1:46 PM. Autre point important sur les vues
  concrètes~: elles peuvent servir à parler de l'historique.  Voir le
  module MetaAST de l'implémentation. Ces vues sont utiles par exemple
  lorsque l'utilisateur doit construire un patch d'adaptation. Il a
  alors toute la puissance des métathéorèmes ainsi que tous les objets
  déjà construits pour décrire son patch.}

\paragraph{Capturing the evolution of abstract proof trees}
\-

Computing the minimal edition script between two abstract syntax trees
is a NP-hard problem that has been heavily studied, in particular in
bio-informatics.  We guess that, in practice, we can restrict the
search to local transformations that preserve substructures (that
roughly consist in inserting, removing or moving around entire
sub-trees). In that case, an existing polynomial algorithm may be
used~\cite{Touzet05alinear}. Besides, abstract trees are relatively small in
practice. 

We are more concerned about the fact that existing algorithms are not
aware of binders that occur in proof-terms. Indeed, some standard
edition scripts must be forbidden because they could break
well-scopedness. As a consequence, we will have to adapt standard
algorithms to only look for scope-preserving edition scripts.

%% We are more concerned about our ability to elaborate high-level patches
%% from these low-level edition scripts. Actually, it is easy to deduce
%% from a low-level edition script, a low-level patch, based on object
%% syntax constructor elimination and introduction. However, such a
%% low-level patch may forbid a more efficient reuse of existing
%% objects. For instance, let us assume a transformation of an
%% object-level term~$x + x$ into~ he term~$2 + 2$. A low-level patch
%% would represents the transformation as the replacement of the first
%% sub-term $x$ by the sub-term~$2$ followed by the replacement of the
%% second sub-term $x$ by the sub-term~$2$. If we had an object-level type
%% derivation for $x + x$, the integration function would have to build
%% two sub-derivations, one for each replaced sub-terms.  Instead, a
%% high-level patch would represent that transformation as a substitution
%% of the free occurrences of~$x$ by the term $2$ in the term~$x + x$. In
%% that case, the integration function has the opportunity to apply a
%% metatheorem about the stability of typing derivations under
%% substitution and finally reuse the initial typing derivation
%% directly. Inferring automatically such a high-level patch seems an
%% unreachable objective~: there is clearly no most general high-level
%% patch. Nonetheless, an interactive process can be designed to help the
%% user building his high-level patch from a set of pre-computed
%% interpretations of the low-level patch.

\remtext{Fri Jun 18, 2010 1:38 PM. Il faut différencier deux arbres de
  syntaxe. Une méthode : la distance d'édition syntaxique permet de
  déduire des applications des règles d'élimination et d'introduction
  que l'on peut utiliser pour construire $\Delta$. Référence sur la
  littérature sur les distances d'édition entre arbres. Discussion sur
  l'élaboration de patchs de haut niveau (comme le renommage etc). }

\paragraph{Propagating changes through dependencies}
\-

There is several ways of integrating a patch $\Delta$ inside a
repository~$R$.  The naive attempt would be a simple concatenation
of~$\Delta$ at the end of~$R$.  This kind of integration, though
possible and correct, is of little interest because it does not take
into account the fact that $\Delta$ probably defines new versions of
objects already present in~$R$.

A more useful integration propagates changes to the context that uses
the modified object in order to trigger the updating of this context
(which may trigger the updating of other tree, and so forth). Again,
in full generality, it is not possible to update the context
automatically. Instead, we choose to generate \textit{challenges},
which are adaptation patches with holes. These challenges can be
addressed by the author that wrote the patch to be integrated, by the
author of the context or automatically when there is no ambiguity
(think of a context that uses a function whose body has changed but
not its type).

\remtext{Fri Jun 18, 2010 1:42 PM. Si on se contente de concaténer
  $\Delta$ à $R$, on n'a intégré qu'une nouvelle version de l'objet
  initial~$t$ dans $R$ mais on n'a pas propagé ces modifications
  auprès des clients de $t$. Cette propagation n'est \textit{a priori}
  pas triviale~: rien ne dit que $t'$ à un sens équivalent à $t$. Il
  faut donc adapter les clients en demandant un patch d'adaptation à
  l'utilisateur. On présente ces demandes sous la forme de patch à
  compléter (on fournit un peu de contexte et on a un trou pour y
  injecter une nouvelle sous-dérivation mais on peut aussi modifier le
  contexte pour l'adapter). La construction d'un patch d'adaptation
  est un processus interactif~: on ne sait pas a priori borner le
  contexte nécessaire à la construction du patchs d'adaptation donc on
  peut imaginer un sorte de jeu où le système fait augmenter le
  contexte (en suivant les dépendances) si l'utilisateur ne sait pas
  construire le patch d'adaptation. En passant, quand on parle
  d'utilisateur ici, ce peut être aussi bien un programme, comme un
  type-checker pour le langage objet, ou bien un programmeur.}

\subsection{Application on simply typed $\lambda$-calculus}

\section{Related work}
\label{rw}

The \textsf{Twelf} project (\cite{pfenning1999system}) is an
implementation of the Logical Framework (LF,
\cite{harper1993framework}). It was used in \cite{anderson1993program}
to devise transformations of proofs in order to extract efficient
programs.

The problems of managing a formal mathematical library have been dealt
with in various proof assistant and mathematical repositories. The
HELM project (\cite{asperti2000content}, \cite{asperti2006content})
was an attempt to create a large library of mathematics, importing
\textsf{Coq}'s developments into a searchable and browsable database.
Most ideas from this project were imported into the \textsf{Matita}
proof assistants (\cite{asperti2007hop}), especially a mechanism of
\emph{invalidation and regeneration} to ensure the global consistency
of its library w.r.t changes, with granularity the whole definitions
or proofs and their dependencies. The MBase project
(\cite{kohlhase2001mbase}) attempts at creating a web-based,
distributed mathematical knowledge database putting forward the idea
of \emph{development graph} (\cite{hutter2000management},
\cite{autexier2000towards}) to manage changes in the database,
allowing semantic-based retrieval and object-level dependency
management.

This idea, generalized over structured, semi-formal documents gave
birth to \texttt{\it locutor} (\cite{muller2008fine}), a fine-grained
extension of the \texttt{svn} version control system for XML
documents, embedding ontology-driven, user-defined semantic knowledge
which allows to go across the filesystem border. It embeds a
\emph{diff} algorithm, operating on the source text modulo some
equality theory to quotient the syntax. On the same line of work, we
should mention the \emph{Coccinelle} tool
(\cite{padioleau2008documenting}). It is an evolution over textual
patches, specialized on the C language, allowing more flexibility in
the matching process, and was developed to deal with the problem of
\emph{collateral evolutions} in the Linux kernel. It embeds a
declarative language for matching and transforming C source code,
operating on text modulo defined isomorphisms.

Our approach to the ``impact of changes'' problem seems novel on
several aspects: first, it applies uniformly on proofs and programming
languages by virtue of the Curry-Howard isomorphism, and because we
operate at the AST level. Secondly, by taking \emph{types} as a
witnesses for the evolution of a development, we refine the usual,
dependency-based approach for a finer granularity.

\section{Further work}
\label{fw}

\remtext{Extension vers une version constructiviste?}

\remtext{Parler du bootstraping?}

\section{Conclusion}

%% In the light of the propositions-as-types paradigms where
%% proof-checking boils down to type-checking, we abstract from whether
%% we are talking about proofs or programs, so that our development could
%% eventually also be used in traditional software development.

%% \section{Methodologies}

%% We propose here to devise a system for \emph{semantic patches}. It
%% % TODO changer le nom?
%% substitutes the idea of textual transformation by:
%% \begin{itemize}
%% \item First preferring an abstract syntax tree representation instead
%%   of plain text;
%% \item Secondly embedding semantic, or typing data into the
%%   transformations, in order to be able to reason on them.
%% \item Finally, perform type-checking in an incremental way, that is
%%   type only the syntactic difference and reuse derivation for the
%%   already known subprograms.
%% \end{itemize}

%% We want to design a \emph{language of patches}, able to represent
%% % TODO changer le nom?
%% these transformations and their semantic properties, thus capturing
%% local syntactic changes in programs (the abstract syntax tree), as
%% well as their global effect on the whole project (the typing
%% derivations). We can see this goal as a refinement of the former idea
%% of ``dependency''.

\bibliographystyle{plainnat}

\bibliography{english}

\end{document}
