\documentclass{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{mathpartir}
\usepackage{color}
\usepackage[backref=page,colorlinks=true]{hyperref}
\usepackage{amsmath, amstext, amsthm, amsfonts}
\usepackage{stmaryrd}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage[all,cmtip]{xy}
%\usepackage[protrusion=true,expansion=true]{microtype}

\definecolor{bwgreen}{rgb}{0.183,1,0.5}
\definecolor{bwmagenta}{rgb}{1,0.169,0.909}
\definecolor{bwblue}{rgb}{0.317,0.161,1}
\hypersetup{
  linkcolor=blue,
  citecolor=blue
}

\newcommand{\sort}{\textsf{s}}
\newcommand{\gor}{\ |\ }
\newcommand{\gdecl}[2]{{#1}\ &::=\ {#2}}
\newcommand{\subst}[2]{\{#1/#2\}}

% Commandes à désactiver avant la soumission.
% Remarques sur la structure.
%\newcommand{\remplan}[1]{\noindent\textcolor{bwblue}{$\triangleright$ \textbf{#1}}}
% Remarques sur le contenu.
%\newcommand{\remtext}[1]{\textcolor{bwgreen}{$\triangleright$ \textsl{#1}}}

\newcommand{\remplan}[1]{}
\newcommand{\remtext}[1]{}

\title{Towards Semantic Patches}
  % pas sûr que le nom soit encore adapté

\author{Matthias Puech $\quad\Leftrightarrow\quad$ Yann Régis-Gianas}
\date{}

\begin{document}

\maketitle

\section{Motivations}

\remplan{Motivations générales}

It may be fruitful to investigate the possible relationship between the
very \emph{methodologies} or even \emph{daily workflow} employed in
both fields of mathematics and computer science: one trying to prove a
theorem, the other constructing a program to fulfill a task. How does
one or the other elaborates his object of study? What kind of \emph{a
  posteriori} modification is he prone to doing? What do these
modifications imply on the validity of the whole edifice or the other
way around, how do one rely on existing work to build up new results?
% Y: Je ne comprends pas tout à fait la dernière question: 
Finally, how to relate how both scientists collaborate in a team?

Recent efforts in the formalization of mathematical results have
naturally led to these questions and many of them remain largely
unanswered, but the tendency seems to be to adapt existing methods
coming from software development, as illustrated for example by the
recent introduction of modules, file-based scripts and separate
compilation in proof assistants like \textsf{Coq}~{\cite{CoqDocWeb}}
or \textsf{Matita}~{\cite{Asperti06userinteraction}}, the use of
dependency management tools (\textsf{Make}) or version control system
(\textsf{GIT}, \textsf{Subversion}) to build and manage versions of a
project. Both for the development of proofs or programs, these tools
attempt to cope with the fact that most of a mathematician or
programmer's time is actually spent \emph{editing}, not
\emph{writing}.

As intelligent and widely adopted as this tool chain has gotten, we
believe that they are not adapted for the new, demanding requirements
of proof developments. Indeed, whereas compilation of a program is
usually fast enough for the programmer to rely on the usual
interaction loop ((edit; compile)*; commit)*, the operation of proof
checking is usually too expensive computationally to mimic this
workflow. But even besides the time factor, this ``traditional'' way
of formalizing mathematics hinders the process of mathematical
discovery process: once a concept contained in a file is compiled, it
is considered frozen and any changes to it require the recompilation
of the whole file; the linearity of the development also gives no room
for alternate, inequivalent definitions. This fact has nonetheless
been shown to be of crucial to the mathematical discovery
process~\cite{lakatos1964proofs}, and we believe that they should be
taken into account in the realization of mathematical assistants.
% \textsf{Coq} or \textsf{Matita}.

\remtext{Fri Jun 18, 2010 9:55. AM Ici, on pourrait rajouter un
  paragraphe pour dire qu'il existe des travaux sur la formalisation
  de metatheories dans les assistants de preuve, que ce soit en TWELF
  ou Coq. Puis, on explicite notre but général qui est de construire
  un système/framework qui remplace la chaîne d'outils traditionnelle
  en important/adaptant la technologie issue de la formalisation de
  metatheories.}

\remplan{Sujet du papier : on focalise sur un problème bien précis}

\remtext{Fri Jun 18, 2010 10:00 AM.
  Un peu de liant. Pour arriver à notre but, il y a une
  question importante à résoudre: si on te donne une méta-théorie
  formalisée, es-tu capable de décrire l'historique d'un développement
  dans cette théorie? Cela induit deux questions : dans quel langage
  et est-ce que ce langage est utilisable (parce qu'a priori, ce n'est
  pas gagné car le développeur travaille en mode ``instantané'' et ne
  veut pas se trimballer ses casseroles tout le temps\ldots).}

We propose to discuss a small part of these questions, namely the
enhancement and adaptation of version control paradigms to the
management of mathematical repositories, to witness with more
precision the \emph{impact of changes}. By studying the dependencies
between objects in a proof, we can get a much finely grained vision of
what a future evolution would impact. In proof assistants based on the
propositions-as-types paradigm, Type Theory offers a powerful tool to
witness the effect of changes finely, \emph{i.e.} types. Eventually,
we hope to be capable of expressing evolutions on formal proofs, check
them efficiently and incrementally, and manage distributed
repositories of mathematics with it, guaranteeing \emph{by typing} the
stepwise global consistency of the repository.

\remtext{Fri Jun 18, 2010 10:06 AM.  Le paragraphe précédent n'est pas
  assez précis je pense et en même temps en dit trop. À la place, je
  propose d'énoncer succintement les contributions de ce papier puis 
  le plan. Ce serait quelque chose comme:
  1. A description language for semantic repositories.
  2. Theory development through semantic patches. 
  3. Related work
  4. Further work
}

\remplan{Fri Jun 18, 2010 10:08 AM. Du coup, je restructure un peu la suite.}

\section{A kernel language to describe semantic repositories}

\subsection{How to describe a development?}

\remplan{Fri Jun 18, 2010 10:09 AM. Pour commencer, on peut essayer 
  de répondre à la question: ``À quoi sert ce langage?''. }

\remtext{Fri Jun 18, 2010 10:10 AM. Il sert à décrire des objets d'une
  théorie et, surtout, des transformations de ces objets (valides) en
  de nouveaux objets (valides). Il sert donc à décrire des applications
  de méta-théorèmes.}

\remplan{Fri Jun 18, 2010 10:13 AM. Ensuite, est-ce qu'il n'existe pas déjà
  des langages adaptés à ça? }

\remtext{Fri Jun 18, 2010 10:15 AM. Pas tout à fait. Certes, LF
  fournit un cadre qui autorise la représentation des metathéories et
  des objets, construits par application de règles
  admissibles/métathéorèmes. Cependant, la syntaxe de ces objets 
  est statique/extensionnel~: on décrit l'objet comme le résultat
  de sa construction en oubliant son histoire. Pour représenter 
  l'historique d'un développement, notre langage
  doit au contraire maintenir l'ordre dans lequel les objets ont
  été construits ainsi que la façon dont ils l'ont été.}

\remtext{Fri Jun 18, 2010 10:26 AM. Ce n'est pas peut être pas tout à fait vrai que personne n'a
  fait de proposition de langages qui décrivent un développement ...
  On peut peut-être citer Pfenning, Klint and co et faire une référence
  en avant vers le related work.}

\remtext{Fri Jun 18, 2010 10:20 AM. Remarque annexe: je pense qu'à ce
  stade de l'explication, il n'est pas nécessaire de parler
  d'incrémentalité et de partage maximal. Je crois que c'est une
  conséquence pratique heureuse de notre formalisation mais qui ne
  fait pas réellement partie des nécessités/besoins initiaux.
  Ce que je veux dire, c'est que l'incrémentalité est une propriété
  orthogonale à la capacité de dénoter une historique.}

Representing syntax and logics is nicely done in a \emph{logical
  framework} like LF (\cite{harper1993framework}): both the syntactic
elements and the typing derivations can sit in the same tree
structure, and both can be rechecked at the same time, thanks to
dependent types.

For the purpose of incremental type checking though, our needs are a
little bit different: first, we need to record, that is to name all
intermediate values of our developments, so as to be able to address
and reuse them multiple times. Secondly, we need to make sure that
those intermediate values (sub-terms) are not recorded twice, so as to
not type-check them twice. Thus we are looking to represent syntactic
and typing objects as a \emph{DAG} rather than a tree, and ensure
\emph{maximal sharing} among its sub-DAGs.

Our system will have these properties \emph{wrt.} LF:
\begin{itemize}
\item In this first iteration of the project, we do not need
  computations to take place within our DAGs. Our syntax will then be
  restricted to product types and applications;
\item Every term should be a flat application of variables, so that we
  don't introduce compound terms without naming them;
\item Finally we need a way to record intermediate definitions: we
  introduce a new kind of binder, the equality binder. We maintain the
  invariant that not two same definition can sit in the context while
  typing.
\end{itemize}

\remtext{Fri Jun 18, 2010 10:26 AM. 
  Du coup, il faudrait reformuler un peu les paragraphes précédents.}

\subsection{Formalization}

\remplan{On rentre dans les détails techniques. Je crois qu'il faut
  prendre un peu de temps pour définir précisément les termes utilisés
  (repository, patch, etc) et expliquer/paraphraser les règles.}

\paragraph{Syntax} There are three syntactic categories: applicative
terms $a$, types $t$ and environments $\Gamma$.

% Y: Remarque en passant: pour maintenir des notations cohérentes, il
% vaut mieux se définir des macros pour absolument tout
% (metavariables, jugements, ...). Ca passe mieux à l'échelle.  On
% verra ca pour le prochain papier :-).

\begin{align*}
 \gdecl{a}{x \gor a\ x } \\
 \gdecl{t}{a \gor \sort \gor (x:t)\cdot t \gor (x=a:t)\cdot t} \\
 \gdecl{\Gamma}{\cdot \gor \Gamma[x:t] \gor \Gamma[x=a:t]}
\end{align*}

Following the standard syntax, we denote by $(x:t)\cdot u$ the
dependent product (elsewhere written with a $\Pi$). The new construct
$(x=a:t)\cdot u$ is a definition, whose value is $a$. We write as
usual $(x_1:t_1)(x_2:t_2)\ldots(x_n:t_n)\cdot t$ for
$(x_1:t_1)\cdot(x_2:t_2)\cdot\ldots\cdot(x_n:t_n)\cdot t$, $t \to u$
for $(x:t)\cdot u$ where $x\notin u$, and $(x\ y : t)\cdot u$ for
$(x:t)(y:t)\cdot u$. The lookup in the environment is written either
$\Gamma(x):t$, denoting ``$\Gamma$ contains a declaration $[x:t]$
\emph{or} a definition $[x=a:t]$'', or $\Gamma(x)=a:t$ denoting
``$\Gamma$ contains a definition $[x=a:t]$''. Capture-avoiding
renaming of variables in a term $t$ is written $t\subst{x}{y}$ for
``replace all occurences of $x$ by $y$'' and is defined the usual way.

\paragraph{Typing} The typing of our system is a restriction of the
usual typing rules for dependently-typed systems with the additional
two rules for typing a singleton product and the application of a
singleton argument (featuring the syntactic verification in the
environment). It depends on: 
\begin{itemize}
  \item$\mathcal S$ a set of sorts \sort,
  \item$\mathcal A \in \mathcal S^2$ the axioms of the system,
  \item$\mathcal R \in \mathcal S^3$ the allowed products.
\end{itemize}
It relies on two distinct judgments:
\begin{itemize}
\item $\Gamma\vdash a : t$ ($a$ has type $t$ in $\Gamma$),
\item $\Gamma\vdash t : \sort$ ($t$ has sort $s$ in $\Gamma$).
\end{itemize}
The rules are:
\begin{mathpar}
  \infer[Axiom]{
  }{\Gamma\vdash\sort_1:\sort_2}
  \quad(\sort_1,\sort_2)\in\mathcal A
  \and
  \infer[Init]{
    \Gamma(x):t
  }{\Gamma\vdash x:t}
  \and
  \infer[Prod]{\Gamma\vdash t:\sort_1 \and 
    \Gamma[x:t]\vdash u:\sort_2
  }{\Gamma\vdash(x:t)\cdot u : \sort_3}
  \quad(\sort_1,\sort_2,\sort_3)\in\mathcal R
  \and
  \infer[Sing]{\Gamma\vdash a:t \and
    \Gamma\vdash t:\sort_1 \and
    \Gamma[x=a:t]\vdash u:\sort_2
  }{\Gamma\vdash(x=a:t)\cdot u : \sort_3} 
  \quad(\sort_1,\sort_2,\sort_3)\in\mathcal R
  \and
  \infer[App]{
    \Gamma\vdash a:(y:t)\cdot u \and \Gamma(x):t
    }{\Gamma\vdash a\ x : u\subst{x}{y}}
  \and
  \infer[Skip]{
    \Gamma\vdash a:(y=b:t)\cdot u \and \Gamma(x)=b:t
    }{\Gamma\vdash a : u\subst{x}{y}}
\end{mathpar}

A term $t$ represents a state of the repository at a given moment. It
contains the declaration of the object language's syntax, its typing
rules, the syntactic objects and derivations recorded in it along with
the patches that let the repository evolve. If $\cdot\vdash t:\sort$,
then the repository is well-typed.

\section{Development through semantic patches}

\remplan{Fri Jun 18, 2010 10:50 AM. L'objectif ici, c'est de décrire
  une (possible) architecture pour rendre utilisable le langage 
  décrit dans la section précédente. En gros, on énumère les 
  opérations qui interviennent dans l'utilisation et on les décrit
  informellement à l'aide d'un exemple.}

The kernel language described in the previous section is clearly not
adapted to human interaction, for several reasons~: 
\begin{itemize}

\item Every single object is flattened, which makes unobvious the
  structure of a program or a proof. 

\item A term of this language denotes the entire history of a
  development. Most of the time, the user is more interested in a
  instantaneous snapshot of his object of study.

\item Even though we were able to provide a snapshot of a particular
  state of the development, the repository represents this state 
  using proof terms, which contain too many syntactic details from the
  user point of view and are not suitable for edition.  

\end{itemize}

\remtext{Fri Jun 18, 2010 10:52 AM. Par contre, je ne sais pas quel
  exemple utilisé. Le STLC? Est-ce que ça fait ``assez'' système
  logique pour ce workshop?}

\subsection{A layered architecture}

\remtext{Fri Jun 18, 2010  1:32 PM. Ici, on décrit la figure en suivant
  le modèle de flot du système. On décrit le rôle des fonctions, 
les problèmes posés.}

% À mettre dans une figure. 
\begin{figure}
\begin{center}
% FIXME: il y a un problème avec la flèche double phi/psi.
\xymatrix{
\textrm{History of metatheory objects}
&R 
\ar@{->}[d]_{\phi}
& \Delta \ar@{->}[r]^{\theta(R)} & R' \\
\textrm{Metatheory objects} 
&t \ar@{->}[d]^{\pi}
\ar@{->}[u]_{\psi}
& t' \ar@{->}[u]_{\nabla(t)}\\
\textrm{Concrete views on metatheory objects} 
&w \ar@{.>}[r] & w' \ar@{->}[u]_{\sigma(t, w)}
}
\end{center}
\label{fig:architecture}
\caption{A layered architecture for a semantic patch framework.}
\end{figure}

With the previous remarks in mind, we designed a generic framework of semantic
patches that can be described using three layers which are summarized in 
Figure~\ref{fig:architecture}. We now describe the role of each layer.

The most internal layer focused on the history of metatheory
objects. It is composed of a type-checker for the kernel language. The
purpose of this layer is to maintain a repository~$R$ and the
invariant that it is well-typed. Through the layer interface, a client
can ask for the \textit{integration} $\theta(R)$ of a patch $\Delta$
to obtain an extended version $R'$ of the repository $R$.

The middle layer is a mechanized formalization of a metatheory. This
module provides a syntax for the objects of the metatheory (formulae,
judgments, \ldots) as well as the admissible rules (that we also call
metatheorems).  An \textit{internalization} function $\phi$ translates
the constructors of these objects and the specification of the
admissible rules as terms of the kernel language. As a consequence,
every object~$t$ of the theory can be injected into the
repository~$R$. An \textit{externalization} function $\psi$ is
implementing the inverse operation. Given a modified version $t'$ of
an object~$t$, we are able to compute the patch $\Delta$ using
the \textit{differentiation} function $\nabla(t)$. 

The external layer offers views on the objects of the theory. A
view~$w$ is a human readable, and editable, partial representation of
an object~$t$. A view is obtained from an object through a
projection~$\pi$.  The user transforms the view $w$ into a view $w'$
which must be $\textit{put back}$ into the metatheory as a modified
object $t'$ using a $\textit{putback}$ function $\sigma(t, w)$.  In
concrete syntax, an object~$t$ may be denoted simply using its
syntactic definition but also, more interestingly, by a reference into
the repository or even as the application of a metatheorem.

The implementation of this architecture raises some interesting
technical difficulties that can be handled, but only partially, 
by existing technology. Indeed, even if internalization and 
externalization are easily implemented thanks to the expressivity
of the kernel language, the projection, putback, differentiation
and integration operations are more intricate to realize. 

\paragraph{From an abstract proof tree to a concrete view, and back}
\-
\remtext{Fri Jun 18, 2010  1:36 PM. Un arbre de preuves, ce n'est
  pas très pratique. Pourtant, c'est moralement sur cet objet 
  que s'opère le développement et non sur le code source, qui
  n'en est qu'une représentation. On justifie la nécessité de
  projection et on fait référence aux lentilles de Pierce pour
  expliquer $\sigma$ et $\pi$.}

The problem of updating some data according to a modification that was
done on a partial view of it is a well-known issue in database
literature called the \textit{view-update} problem. Pierce \textit{et
  al} have recently attacked it from a language design approach. Their
idea is to develop projection and putback functions simultaneously by
writing a so-called lens in a bidirectional programming
language~\cite{pierce-bidirectional}. Roughly speaking, a lens is a
software component that can be used to produce output from input but
also input from output (hence the bidirectionality). A lens is able
to keep track of the information that was erased by a projection to
produce a projected output from the initial input. When we supply a
modified output to the lens, it can elaborate, from the initial input
and under certain conditions, the missing part of the modified input. 

We plan to design a bidirectional language to define the
projection~function~$\pi$ and the putback~function~$\sigma$ as a
lens. Yet, we cannot just import existing lenses combinators from the
Harmony system that already works on trees. Indeed, most of the time,
our trees have extra structural constraints induced by the presence of
binders. 

\remtext{Fri Jun 18, 2010 1:46 PM. Autre point important sur les vues
  concrètes~: elles peuvent servir à parler de l'historique.  Voir le
  module MetaAST de l'implémentation. Ces vues sont utiles par exemple
  lorsque l'utilisateur doit construire un patch d'adaptation. Il a
  alors toute la puissance des métathéorèmes ainsi que tous les objets
  déjà construits pour décrire son patch.}

\paragraph{Capturing the evolution of abstract proof trees}
\-

Computing the minimal edition script between two abstract syntax trees
is a NP-hard problem that has been heavily studied, in particular in
bio-informatics.  We guess that, in practice, we can restrict the
search to local transformations that preserve substructures (that
roughly consist in inserting, removing or moving around entire
sub-trees). In that case, an existing polynomial algorithm might be
used~\cite{REF!}. Besides, abstract trees are relatively small in
practice. 

We are more concerned about our ability to elaborate high-level patches
from these low-level edition scripts. Actually, it is easy to deduce
from a low-level edition script, a low-level patch, based on object
syntax constructor elimination and introduction. However, such a
low-level patch may forbid a more efficient reuse of existing
objects. For instance, let us assume a transformation of an
object-level term~$x + x$ into~ he term~$2 + 2$. A low-level patch
would represents the transformation as the replacement of the first
sub-term $x$ by the sub-term~$2$ followed by the replacement of the
second sub-term $x$ by the sub-term~$2$. If we had an object-level type
derivation for $x + x$, the integration function would have to build
two sub-derivations, one for each replaced sub-terms.  Instead, a
high-level patch would represent that transformation as a substitution
of the free occurrences of~$x$ by the term $2$ in the term~$x + x$. In
that case, the integration function has the opportunity to apply a
metatheorem about the stability of typing derivations under
substitution and finally reuse the initial typing derivation
directly. Inferring automatically such a high-level patch seems an
unreachable objective~: there is clearly no most general high-level
patch. Nonetheless, an interactive process can be designed to help the
user building his high-level patch from a set of pre-computed
interpretations of the low-level patch.

\remtext{Fri Jun 18, 2010 1:38 PM. Il faut différencier deux arbres de
  syntaxe. Une méthode : la distance d'édition syntaxique permet de
  déduire des applications des règles d'élimination et d'introduction
  que l'on peut utiliser pour construire $\Delta$. Référence sur la
  littérature sur les distances d'édition entre arbres. Discussion sur
  l'élaboration de patchs de haut niveau (comme le renommage etc). }

\paragraph{Propagating changes through dependencies}
\-

There is several ways of integrating a patch $\Delta$ inside a
repository~$R$.  The naive attempt would be a simple concatenation
of~$\Delta$ at the end of~$R$.  This kind of integration, though
possible and correct, is of little interest because it does not take
into account the fact that $\Delta$ probably defines new versions of
objects already present in~$R$.

A more useful integration propagates changes to the context that uses
the modified object in order to trigger the updating this context
too. Again, in full generality, it is not possible to update the
context automatically. Instead, we choose to generate
\textit{challenges}, which are adaptation patches with holes. These
challenges can be addressed by the author that wrote the patch to be
integrated, by the author of the context or automatically when there
is no ambiguity (think of a context that uses a function whose
body has changed but not its type). 

\remtext{Fri Jun 18, 2010 1:42 PM. Si on se contente de concaténer
  $\Delta$ à $R$, on n'a intégré qu'une nouvelle version de l'objet
  initial~$t$ dans $R$ mais on n'a pas propagé ces modifications
  auprès des clients de $t$. Cette propagation n'est \textit{a priori}
  pas triviale~: rien ne dit que $t'$ à un sens équivalent à $t$. Il
  faut donc adapter les clients en demandant un patch d'adaptation à
  l'utilisateur. On présente ces demandes sous la forme de patch à
  compléter (on fournit un peu de contexte et on a un trou pour y
  injecter une nouvelle sous-dérivation mais on peut aussi modifier le
  contexte pour l'adapter). La construction d'un patch d'adaptation
  est un processus interactif~: on ne sait pas a priori borner le
  contexte nécessaire à la construction du patchs d'adaptation donc on
  peut imaginer un sorte de jeu où le système fait augmenter le
  contexte (en suivant les dépendances) si l'utilisateur ne sait pas
  construire le patch d'adaptation. En passant, quand on parle
  d'utilisateur ici, ce peut être aussi bien un programme, comme un
  type-checker pour le langage objet, ou bien un programmeur.}

\subsection{Application on simply typed $\lambda$-calculus}

\section{Related work}

\remtext{Dolphin, Twelf, etc}

\remtext{Semantic patches à la coccinnelle}

\section{Further work}

\remtext{Extension vers une version constructiviste?}

\remtext{Parler du bootstraping?}

\section{Conclusion}

%% In the light of the propositions-as-types paradigms where
%% proof-checking boils down to type-checking, we abstract from whether
%% we are talking about proofs or programs, so that our development could
%% eventually also be used in traditional software development.

%% \section{Methodologies}

%% We propose here to devise a system for \emph{semantic patches}. It
%% % TODO changer le nom?
%% substitutes the idea of textual transformation by:
%% \begin{itemize}
%% \item First preferring an abstract syntax tree representation instead
%%   of plain text;
%% \item Secondly embedding semantic, or typing data into the
%%   transformations, in order to be able to reason on them.
%% \item Finally, perform type-checking in an incremental way, that is
%%   type only the syntactic difference and reuse derivation for the
%%   already known subprograms.
%% \end{itemize}

%% We want to design a \emph{language of patches}, able to represent
%% % TODO changer le nom?
%% these transformations and their semantic properties, thus capturing
%% local syntactic changes in programs (the abstract syntax tree), as
%% well as their global effect on the whole project (the typing
%% derivations). We can see this goal as a refinement of the former idea
%% of ``dependency''.

\bibliographystyle{plainnat}

\bibliography{english}

\end{document}
