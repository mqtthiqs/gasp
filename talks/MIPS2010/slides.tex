% Mode d'emploi: Switcher les deux lignes suivantes pour passer du
% mode présentation au handout
% ===================================================
\documentclass[ignorenonframetext,red]{beamer}
% \documentclass[12pt]{article} \usepackage{beamerarticle} \usepackage{fullpage}
% ===================================================

\usepackage{ucs}
\usepackage{mathpartir}
\usepackage{amsfonts,amsmath,amscd}
\usepackage{stmaryrd}
\usepackage[utf8x]{inputenc}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{setspace}
\usepackage{graphicx}

\title{Towards typed repositories of proofs \\[0.6em] 
  \small \textsf{MIPS 2010}}

\date{July 10, 2010}

\author[Matthias Puech \& Yann Régis-Gianas] {
Matthias Puech\inst{1} \and Yann Régis-Gianas\inst{2} \\
{\small \url{puech@cs.unibo.it}} \and {\small \url{yrg@pps.jussieu.fr}}
}
\institute {
  \inst 1 {\small Dept. of Computer Science, University of Bologna} \and
  \inst 2 {\small University Paris 7, CNRS, and INRIA, PPS, team ${\pi}r^2$}
}

\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{}

% \AtBeginSection[]
% {\begin{frame}<beamer>{Outline}
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\usefonttheme{serif}

\begin{document}

\begin{frame}
  \titlepage
  \mode<article>{
    \newcommand\url\texttt
    \maketitle}
\end{frame}

\section{Motivation}

This talk will be about some remarks and directions on the way the
edition of formal proof is done in proof assistants.

\hrule
\begin{frame}{How are \emph{constructed} formal mathematics?}
  \large
  \begin{tabular}{ll}
    {\Huge Q :} & \parbox{0.8\textwidth}{What is the common point between the working
      mathematician and the working programmer?} \\[2em]
    \pause
    {\Huge A :} & They both spend more time \emph{editing} than \emph{writing}
  \end{tabular}
\end{frame}
\hrule

Let me start with a simple observation relating the activity of both
the working mathematician and the computer scientist. Whereas both
activities are percieved, from the outside, as apodictic because the
final object of their study, the proof or the program, seems to be
unquestionably valid (or not).

If we observe the day-to-day work of these scientists though, their
workflow resembles a lot more that of experimental sciences: it is
highly non-linear, involving multiple parallel experiments, fixes,
backtrack on previous modifications\ldots\ eventually validated or
invalidated by some \emph{a posteriori} criterion, the absence of
bugs, the validity of a proof.

In other words, the mathematician and the programmer spend more time
\emph{editing} their developments non-linearly than monotonously
\emph{writing} new material.

It is the case for programs, for classical, paper mathematics so it
should be the case for formal mathematics, edited in a proof
assistant.

\hrule
\begin{frame}{A paradoxical situation}  
  \begin{block}{Observation}
    Workflow of formal mathematics was largely inspired by software
    development:
    \begin{itemize}
    \item File-based scripts (\textsf{emacs})
    \item Separate compilation (\textsf{make})
    \item Text-based versioning (\textsf{svn}, \textsf{diff}s\ldots)
    \end{itemize}
  \end{block}
  \pause
  \begin{block}{\ldots\ And yet,}
    Powerful tools to mechanize the metatheory of (proof) languages
  \end{block}
  \vspace{0.6em}
  \pause
  \begin{center}
    {\large \it Isn't it time to make these tools metatheory-aware?}
  \end{center}
\end{frame}
\hrule

Since the advent of modern proof assistants, large bodies of
mathematics have been formalized and the way people managed these
developments is very similar to the one used in software development
for years: we write proofs in text files, which are linear, and launch
proof-checking when we are done. To avoid re-checking the whole
development, we split it into inter-dependent files and re-check only
those who have changed. To keep track of old versions, we use
line-based version control systems that have no awareness of the
actual content of the files. In a sense, the non-linearity of proof
development is managed outside the scope of the tool, by manual,
textual transformations that are often very unsatisfactory.

The paradox about this situation is that these very tools can be used
to reason about formal languages and especially proof languages: some
of these tools are even dedicated to the mechanization of the
metatheory of proof languages. But we still use the legacy generic
tools to manage our developments.

Couldn't we adapt the methods developed to reason about formal
languages to the language of the proof assistant itself? Could we
replace this legacy toolchain and make it language-aware, even
semantic-aware?

\hrule
\begin{frame}{Motivations}
  \begin{block}{Rigidity of linear edition}
    \begin{itemize}
    \item ((edit; compile)*; commit)* loop does not scale to proofs
    \item Concept freeze inhibits the discovery process
    \item No room for alternate definitions
    \end{itemize}
  \end{block}
  \pause
  \begin{block}{Laxity of textual representation}
    \begin{itemize}
    \item Textual scripts \textsf{diff}s do not reflect the semantics
    \item Not even the syntax
    \end{itemize}
  \end{block}
  \vspace{2em}
  \pause
  {\tiny \ldots\ Maybe it wasn't adapted to software development}
\end{frame}
\hrule

More precisely, here are some problems that arise with the traditional
tools when editing proofs. Whereas the usual edit, compile, commit
loop may be a satisfactory approximation when developing software, the
extra computational cost of proof-checking makes it unsatisfactory as
the development grows: often the time of recompilation is prohibitive.

But it is not only a matter of time: once a concept is proof-checked,
it is frozen and any future modification of it will require to
re-check a large part of the development, often much more than what is
actually required. We believe that this fact inhibits the use of proof
assistants as tools to help the mathematician in the discovery
process, as an experimentation tool, and not only to formally
transcribe paper proofs.

The same way, the linearity of proof scripts does not allow any
flexibility in the alternate definition of concepts.

On the other side, the way we store mathematical developments, that is
as textual files or textual diffs does not reflect neither their
syntax nor their semantics, and as I will try to show, there is a lot
to gain in changing this base representation.

In fact, some will say that this model of interaction wasn't even
adapted to programming in the first place.

\begin{frame}{The impact of changes}
  \vspace{0.5em}
  \begin{center}
    \only<1>{\includegraphics[width=0.75\textwidth]{images/coqide00.png}}%
    \only<2>{\includegraphics[width=0.75\textwidth]{images/coqide0.png}}%
    \only<3>{\includegraphics[width=0.75\textwidth]{images/coqide.png}}%
    \only<4>{\includegraphics[width=0.75\textwidth]{images/coqide2.png}}%
    \only<5>{\includegraphics[width=0.75\textwidth]{images/coqide4.png}}%
    \only<6>{\includegraphics[width=0.75\textwidth]{images/coqide5.png}}%
    \only<7>{\includegraphics[width=0.75\textwidth]{images/coqide6.png}}%
    \only<8>{\includegraphics[width=0.75\textwidth]{images/coqide7.png}}%
  \end{center}
  \pause
  \uncover<2->{
    \begin{itemize}
    \item File-based separate compilation
      \pause
    \item Interaction loop with global undo
    \end{itemize}
  }
\end{frame}

We can then see a number of extra-logical features of modern proof
assistants as ways to cope with these problems. 

Instead of writing our complete development in one unique file and let
the program check it when done, we can split it in several files or
modules and establish dependencies among them: only those who changed
(and their dependencies) have to be rechecked.

But this separation serves two different purpose, and the fact that
they coincide is pure contingency: the purpose of splitting a
development into logical units, and the purpose of providing a coarse
notion of dependency to ease the non-linear process of proof editing
by accelerating recompilation upon changes.

Secondly, the traditional interaction loop makes unnecessary the
recompilation upon local changes by providing a global undo feature of
the whole state of the proof assistant. But if I want to change this
proof, I have to backtrack all the way here (because the green region
is locked), do my changes and then replay the whole proofs in between,
even if my change had no impact on this one, even if I just added a
comment.

The actual text of the proofs for versioning is often
non-satisfactory: on one side it contains mostly non-relevant
informations (line breaks, spaces\ldots), on the other the relevant
ones, the validity of the proof, the dependency between objects, is
absent.

So both these facilities, among others, seem to provide ad-hoc ways to
deal with the non-linearity of the script and its edition. They
provide a coarse approximation of the dependency amongst objects. But
this notion of dependency \emph{is} a metatheoretical property of a
logic, and it could be used to provide an \emph{exact} way of dealing
with the impact of changes, in particular to guide proof-checking to
recheck only minimal parts of the developments.

This is what we will try to do in the following.

\begin{frame}{Methodology}
  \begin{center}
    \only<1-2>{\fbox{\parbox{20em}{\only<2>\alert{version
            management}\\[1em]%
          \fbox{\parbox{18em}{script files\\[1em]%
              \fbox{\parbox{17em}{parsing\\[1em]%
                  \fbox{\parbox{14em}{proof-checking\\} }}}}}}}}%
    \only<3>{\fbox{\parbox{20em}{script files\\[1em]%
          \fbox{\parbox{18em}{\alert{version management}\\[1em]%
              \fbox{\parbox{17em}{parsing\\[1em]%
                  \fbox{\parbox{14em}{proof-checking\\} }}}}}}}}%
    \only<4->{\fbox{\parbox{20em}{\alt<-5>{script files}{user
            interaction}\\[1em]%
          \fbox{\parbox{18em}{parsing\\[1em]%
              \fbox{\parbox{17em}{\alert{version management
                    \only<7->{+ proof-checking}}\\[1em]%
                  \uncover<4-6>{\fbox{\parbox{14.5em}{proof-checking\\}}}}}}}}}}%
  \end{center}
  \begin{itemize}
  \item<4-> AST representation
  \item<5-> Explicit dependency DAG
  \item<7-> Typing annotations
  \item<8-> Incremental type-checking
  \end{itemize}
\end{frame}

Here is a schematic view of the traditional way to manage formal
developments. The user writes scripts which are version controlled,
and these scripts are parsed and checked on demand. By pushing version
management inwards, we abstract from syntax, and more informations
become available. For instance we can compute the graph of
dependencies. At this level already, the situation is greatly
enhanced: syntax, and file-based development, get the place they
deserve: they are relegated to a tool for user interaction, the actual
development being stored and manipulated in an abstract form. We could
even imagine syntax being dynamically switchable, part of user
preferences!

But let's go one step further and embed version management into the
type-checker. We gain access to type informations at the level of
changes, that is, we can track changes, dependencies and types at the
same time, and decide to type-check only local changes of the
development and their global effect, in a minimal way. We can perform
\emph{incremental type-checking}.

\begin{frame}{A core meta-language for incremental type-checking}
  \begin{columns}[t]
    \begin{column}{0.5\textwidth}
      \begin{block}{Expresses}
        \begin{itemize}
        \item (abstract) Syntax
        \item (object-) Logics
        \item Proofs (-terms)
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{block}{Features}
        \begin{itemize}
        \item Typing
        \item Incrementality
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
  \vspace{2em}
  \begin{center}
    \emph{A kernel for a typed version control system?}
  \end{center}
\end{frame}

In the following, I will present a preliminary implementation of these
ideas. It is a meta-language for incremental proof-checking, that is a
language in which one can define abstract syntaxes, logical rules and
proofs, and allows to check validity, and express the incrementality
of the constructions. It can be viewed as a kernel for a generic,
typed version control system.

\begin{frame}{Inspiration: the \textsf{git} storage model}
  \begin{center}
    \begin{overlayarea}{\textwidth}{12em}
      \only<2>{\includegraphics[width=0.85\textwidth]{images/git1.pdf}}%
      \only<3>{\includegraphics[width=0.85\textwidth]{images/git2.pdf}}%
      \only<4>{\includegraphics[width=0.85\textwidth]{images/git3.pdf}}%
      \only<5>{\includegraphics[width=0.85\textwidth]{images/git4.pdf}}%
      \only<6-9>{\includegraphics[width=0.85\textwidth]{images/git5.pdf}}%
      \only<10>{
        \begin{center}
          \vspace{5em}
          {\Large Let's design a \emph{typed} \textsf{git}}
        \end{center}
      }%
    \end{overlayarea}
  \end{center}%
  \begin{itemize}\small %
  \item<7-> ``Content-adressable''%
  \item<8-> Name reflects content%
  \item<9-> Maximal sharing (or hash-consing)%
  \end{itemize}%
\end{frame}

\textsf{Git} is one of the most successful version control system, and
part of its success is due to the very simple, yet very efficient
storage model. It doesn't keep track of changes \emph{per se}, but of
similarities between version. Given a base directory structure,
already committed in its database, and a new version of it with, let's
say, one file changed -- \texttt{f3.txt}, it stores the new version by
linking all common subdirectory structure to the previous version.
This is possible because every node in the tree, directories and
files, have a unique name. So actually, the database of \textsf{git}
looks like this.

Moreover, similarities are easily tracked because every name is
carefully chosen to reflect the \emph{content} of a file or directory.
In practice, \texttt{git} uses a hash function to produce all names in
the database: files are given the hash of their content, and
directories are given the hash of the hash of all the files it
contains, recursively.

This way, we ensure that no two subdirectories with different content
appear in the database, that is, we enforce maximal sharing among
subdirectories. In functional programming, this is called
hash-consing.

\begin{frame}{Incremental type-checking}
  A small language for representing proofs:
  \only<1>{\[ t\ ::=\ (x:t)\cdot t\ |\ x\ |\ t\ t\ |\ *\] \[\]}%
  \only<2>{\[ t\ ::=\ (x:t)\cdot t\ |\ \alert x\ |\ \alert{t\ t}\ |\ *\] \[\]}%
  \only<3>{\[ t\ ::=\ (x:t)\cdot t\ |\ \alert a\ |\ *\]%
    \[ a\ ::=\ x\ |\ a\ x \]} 
  \only<4>{\[ t\ ::=\ (x:t)\cdot t\ |\ a\
    |\ *\ |\ \alert{(x = a)\cdot t}\]%
    \[ a\ ::=\ x\ |\ a\ x \]}
  \begin{example}
    \[ \infer[Lam]{\Gamma,A\vdash B}{\Gamma\vdash A\to B} \]
    \[ \textsc{Lam}\ :\ (\Gamma : env)\cdot(A B : type)\cdot
    (d : \Gamma,A\vdash B)\cdot \Gamma\vdash A\to B \]
  \end{example}
\end{frame}

\end{document}
